{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Transformer from scratch - 10 March - kn to ta.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "cDW7qYm5TC9S"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        " # w and b (wandb) for logging\n",
        "! pip install wandb\n",
        "\n",
        "# sacremos - for tokenizing\n",
        "! pip install sacremos\n",
        "\n",
        "# fairseq - for training and evaluation of the model\n",
        "! git clone https://github.com/pytorch/fairseq\n",
        "%cd fairseq\n",
        "! pip install --editable ./\n",
        "%cd ..\n",
        "\n",
        "%cd /content/fairseq/\n",
        "!python -m pip install --editable .\n",
        "%cd /content\n",
        "\n",
        "! echo $PYTHONPATH\n",
        "\n",
        "import os\n",
        "os.environ['PYTHONPATH'] += \":/content/fairseq/\"\n",
        "\n",
        "! echo $PYTHONPATH"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import wandb\n",
        "wandb.login()\n",
        "# login authorization."
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 69
        },
        "id": "v2z1sA0FUJsC",
        "outputId": "3385003b-05f4-4814-ec67-5e3c21e013ee"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/javascript": [
              "\n",
              "        window._wandbApiKey = new Promise((resolve, reject) => {\n",
              "            function loadScript(url) {\n",
              "            return new Promise(function(resolve, reject) {\n",
              "                let newScript = document.createElement(\"script\");\n",
              "                newScript.onerror = reject;\n",
              "                newScript.onload = resolve;\n",
              "                document.body.appendChild(newScript);\n",
              "                newScript.src = url;\n",
              "            });\n",
              "            }\n",
              "            loadScript(\"https://cdn.jsdelivr.net/npm/postmate/build/postmate.min.js\").then(() => {\n",
              "            const iframe = document.createElement('iframe')\n",
              "            iframe.style.cssText = \"width:0;height:0;border:none\"\n",
              "            document.body.appendChild(iframe)\n",
              "            const handshake = new Postmate({\n",
              "                container: iframe,\n",
              "                url: 'https://wandb.ai/authorize'\n",
              "            });\n",
              "            const timeout = setTimeout(() => reject(\"Couldn't auto authenticate\"), 5000)\n",
              "            handshake.then(function(child) {\n",
              "                child.on('authorize', data => {\n",
              "                    clearTimeout(timeout)\n",
              "                    resolve(data)\n",
              "                });\n",
              "            });\n",
              "            })\n",
              "        });\n",
              "    "
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!unzip /content/tokenized_kn_ta.zip"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qNDj13-NbG72",
        "outputId": "01f4fd40-a2a0-4629-b692-786cb46fa572"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Archive:  /content/tokenized_kn_ta.zip\n",
            "   creating: content/nmt-kn-ta/tokenized.kn-ta/\n",
            "  inflating: content/nmt-kn-ta/tokenized.kn-ta/valid.kn-ta.kn.bin  \n",
            "  inflating: content/nmt-kn-ta/tokenized.kn-ta/valid.kn-ta.ta.bin  \n",
            "  inflating: content/nmt-kn-ta/tokenized.kn-ta/train.kn-ta.ta.idx  \n",
            "  inflating: content/nmt-kn-ta/tokenized.kn-ta/test.kn-ta.ta.bin  \n",
            "  inflating: content/nmt-kn-ta/tokenized.kn-ta/dict.ta.txt  \n",
            "  inflating: content/nmt-kn-ta/tokenized.kn-ta/preprocess.log  \n",
            "  inflating: content/nmt-kn-ta/tokenized.kn-ta/test.kn-ta.kn.idx  \n",
            "  inflating: content/nmt-kn-ta/tokenized.kn-ta/train.kn-ta.kn.bin  \n",
            "  inflating: content/nmt-kn-ta/tokenized.kn-ta/valid.kn-ta.kn.idx  \n",
            "  inflating: content/nmt-kn-ta/tokenized.kn-ta/test.kn-ta.ta.idx  \n",
            "  inflating: content/nmt-kn-ta/tokenized.kn-ta/test.kn-ta.kn.bin  \n",
            "  inflating: content/nmt-kn-ta/tokenized.kn-ta/train.kn-ta.kn.idx  \n",
            "  inflating: content/nmt-kn-ta/tokenized.kn-ta/valid.kn-ta.ta.idx  \n",
            "  inflating: content/nmt-kn-ta/tokenized.kn-ta/train.kn-ta.ta.bin  \n",
            "  inflating: content/nmt-kn-ta/tokenized.kn-ta/dict.kn.txt  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "! fairseq-train /content/content/nmt-kn-ta/tokenized.kn-ta \\\n",
        "--arch transformer \\\n",
        "--dropout 0.1 \\\n",
        "--attention-dropout 0.1 \\\n",
        "--activation-dropout 0.1 \\\n",
        "--encoder-embed-dim 256 \\\n",
        "--encoder-ffn-embed-dim 512 \\\n",
        "--encoder-layers 3 \\\n",
        "--encoder-attention-heads 8 \\\n",
        "--encoder-learned-pos \\\n",
        "--decoder-ffn-embed-dim 512 \\\n",
        "--decoder-layers 3 \\\n",
        "--decoder-attention-heads 8 \\\n",
        "--decoder-learned-pos \\\n",
        "--max-epoch 10 \\\n",
        "--optimizer adam \\\n",
        "--lr 5e-4 \\\n",
        "--batch-size 128 \\\n",
        "--seed 1 \\\n",
        "--wandb-project \"Transformer from scratch - 10 March - kn to ta\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UiKzsKA0bU7a",
        "outputId": "85201f1f-a7ef-466d-d447-5cc7b5935bdc"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2022-03-10 06:05:03 | INFO | numexpr.utils | NumExpr defaulting to 2 threads.\n",
            "2022-03-10 06:05:04 | INFO | fairseq.tasks.text_to_speech | Please install tensorboardX: pip install tensorboardX\n",
            "2022-03-10 06:05:08 | INFO | fairseq_cli.train | {'_name': None, 'common': {'_name': None, 'no_progress_bar': False, 'log_interval': 100, 'log_format': None, 'log_file': None, 'tensorboard_logdir': None, 'wandb_project': 'Transformer from scratch - 10 March - kn to ta', 'azureml_logging': False, 'seed': 1, 'cpu': False, 'tpu': False, 'bf16': False, 'memory_efficient_bf16': False, 'fp16': False, 'memory_efficient_fp16': False, 'fp16_no_flatten_grads': False, 'fp16_init_scale': 128, 'fp16_scale_window': None, 'fp16_scale_tolerance': 0.0, 'on_cpu_convert_precision': False, 'min_loss_scale': 0.0001, 'threshold_loss_scale': None, 'amp': False, 'amp_batch_retries': 2, 'amp_init_scale': 128, 'amp_scale_window': None, 'user_dir': None, 'empty_cache_freq': 0, 'all_gather_list_size': 16384, 'model_parallel_size': 1, 'quantization_config_path': None, 'profile': False, 'reset_logging': False, 'suppress_crashes': False, 'use_plasma_view': False, 'plasma_path': '/tmp/plasma'}, 'common_eval': {'_name': None, 'path': None, 'post_process': None, 'quiet': False, 'model_overrides': '{}', 'results_path': None}, 'distributed_training': {'_name': None, 'distributed_world_size': 1, 'distributed_num_procs': 1, 'distributed_rank': 0, 'distributed_backend': 'nccl', 'distributed_init_method': None, 'distributed_port': -1, 'device_id': 0, 'distributed_no_spawn': False, 'ddp_backend': 'pytorch_ddp', 'ddp_comm_hook': 'none', 'bucket_cap_mb': 25, 'fix_batches_to_gpus': False, 'find_unused_parameters': False, 'gradient_as_bucket_view': False, 'fast_stat_sync': False, 'heartbeat_timeout': -1, 'broadcast_buffers': False, 'slowmo_momentum': None, 'slowmo_base_algorithm': 'localsgd', 'localsgd_frequency': 3, 'nprocs_per_node': 1, 'pipeline_model_parallel': False, 'pipeline_balance': None, 'pipeline_devices': None, 'pipeline_chunks': 0, 'pipeline_encoder_balance': None, 'pipeline_encoder_devices': None, 'pipeline_decoder_balance': None, 'pipeline_decoder_devices': None, 'pipeline_checkpoint': 'never', 'zero_sharding': 'none', 'fp16': False, 'memory_efficient_fp16': False, 'tpu': False, 'no_reshard_after_forward': False, 'fp32_reduce_scatter': False, 'cpu_offload': False, 'use_sharded_state': False, 'not_fsdp_flatten_parameters': False}, 'dataset': {'_name': None, 'num_workers': 1, 'skip_invalid_size_inputs_valid_test': False, 'max_tokens': None, 'batch_size': 128, 'required_batch_size_multiple': 8, 'required_seq_len_multiple': 1, 'dataset_impl': None, 'data_buffer_size': 10, 'train_subset': 'train', 'valid_subset': 'valid', 'combine_valid_subsets': None, 'ignore_unused_valid_subsets': False, 'validate_interval': 1, 'validate_interval_updates': 0, 'validate_after_updates': 0, 'fixed_validation_seed': None, 'disable_validation': False, 'max_tokens_valid': None, 'batch_size_valid': 128, 'max_valid_steps': None, 'curriculum': 0, 'gen_subset': 'test', 'num_shards': 1, 'shard_id': 0, 'grouped_shuffling': False, 'update_epoch_batch_itr': False, 'update_ordered_indices_seed': False}, 'optimization': {'_name': None, 'max_epoch': 10, 'max_update': 0, 'stop_time_hours': 0.0, 'clip_norm': 0.0, 'sentence_avg': False, 'update_freq': [1], 'lr': [0.0005], 'stop_min_lr': -1.0, 'use_bmuf': False, 'skip_remainder_batch': False}, 'checkpoint': {'_name': None, 'save_dir': 'checkpoints', 'restore_file': 'checkpoint_last.pt', 'continue_once': None, 'finetune_from_model': None, 'reset_dataloader': False, 'reset_lr_scheduler': False, 'reset_meters': False, 'reset_optimizer': False, 'optimizer_overrides': '{}', 'save_interval': 1, 'save_interval_updates': 0, 'keep_interval_updates': -1, 'keep_interval_updates_pattern': -1, 'keep_last_epochs': -1, 'keep_best_checkpoints': -1, 'no_save': False, 'no_epoch_checkpoints': False, 'no_last_checkpoints': False, 'no_save_optimizer_state': False, 'best_checkpoint_metric': 'loss', 'maximize_best_checkpoint_metric': False, 'patience': -1, 'checkpoint_suffix': '', 'checkpoint_shard_count': 1, 'load_checkpoint_on_all_dp_ranks': False, 'write_checkpoints_asynchronously': False, 'model_parallel_size': 1}, 'bmuf': {'_name': None, 'block_lr': 1.0, 'block_momentum': 0.875, 'global_sync_iter': 50, 'warmup_iterations': 500, 'use_nbm': False, 'average_sync': False, 'distributed_world_size': 1}, 'generation': {'_name': None, 'beam': 5, 'nbest': 1, 'max_len_a': 0.0, 'max_len_b': 200, 'min_len': 1, 'match_source_len': False, 'unnormalized': False, 'no_early_stop': False, 'no_beamable_mm': False, 'lenpen': 1.0, 'unkpen': 0.0, 'replace_unk': None, 'sacrebleu': False, 'score_reference': False, 'prefix_size': 0, 'no_repeat_ngram_size': 0, 'sampling': False, 'sampling_topk': -1, 'sampling_topp': -1.0, 'constraints': None, 'temperature': 1.0, 'diverse_beam_groups': -1, 'diverse_beam_strength': 0.5, 'diversity_rate': -1.0, 'print_alignment': None, 'print_step': False, 'lm_path': None, 'lm_weight': 0.0, 'iter_decode_eos_penalty': 0.0, 'iter_decode_max_iter': 10, 'iter_decode_force_max_iter': False, 'iter_decode_with_beam': 1, 'iter_decode_with_external_reranker': False, 'retain_iter_history': False, 'retain_dropout': False, 'retain_dropout_modules': None, 'decoding_format': None, 'no_seed_provided': False}, 'eval_lm': {'_name': None, 'output_word_probs': False, 'output_word_stats': False, 'context_window': 0, 'softmax_batch': 9223372036854775807}, 'interactive': {'_name': None, 'buffer_size': 0, 'input': '-'}, 'model': Namespace(_name='transformer', activation_dropout=0.1, activation_fn='relu', adam_betas=(0.9, 0.999), adam_eps=1e-08, adaptive_input=False, adaptive_softmax_cutoff=None, adaptive_softmax_dropout=0, all_gather_list_size=16384, amp=False, amp_batch_retries=2, amp_init_scale=128, amp_scale_window=None, arch='transformer', attention_dropout=0.1, azureml_logging=False, batch_size=128, batch_size_valid=128, best_checkpoint_metric='loss', bf16=False, bpe=None, broadcast_buffers=False, bucket_cap_mb=25, checkpoint_activations=False, checkpoint_shard_count=1, checkpoint_suffix='', clip_norm=0.0, combine_valid_subsets=None, continue_once=None, cpu=False, cpu_offload=False, criterion='cross_entropy', cross_self_attention=False, curriculum=0, data='/content/content/nmt-kn-ta/tokenized.kn-ta', data_buffer_size=10, dataset_impl=None, ddp_backend='pytorch_ddp', ddp_comm_hook='none', decoder_attention_heads=8, decoder_embed_dim=256, decoder_embed_path=None, decoder_ffn_embed_dim=512, decoder_input_dim=256, decoder_layerdrop=0, decoder_layers=3, decoder_layers_to_keep=None, decoder_learned_pos=True, decoder_normalize_before=False, decoder_output_dim=256, device_id=0, disable_validation=False, distributed_backend='nccl', distributed_init_method=None, distributed_no_spawn=False, distributed_num_procs=1, distributed_port=-1, distributed_rank=0, distributed_world_size=1, dropout=0.1, ema_decay=0.9999, ema_fp32=False, ema_seed_model=None, ema_start_update=0, ema_update_freq=1, empty_cache_freq=0, encoder_attention_heads=8, encoder_embed_dim=256, encoder_embed_path=None, encoder_ffn_embed_dim=512, encoder_layerdrop=0, encoder_layers=3, encoder_layers_to_keep=None, encoder_learned_pos=True, encoder_normalize_before=False, eos=2, eval_bleu=False, eval_bleu_args='{}', eval_bleu_detok='space', eval_bleu_detok_args='{}', eval_bleu_print_samples=False, eval_bleu_remove_bpe=None, eval_tokenized_bleu=False, fast_stat_sync=False, find_unused_parameters=False, finetune_from_model=None, fix_batches_to_gpus=False, fixed_validation_seed=None, force_anneal=None, fp16=False, fp16_adam_stats=False, fp16_init_scale=128, fp16_no_flatten_grads=False, fp16_scale_tolerance=0.0, fp16_scale_window=None, fp32_reduce_scatter=False, gen_subset='test', gradient_as_bucket_view=False, grouped_shuffling=False, heartbeat_timeout=-1, ignore_unused_valid_subsets=False, keep_best_checkpoints=-1, keep_interval_updates=-1, keep_interval_updates_pattern=-1, keep_last_epochs=-1, layernorm_embedding=False, left_pad_source=True, left_pad_target=False, load_alignments=False, load_checkpoint_on_all_dp_ranks=False, localsgd_frequency=3, log_file=None, log_format=None, log_interval=100, lr=[0.0005], lr_scheduler='fixed', lr_shrink=0.1, max_epoch=10, max_tokens=None, max_tokens_valid=None, max_update=0, max_valid_steps=None, maximize_best_checkpoint_metric=False, memory_efficient_bf16=False, memory_efficient_fp16=False, min_loss_scale=0.0001, model_parallel_size=1, no_cross_attention=False, no_epoch_checkpoints=False, no_last_checkpoints=False, no_progress_bar=False, no_reshard_after_forward=False, no_save=False, no_save_optimizer_state=False, no_scale_embedding=False, no_seed_provided=False, no_token_positional_embeddings=False, not_fsdp_flatten_parameters=False, nprocs_per_node=1, num_batch_buckets=0, num_shards=1, num_workers=1, offload_activations=False, on_cpu_convert_precision=False, optimizer='adam', optimizer_overrides='{}', pad=1, patience=-1, pipeline_balance=None, pipeline_checkpoint='never', pipeline_chunks=0, pipeline_decoder_balance=None, pipeline_decoder_devices=None, pipeline_devices=None, pipeline_encoder_balance=None, pipeline_encoder_devices=None, pipeline_model_parallel=False, plasma_path='/tmp/plasma', profile=False, quant_noise_pq=0, quant_noise_pq_block_size=8, quant_noise_scalar=0, quantization_config_path=None, required_batch_size_multiple=8, required_seq_len_multiple=1, reset_dataloader=False, reset_logging=False, reset_lr_scheduler=False, reset_meters=False, reset_optimizer=False, restore_file='checkpoint_last.pt', save_dir='checkpoints', save_interval=1, save_interval_updates=0, scoring='bleu', seed=1, sentence_avg=False, shard_id=0, share_all_embeddings=False, share_decoder_input_output_embed=False, simul_type=None, skip_invalid_size_inputs_valid_test=False, skip_remainder_batch=False, slowmo_base_algorithm='localsgd', slowmo_momentum=None, source_lang=None, stop_min_lr=-1.0, stop_time_hours=0, store_ema=False, suppress_crashes=False, target_lang=None, task='translation', tensorboard_logdir=None, threshold_loss_scale=None, tie_adaptive_weights=False, tokenizer=None, tpu=False, train_subset='train', truncate_source=False, unk=3, update_epoch_batch_itr=False, update_freq=[1], update_ordered_indices_seed=False, upsample_primary=-1, use_bmuf=False, use_old_adam=False, use_plasma_view=False, use_sharded_state=False, user_dir=None, valid_subset='valid', validate_after_updates=0, validate_interval=1, validate_interval_updates=0, wandb_project='Transformer from scratch - 10 March - kn to ta', warmup_updates=0, weight_decay=0.0, write_checkpoints_asynchronously=False, zero_sharding='none'), 'task': {'_name': 'translation', 'data': '/content/content/nmt-kn-ta/tokenized.kn-ta', 'source_lang': None, 'target_lang': None, 'load_alignments': False, 'left_pad_source': True, 'left_pad_target': False, 'max_source_positions': 1024, 'max_target_positions': 1024, 'upsample_primary': -1, 'truncate_source': False, 'num_batch_buckets': 0, 'train_subset': 'train', 'dataset_impl': None, 'required_seq_len_multiple': 1, 'eval_bleu': False, 'eval_bleu_args': '{}', 'eval_bleu_detok': 'space', 'eval_bleu_detok_args': '{}', 'eval_tokenized_bleu': False, 'eval_bleu_remove_bpe': None, 'eval_bleu_print_samples': False}, 'criterion': {'_name': 'cross_entropy', 'sentence_avg': False}, 'optimizer': {'_name': 'adam', 'adam_betas': [0.9, 0.999], 'adam_eps': 1e-08, 'weight_decay': 0.0, 'use_old_adam': False, 'fp16_adam_stats': False, 'tpu': False, 'lr': [0.0005]}, 'lr_scheduler': {'_name': 'fixed', 'force_anneal': None, 'lr_shrink': 0.1, 'warmup_updates': 0, 'lr': [0.0005]}, 'scoring': {'_name': 'bleu', 'pad': 1, 'eos': 2, 'unk': 3}, 'bpe': None, 'tokenizer': None, 'ema': {'_name': None, 'store_ema': False, 'ema_decay': 0.9999, 'ema_start_update': 0, 'ema_seed_model': None, 'ema_update_freq': 1, 'ema_fp32': False}, 'simul_type': None}\n",
            "2022-03-10 06:05:08 | INFO | fairseq.tasks.translation | [kn] dictionary: 19144 types\n",
            "2022-03-10 06:05:08 | INFO | fairseq.tasks.translation | [ta] dictionary: 13456 types\n",
            "2022-03-10 06:05:08 | INFO | fairseq_cli.train | TransformerModel(\n",
            "  (encoder): TransformerEncoderBase(\n",
            "    (dropout_module): FairseqDropout()\n",
            "    (embed_tokens): Embedding(19144, 256, padding_idx=1)\n",
            "    (embed_positions): LearnedPositionalEmbedding(1026, 256, padding_idx=1)\n",
            "    (layers): ModuleList(\n",
            "      (0): TransformerEncoderLayerBase(\n",
            "        (self_attn): MultiheadAttention(\n",
            "          (dropout_module): FairseqDropout()\n",
            "          (k_proj): Linear(in_features=256, out_features=256, bias=True)\n",
            "          (v_proj): Linear(in_features=256, out_features=256, bias=True)\n",
            "          (q_proj): Linear(in_features=256, out_features=256, bias=True)\n",
            "          (out_proj): Linear(in_features=256, out_features=256, bias=True)\n",
            "        )\n",
            "        (self_attn_layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
            "        (dropout_module): FairseqDropout()\n",
            "        (activation_dropout_module): FairseqDropout()\n",
            "        (fc1): Linear(in_features=256, out_features=512, bias=True)\n",
            "        (fc2): Linear(in_features=512, out_features=256, bias=True)\n",
            "        (final_layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
            "      )\n",
            "      (1): TransformerEncoderLayerBase(\n",
            "        (self_attn): MultiheadAttention(\n",
            "          (dropout_module): FairseqDropout()\n",
            "          (k_proj): Linear(in_features=256, out_features=256, bias=True)\n",
            "          (v_proj): Linear(in_features=256, out_features=256, bias=True)\n",
            "          (q_proj): Linear(in_features=256, out_features=256, bias=True)\n",
            "          (out_proj): Linear(in_features=256, out_features=256, bias=True)\n",
            "        )\n",
            "        (self_attn_layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
            "        (dropout_module): FairseqDropout()\n",
            "        (activation_dropout_module): FairseqDropout()\n",
            "        (fc1): Linear(in_features=256, out_features=512, bias=True)\n",
            "        (fc2): Linear(in_features=512, out_features=256, bias=True)\n",
            "        (final_layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
            "      )\n",
            "      (2): TransformerEncoderLayerBase(\n",
            "        (self_attn): MultiheadAttention(\n",
            "          (dropout_module): FairseqDropout()\n",
            "          (k_proj): Linear(in_features=256, out_features=256, bias=True)\n",
            "          (v_proj): Linear(in_features=256, out_features=256, bias=True)\n",
            "          (q_proj): Linear(in_features=256, out_features=256, bias=True)\n",
            "          (out_proj): Linear(in_features=256, out_features=256, bias=True)\n",
            "        )\n",
            "        (self_attn_layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
            "        (dropout_module): FairseqDropout()\n",
            "        (activation_dropout_module): FairseqDropout()\n",
            "        (fc1): Linear(in_features=256, out_features=512, bias=True)\n",
            "        (fc2): Linear(in_features=512, out_features=256, bias=True)\n",
            "        (final_layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
            "      )\n",
            "    )\n",
            "  )\n",
            "  (decoder): TransformerDecoderBase(\n",
            "    (dropout_module): FairseqDropout()\n",
            "    (embed_tokens): Embedding(13456, 256, padding_idx=1)\n",
            "    (embed_positions): LearnedPositionalEmbedding(1026, 256, padding_idx=1)\n",
            "    (layers): ModuleList(\n",
            "      (0): TransformerDecoderLayerBase(\n",
            "        (dropout_module): FairseqDropout()\n",
            "        (self_attn): MultiheadAttention(\n",
            "          (dropout_module): FairseqDropout()\n",
            "          (k_proj): Linear(in_features=256, out_features=256, bias=True)\n",
            "          (v_proj): Linear(in_features=256, out_features=256, bias=True)\n",
            "          (q_proj): Linear(in_features=256, out_features=256, bias=True)\n",
            "          (out_proj): Linear(in_features=256, out_features=256, bias=True)\n",
            "        )\n",
            "        (activation_dropout_module): FairseqDropout()\n",
            "        (self_attn_layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
            "        (encoder_attn): MultiheadAttention(\n",
            "          (dropout_module): FairseqDropout()\n",
            "          (k_proj): Linear(in_features=256, out_features=256, bias=True)\n",
            "          (v_proj): Linear(in_features=256, out_features=256, bias=True)\n",
            "          (q_proj): Linear(in_features=256, out_features=256, bias=True)\n",
            "          (out_proj): Linear(in_features=256, out_features=256, bias=True)\n",
            "        )\n",
            "        (encoder_attn_layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
            "        (fc1): Linear(in_features=256, out_features=512, bias=True)\n",
            "        (fc2): Linear(in_features=512, out_features=256, bias=True)\n",
            "        (final_layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
            "      )\n",
            "      (1): TransformerDecoderLayerBase(\n",
            "        (dropout_module): FairseqDropout()\n",
            "        (self_attn): MultiheadAttention(\n",
            "          (dropout_module): FairseqDropout()\n",
            "          (k_proj): Linear(in_features=256, out_features=256, bias=True)\n",
            "          (v_proj): Linear(in_features=256, out_features=256, bias=True)\n",
            "          (q_proj): Linear(in_features=256, out_features=256, bias=True)\n",
            "          (out_proj): Linear(in_features=256, out_features=256, bias=True)\n",
            "        )\n",
            "        (activation_dropout_module): FairseqDropout()\n",
            "        (self_attn_layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
            "        (encoder_attn): MultiheadAttention(\n",
            "          (dropout_module): FairseqDropout()\n",
            "          (k_proj): Linear(in_features=256, out_features=256, bias=True)\n",
            "          (v_proj): Linear(in_features=256, out_features=256, bias=True)\n",
            "          (q_proj): Linear(in_features=256, out_features=256, bias=True)\n",
            "          (out_proj): Linear(in_features=256, out_features=256, bias=True)\n",
            "        )\n",
            "        (encoder_attn_layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
            "        (fc1): Linear(in_features=256, out_features=512, bias=True)\n",
            "        (fc2): Linear(in_features=512, out_features=256, bias=True)\n",
            "        (final_layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
            "      )\n",
            "      (2): TransformerDecoderLayerBase(\n",
            "        (dropout_module): FairseqDropout()\n",
            "        (self_attn): MultiheadAttention(\n",
            "          (dropout_module): FairseqDropout()\n",
            "          (k_proj): Linear(in_features=256, out_features=256, bias=True)\n",
            "          (v_proj): Linear(in_features=256, out_features=256, bias=True)\n",
            "          (q_proj): Linear(in_features=256, out_features=256, bias=True)\n",
            "          (out_proj): Linear(in_features=256, out_features=256, bias=True)\n",
            "        )\n",
            "        (activation_dropout_module): FairseqDropout()\n",
            "        (self_attn_layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
            "        (encoder_attn): MultiheadAttention(\n",
            "          (dropout_module): FairseqDropout()\n",
            "          (k_proj): Linear(in_features=256, out_features=256, bias=True)\n",
            "          (v_proj): Linear(in_features=256, out_features=256, bias=True)\n",
            "          (q_proj): Linear(in_features=256, out_features=256, bias=True)\n",
            "          (out_proj): Linear(in_features=256, out_features=256, bias=True)\n",
            "        )\n",
            "        (encoder_attn_layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
            "        (fc1): Linear(in_features=256, out_features=512, bias=True)\n",
            "        (fc2): Linear(in_features=512, out_features=256, bias=True)\n",
            "        (final_layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
            "      )\n",
            "    )\n",
            "    (output_projection): Linear(in_features=256, out_features=13456, bias=False)\n",
            "  )\n",
            ")\n",
            "2022-03-10 06:05:08 | INFO | fairseq_cli.train | task: TranslationTask\n",
            "2022-03-10 06:05:08 | INFO | fairseq_cli.train | model: TransformerModel\n",
            "2022-03-10 06:05:08 | INFO | fairseq_cli.train | criterion: CrossEntropyCriterion\n",
            "2022-03-10 06:05:08 | INFO | fairseq_cli.train | num. shared model params: 16,269,312 (num. trained: 16,269,312)\n",
            "2022-03-10 06:05:08 | INFO | fairseq_cli.train | num. expert model params: 0 (num. trained: 0)\n",
            "2022-03-10 06:05:08 | INFO | fairseq.data.data_utils | loaded 1,000 examples from: /content/content/nmt-kn-ta/tokenized.kn-ta/valid.kn-ta.kn\n",
            "2022-03-10 06:05:08 | INFO | fairseq.data.data_utils | loaded 1,000 examples from: /content/content/nmt-kn-ta/tokenized.kn-ta/valid.kn-ta.ta\n",
            "2022-03-10 06:05:08 | INFO | fairseq.tasks.translation | /content/content/nmt-kn-ta/tokenized.kn-ta valid kn-ta 1000 examples\n",
            "2022-03-10 06:05:17 | INFO | fairseq.utils | ***********************CUDA enviroments for all 1 workers***********************\n",
            "2022-03-10 06:05:17 | INFO | fairseq.utils | rank   0: capabilities =  3.7  ; total memory = 11.173 GB ; name = Tesla K80                               \n",
            "2022-03-10 06:05:17 | INFO | fairseq.utils | ***********************CUDA enviroments for all 1 workers***********************\n",
            "2022-03-10 06:05:17 | INFO | fairseq_cli.train | training on 1 devices (GPUs/TPUs)\n",
            "2022-03-10 06:05:17 | INFO | fairseq_cli.train | max tokens per device = None and max sentences per device = 128\n",
            "2022-03-10 06:05:17 | INFO | fairseq.trainer | Preparing to load checkpoint checkpoints/checkpoint_last.pt\n",
            "2022-03-10 06:05:17 | INFO | fairseq.trainer | No existing checkpoint found checkpoints/checkpoint_last.pt\n",
            "2022-03-10 06:05:17 | INFO | fairseq.trainer | loading train data for epoch 1\n",
            "2022-03-10 06:05:17 | INFO | fairseq.data.data_utils | loaded 88,752 examples from: /content/content/nmt-kn-ta/tokenized.kn-ta/train.kn-ta.kn\n",
            "2022-03-10 06:05:17 | INFO | fairseq.data.data_utils | loaded 88,752 examples from: /content/content/nmt-kn-ta/tokenized.kn-ta/train.kn-ta.ta\n",
            "2022-03-10 06:05:17 | INFO | fairseq.tasks.translation | /content/content/nmt-kn-ta/tokenized.kn-ta train kn-ta 88752 examples\n",
            "2022-03-10 06:05:17 | INFO | fairseq.data.iterators | grouped total_num_itrs = 694\n",
            "epoch 001:   0% 0/694 [00:00<?, ?it/s]\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mrahul2023_usa\u001b[0m (use `wandb login --relogin` to force relogin)\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.12.11\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in \u001b[35m\u001b[1m/content/wandb/run-20220310_060517-3q2aylep\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run \u001b[1m`wandb offline`\u001b[0m to turn off syncing.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33mcheckpoints\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: â­ï¸ View project at \u001b[34m\u001b[4mhttps://wandb.ai/rahul2023_usa/Transformer%20from%20scratch%20-%2010%20March%20-%20kn%20to%20ta\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: ðŸš€ View run at \u001b[34m\u001b[4mhttps://wandb.ai/rahul2023_usa/Transformer%20from%20scratch%20-%2010%20March%20-%20kn%20to%20ta/runs/3q2aylep\u001b[0m\n",
            "2022-03-10 06:05:21 | INFO | fairseq.trainer | begin training epoch 1\n",
            "2022-03-10 06:05:21 | INFO | fairseq_cli.train | Start iterating over samples\n",
            "/content/fairseq/fairseq/utils.py:375: UserWarning: amp_C fused kernels unavailable, disabling multi_tensor_l2norm; you may get better performance by installing NVIDIA's apex library\n",
            "  \"amp_C fused kernels unavailable, disabling multi_tensor_l2norm; \"\n",
            "epoch 001:  13% 92/694 [00:19<02:23,  4.20it/s]2022-03-10 06:05:37 | WARNING | fairseq.trainer | OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 1.93 GiB (GPU 0; 11.17 GiB total capacity; 8.11 GiB already allocated; 1.31 GiB free; 9.34 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
            "2022-03-10 06:05:37 | WARNING | fairseq.trainer | |===========================================================================|\n",
            "|                  PyTorch CUDA memory summary, device ID 0                 |\n",
            "|---------------------------------------------------------------------------|\n",
            "|            CUDA OOMs: 1            |        cudaMalloc retries: 1         |\n",
            "|===========================================================================|\n",
            "|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Allocated memory      |    8262 MB |    8299 MB |  200389 MB |  192127 MB |\n",
            "|       from large pool |    8212 MB |    8249 MB |  191139 MB |  182927 MB |\n",
            "|       from small pool |      50 MB |      60 MB |    9250 MB |    9200 MB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Active memory         |    8262 MB |    8299 MB |  200389 MB |  192127 MB |\n",
            "|       from large pool |    8212 MB |    8249 MB |  191139 MB |  182927 MB |\n",
            "|       from small pool |      50 MB |      60 MB |    9250 MB |    9200 MB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| GPU reserved memory   |    9564 MB |    9636 MB |    9986 MB |  432128 KB |\n",
            "|       from large pool |    9506 MB |    9506 MB |    9842 MB |  344064 KB |\n",
            "|       from small pool |      58 MB |     130 MB |     144 MB |   88064 KB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Non-releasable memory |    1301 MB |    1451 MB |  203349 MB |  202047 MB |\n",
            "|       from large pool |    1293 MB |    1441 MB |  193168 MB |  191874 MB |\n",
            "|       from small pool |       7 MB |      12 MB |   10180 MB |   10172 MB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Allocations           |     591    |     593    |  103953    |  103362    |\n",
            "|       from large pool |     162    |     163    |   35126    |   34964    |\n",
            "|       from small pool |     429    |     514    |   68827    |   68398    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Active allocs         |     591    |     593    |  103953    |  103362    |\n",
            "|       from large pool |     162    |     163    |   35126    |   34964    |\n",
            "|       from small pool |     429    |     514    |   68827    |   68398    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| GPU reserved segments |      65    |     101    |     114    |      49    |\n",
            "|       from large pool |      36    |      36    |      42    |       6    |\n",
            "|       from small pool |      29    |      65    |      72    |      43    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Non-releasable allocs |      55    |      55    |   45333    |   45278    |\n",
            "|       from large pool |      30    |      30    |   21731    |   21701    |\n",
            "|       from small pool |      25    |      29    |   23602    |   23577    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Oversize allocations  |       0    |       0    |       0    |       0    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Oversize GPU segments |       0    |       0    |       0    |       0    |\n",
            "|===========================================================================|\n",
            "\n",
            "2022-03-10 06:05:37 | WARNING | fairseq.trainer | attempting to recover from OOM in forward/backward pass\n",
            "epoch 001:  33% 226/694 [00:43<01:05,  7.10it/s, loss=6.124, ppl=69.72, wps=14175.8, ups=6.11, wpb=2321.8, bsz=128, num_updates=200, lr=0.0005, gnorm=0.958, train_wall=16, gb_free=6.7, wall=38]2022-03-10 06:06:01 | WARNING | fairseq.trainer | OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 1.56 GiB (GPU 0; 11.17 GiB total capacity; 7.87 GiB already allocated; 1.46 GiB free; 9.20 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
            "2022-03-10 06:06:01 | WARNING | fairseq.trainer | |===========================================================================|\n",
            "|                  PyTorch CUDA memory summary, device ID 0                 |\n",
            "|---------------------------------------------------------------------------|\n",
            "|            CUDA OOMs: 2            |        cudaMalloc retries: 4         |\n",
            "|===========================================================================|\n",
            "|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Allocated memory      |    8060 MB |    8126 MB |  496420 MB |  488360 MB |\n",
            "|       from large pool |    8011 MB |    8077 MB |  472422 MB |  464410 MB |\n",
            "|       from small pool |      48 MB |      60 MB |   23998 MB |   23949 MB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Active memory         |    8060 MB |    8126 MB |  496420 MB |  488360 MB |\n",
            "|       from large pool |    8011 MB |    8077 MB |  472422 MB |  464410 MB |\n",
            "|       from small pool |      48 MB |      60 MB |   23998 MB |   23949 MB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| GPU reserved memory   |    9416 MB |   10514 MB |   18568 MB |    9152 MB |\n",
            "|       from large pool |    9358 MB |   10376 MB |   18270 MB |    8912 MB |\n",
            "|       from small pool |      58 MB |     138 MB |     298 MB |     240 MB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Non-releasable memory |    1355 MB |    1355 MB |  504787 MB |  503432 MB |\n",
            "|       from large pool |    1346 MB |    1346 MB |  478577 MB |  477231 MB |\n",
            "|       from small pool |       9 MB |      33 MB |   26210 MB |   26200 MB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Allocations           |     591    |     593    |  254688    |  254097    |\n",
            "|       from large pool |     160    |     164    |   84905    |   84745    |\n",
            "|       from small pool |     431    |     514    |  169783    |  169352    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Active allocs         |     591    |     593    |  254688    |  254097    |\n",
            "|       from large pool |     160    |     164    |   84905    |   84745    |\n",
            "|       from small pool |     431    |     514    |  169783    |  169352    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| GPU reserved segments |      63    |     104    |     197    |     134    |\n",
            "|       from large pool |      34    |      35    |      48    |      14    |\n",
            "|       from small pool |      29    |      69    |     149    |     120    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Non-releasable allocs |      47    |      58    |  113552    |  113505    |\n",
            "|       from large pool |      19    |      19    |   52723    |   52704    |\n",
            "|       from small pool |      28    |      54    |   60829    |   60801    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Oversize allocations  |       0    |       0    |       0    |       0    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Oversize GPU segments |       0    |       0    |       0    |       0    |\n",
            "|===========================================================================|\n",
            "\n",
            "2022-03-10 06:06:01 | WARNING | fairseq.trainer | attempting to recover from OOM in forward/backward pass\n",
            "epoch 001:  35% 240/694 [00:46<01:30,  5.00it/s, loss=6.124, ppl=69.72, wps=14175.8, ups=6.11, wpb=2321.8, bsz=128, num_updates=200, lr=0.0005, gnorm=0.958, train_wall=16, gb_free=6.7, wall=38]2022-03-10 06:06:04 | WARNING | fairseq.trainer | OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 1.38 GiB (GPU 0; 11.17 GiB total capacity; 8.39 GiB already allocated; 1.37 GiB free; 9.29 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
            "2022-03-10 06:06:04 | WARNING | fairseq.trainer | |===========================================================================|\n",
            "|                  PyTorch CUDA memory summary, device ID 0                 |\n",
            "|---------------------------------------------------------------------------|\n",
            "|            CUDA OOMs: 3            |        cudaMalloc retries: 5         |\n",
            "|===========================================================================|\n",
            "|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Allocated memory      |    7181 MB |    8593 MB |  548969 MB |  541788 MB |\n",
            "|       from large pool |    7132 MB |    8545 MB |  523526 MB |  516393 MB |\n",
            "|       from small pool |      48 MB |      60 MB |   25442 MB |   25394 MB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Active memory         |    7181 MB |    8593 MB |  548969 MB |  541788 MB |\n",
            "|       from large pool |    7132 MB |    8545 MB |  523526 MB |  516393 MB |\n",
            "|       from small pool |      48 MB |      60 MB |   25442 MB |   25394 MB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| GPU reserved memory   |    9508 MB |   10904 MB |   20056 MB |   10548 MB |\n",
            "|       from large pool |    9450 MB |   10772 MB |   19684 MB |   10234 MB |\n",
            "|       from small pool |      58 MB |     132 MB |     372 MB |     314 MB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Non-releasable memory |     912 MB |    1237 MB |  557557 MB |  556644 MB |\n",
            "|       from large pool |     903 MB |    1227 MB |  529794 MB |  528891 MB |\n",
            "|       from small pool |       9 MB |      20 MB |   27762 MB |   27753 MB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Allocations           |     590    |     593    |  269709    |  269119    |\n",
            "|       from large pool |     160    |     164    |   90032    |   89872    |\n",
            "|       from small pool |     430    |     514    |  179677    |  179247    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Active allocs         |     590    |     593    |  269709    |  269119    |\n",
            "|       from large pool |     160    |     164    |   90032    |   89872    |\n",
            "|       from small pool |     430    |     514    |  179677    |  179247    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| GPU reserved segments |      62    |     101    |     235    |     173    |\n",
            "|       from large pool |      33    |      35    |      49    |      16    |\n",
            "|       from small pool |      29    |      66    |     186    |     157    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Non-releasable allocs |      44    |      51    |  120230    |  120186    |\n",
            "|       from large pool |      17    |      18    |   55766    |   55749    |\n",
            "|       from small pool |      27    |      47    |   64464    |   64437    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Oversize allocations  |       0    |       0    |       0    |       0    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Oversize GPU segments |       0    |       0    |       0    |       0    |\n",
            "|===========================================================================|\n",
            "\n",
            "2022-03-10 06:06:04 | WARNING | fairseq.trainer | attempting to recover from OOM in forward/backward pass\n",
            "epoch 001:  73% 505/694 [01:33<00:30,  6.22it/s, loss=5.319, ppl=39.93, wps=13971.7, ups=5.33, wpb=2619.9, bsz=128, num_updates=500, lr=0.0005, gnorm=0.847, train_wall=18, gb_free=10.6, wall=93]2022-03-10 06:06:51 | WARNING | fairseq.trainer | OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 1.78 GiB (GPU 0; 11.17 GiB total capacity; 7.62 GiB already allocated; 1.56 GiB free; 9.09 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
            "2022-03-10 06:06:51 | WARNING | fairseq.trainer | |===========================================================================|\n",
            "|                  PyTorch CUDA memory summary, device ID 0                 |\n",
            "|---------------------------------------------------------------------------|\n",
            "|            CUDA OOMs: 4            |        cudaMalloc retries: 7         |\n",
            "|===========================================================================|\n",
            "|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Allocated memory      |    7805 MB |    7818 MB |    1106 GB |    1099 GB |\n",
            "|       from large pool |    7756 MB |    7769 MB |    1055 GB |    1047 GB |\n",
            "|       from small pool |      49 MB |      60 MB |      51 GB |      51 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Active memory         |    7805 MB |    7818 MB |    1106 GB |    1099 GB |\n",
            "|       from large pool |    7756 MB |    7769 MB |    1055 GB |    1047 GB |\n",
            "|       from small pool |      49 MB |      60 MB |      51 GB |      51 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| GPU reserved memory   |    9312 MB |    9312 MB |   22950 MB |   13638 MB |\n",
            "|       from large pool |    9252 MB |    9252 MB |   22498 MB |   13246 MB |\n",
            "|       from small pool |      60 MB |     138 MB |     452 MB |     392 MB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Non-releasable memory |    1506 MB |    2105 MB |    1152 GB |    1150 GB |\n",
            "|       from large pool |    1495 MB |    2094 MB |    1096 GB |    1094 GB |\n",
            "|       from small pool |      10 MB |      18 MB |      56 GB |      56 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Allocations           |     590    |     593    |  565684    |  565094    |\n",
            "|       from large pool |     162    |     164    |  190029    |  189867    |\n",
            "|       from small pool |     428    |     514    |  375655    |  375227    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Active allocs         |     590    |     593    |  565684    |  565094    |\n",
            "|       from large pool |     162    |     164    |  190029    |  189867    |\n",
            "|       from small pool |     428    |     514    |  375655    |  375227    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| GPU reserved segments |      63    |     102    |     277    |     214    |\n",
            "|       from large pool |      33    |      33    |      51    |      18    |\n",
            "|       from small pool |      30    |      69    |     226    |     196    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Non-releasable allocs |      50    |      50    |  252156    |  252106    |\n",
            "|       from large pool |      19    |      19    |  117916    |  117897    |\n",
            "|       from small pool |      31    |      36    |  134240    |  134209    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Oversize allocations  |       0    |       0    |       0    |       0    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Oversize GPU segments |       0    |       0    |       0    |       0    |\n",
            "|===========================================================================|\n",
            "\n",
            "2022-03-10 06:06:51 | WARNING | fairseq.trainer | attempting to recover from OOM in forward/backward pass\n",
            "epoch 001:  82% 566/694 [01:47<00:35,  3.60it/s, loss=5.319, ppl=39.93, wps=13971.7, ups=5.33, wpb=2619.9, bsz=128, num_updates=500, lr=0.0005, gnorm=0.847, train_wall=18, gb_free=10.6, wall=93]2022-03-10 06:07:05 | WARNING | fairseq.trainer | OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 1.43 GiB (GPU 0; 11.17 GiB total capacity; 8.46 GiB already allocated; 1.39 GiB free; 9.26 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
            "2022-03-10 06:07:05 | WARNING | fairseq.trainer | |===========================================================================|\n",
            "|                  PyTorch CUDA memory summary, device ID 0                 |\n",
            "|---------------------------------------------------------------------------|\n",
            "|            CUDA OOMs: 5            |        cudaMalloc retries: 10        |\n",
            "|===========================================================================|\n",
            "|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Allocated memory      |    7196 MB |    8662 MB |    1297 GB |    1290 GB |\n",
            "|       from large pool |    7148 MB |    8614 MB |    1239 GB |    1232 GB |\n",
            "|       from small pool |      48 MB |      60 MB |      57 GB |      57 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Active memory         |    7196 MB |    8662 MB |    1297 GB |    1290 GB |\n",
            "|       from large pool |    7148 MB |    8614 MB |    1239 GB |    1232 GB |\n",
            "|       from small pool |      48 MB |      60 MB |      57 GB |      57 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| GPU reserved memory   |    9486 MB |   10548 MB |   27726 MB |   18240 MB |\n",
            "|       from large pool |    9428 MB |   10440 MB |   27146 MB |   17718 MB |\n",
            "|       from small pool |      58 MB |     108 MB |     580 MB |     522 MB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Non-releasable memory |     823 MB |    1005 MB |    1353 GB |    1352 GB |\n",
            "|       from large pool |     813 MB |     995 MB |    1290 GB |    1289 GB |\n",
            "|       from small pool |       9 MB |      23 MB |      62 GB |      62 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Allocations           |     590    |     593    |  633783    |  633193    |\n",
            "|       from large pool |     160    |     164    |  213236    |  213076    |\n",
            "|       from small pool |     430    |     514    |  420547    |  420117    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Active allocs         |     590    |     593    |  633783    |  633193    |\n",
            "|       from large pool |     160    |     164    |  213236    |  213076    |\n",
            "|       from small pool |     430    |     514    |  420547    |  420117    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| GPU reserved segments |      62    |      88    |     345    |     283    |\n",
            "|       from large pool |      33    |      34    |      55    |      22    |\n",
            "|       from small pool |      29    |      54    |     290    |     261    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Non-releasable allocs |      47    |      50    |  282562    |  282515    |\n",
            "|       from large pool |      21    |      21    |  132018    |  131997    |\n",
            "|       from small pool |      26    |      43    |  150544    |  150518    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Oversize allocations  |       0    |       0    |       0    |       0    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Oversize GPU segments |       0    |       0    |       0    |       0    |\n",
            "|===========================================================================|\n",
            "\n",
            "2022-03-10 06:07:05 | WARNING | fairseq.trainer | attempting to recover from OOM in forward/backward pass\n",
            "epoch 001: 100% 693/694 [02:13<00:00,  6.35it/s, loss=5.251, ppl=38.08, wps=12590.4, ups=4.3, wpb=2925.4, bsz=128, num_updates=600, lr=0.0005, gnorm=0.904, train_wall=21, gb_free=10.5, wall=116]2022-03-10 06:07:31 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "\n",
            "epoch 001 | valid on 'valid' subset:   0% 0/8 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 001 | valid on 'valid' subset:  12% 1/8 [00:00<00:00,  7.37it/s]\u001b[A\n",
            "epoch 001 | valid on 'valid' subset:  38% 3/8 [00:00<00:00, 12.82it/s]\u001b[A\n",
            "epoch 001 | valid on 'valid' subset:  62% 5/8 [00:00<00:00, 12.07it/s]\u001b[A\n",
            "epoch 001 | valid on 'valid' subset:  88% 7/8 [00:00<00:00, 10.18it/s]\u001b[A\n",
            "                                                                      \u001b[A2022-03-10 06:07:32 | INFO | valid | epoch 001 | valid on 'valid' subset | loss 4.806 | ppl 27.98 | wps 26313.2 | wpb 2673.1 | bsz 125 | num_updates 689\n",
            "2022-03-10 06:07:32 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 1 @ 689 updates\n",
            "2022-03-10 06:07:32 | INFO | fairseq.trainer | Saving checkpoint to /content/checkpoints/checkpoint1.pt\n",
            "2022-03-10 06:07:32 | INFO | fairseq.trainer | Finished saving checkpoint to /content/checkpoints/checkpoint1.pt\n",
            "epoch 001: 100% 694/694 [02:16<00:00,  1.03it/s, loss=5.251, ppl=38.08, wps=12590.4, ups=4.3, wpb=2925.4, bsz=128, num_updates=600, lr=0.0005, gnorm=0.904, train_wall=21, gb_free=10.5, wall=116]2022-03-10 06:07:34 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/checkpoint1.pt (epoch 1 @ 689 updates, score 4.806) (writing took 1.981151404000002 seconds)\n",
            "2022-03-10 06:07:34 | INFO | fairseq_cli.train | end of epoch 1 (average epoch stats below)\n",
            "2022-03-10 06:07:34 | INFO | train | epoch 001 | loss 5.745 | ppl 53.63 | wps 13360.2 | ups 5.21 | wpb 2564 | bsz 127.9 | num_updates 689 | lr 0.0005 | gnorm 1.039 | train_wall 124 | gb_free 10.6 | wall 137\n",
            "2022-03-10 06:07:34 | INFO | fairseq.data.iterators | grouped total_num_itrs = 694\n",
            "epoch 002:   0% 0/694 [00:00<?, ?it/s]2022-03-10 06:07:34 | INFO | fairseq.trainer | begin training epoch 2\n",
            "2022-03-10 06:07:34 | INFO | fairseq_cli.train | Start iterating over samples\n",
            "epoch 002:   1% 7/694 [00:01<01:33,  7.37it/s]2022-03-10 06:07:35 | WARNING | fairseq.trainer | OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 1.56 GiB (GPU 0; 11.17 GiB total capacity; 9.43 GiB already allocated; 93.81 MiB free; 10.56 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
            "2022-03-10 06:07:35 | WARNING | fairseq.trainer | |===========================================================================|\n",
            "|                  PyTorch CUDA memory summary, device ID 0                 |\n",
            "|---------------------------------------------------------------------------|\n",
            "|            CUDA OOMs: 6            |        cudaMalloc retries: 13        |\n",
            "|===========================================================================|\n",
            "|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Allocated memory      |    8058 MB |    9655 MB |    1667 GB |    1659 GB |\n",
            "|       from large pool |    8009 MB |    9606 MB |    1596 GB |    1588 GB |\n",
            "|       from small pool |      48 MB |      60 MB |      70 GB |      70 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Active memory         |    8058 MB |    9655 MB |    1667 GB |    1659 GB |\n",
            "|       from large pool |    8009 MB |    9606 MB |    1596 GB |    1588 GB |\n",
            "|       from small pool |      48 MB |      60 MB |      70 GB |      70 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| GPU reserved memory   |   10814 MB |   10814 MB |   34924 MB |   24110 MB |\n",
            "|       from large pool |   10752 MB |   10752 MB |   34198 MB |   23446 MB |\n",
            "|       from small pool |      62 MB |     134 MB |     726 MB |     664 MB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Non-releasable memory |    1157 MB |    1380 MB |    1736 GB |    1735 GB |\n",
            "|       from large pool |    1144 MB |    1366 MB |    1659 GB |    1658 GB |\n",
            "|       from small pool |      13 MB |      20 MB |      77 GB |      77 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Allocations           |     592    |     595    |     787 K  |     786 K  |\n",
            "|       from large pool |     160    |     164    |     266 K  |     266 K  |\n",
            "|       from small pool |     432    |     516    |     520 K  |     520 K  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Active allocs         |     592    |     595    |     787 K  |     786 K  |\n",
            "|       from large pool |     160    |     164    |     266 K  |     266 K  |\n",
            "|       from small pool |     432    |     516    |     520 K  |     520 K  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| GPU reserved segments |      65    |     101    |     424    |     359    |\n",
            "|       from large pool |      34    |      34    |      61    |      27    |\n",
            "|       from small pool |      31    |      67    |     363    |     332    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Non-releasable allocs |      50    |      52    |  350885    |  350835    |\n",
            "|       from large pool |      19    |      20    |  164487    |  164468    |\n",
            "|       from small pool |      31    |      46    |  186398    |  186367    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Oversize allocations  |       0    |       0    |       0    |       0    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Oversize GPU segments |       0    |       0    |       0    |       0    |\n",
            "|===========================================================================|\n",
            "\n",
            "2022-03-10 06:07:35 | WARNING | fairseq.trainer | attempting to recover from OOM in forward/backward pass\n",
            "epoch 002:   4% 30/694 [00:05<01:58,  5.60it/s, loss=4.943, ppl=30.76, wps=11432.5, ups=4.32, wpb=2645, bsz=128, num_updates=700, lr=0.0005, gnorm=0.865, train_wall=19, gb_free=10.4, wall=139]2022-03-10 06:07:40 | WARNING | fairseq.trainer | OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 1.93 GiB (GPU 0; 11.17 GiB total capacity; 8.11 GiB already allocated; 1.66 GiB free; 9.00 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
            "2022-03-10 06:07:40 | WARNING | fairseq.trainer | |===========================================================================|\n",
            "|                  PyTorch CUDA memory summary, device ID 0                 |\n",
            "|---------------------------------------------------------------------------|\n",
            "|            CUDA OOMs: 7            |        cudaMalloc retries: 14        |\n",
            "|===========================================================================|\n",
            "|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Allocated memory      |    8263 MB |    8300 MB |    1723 GB |    1715 GB |\n",
            "|       from large pool |    8213 MB |    8250 MB |    1650 GB |    1642 GB |\n",
            "|       from small pool |      50 MB |      60 MB |      72 GB |      72 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Active memory         |    8263 MB |    8300 MB |    1723 GB |    1715 GB |\n",
            "|       from large pool |    8213 MB |    8250 MB |    1650 GB |    1642 GB |\n",
            "|       from small pool |      50 MB |      60 MB |      72 GB |      72 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| GPU reserved memory   |    9212 MB |    9286 MB |   34994 MB |   25782 MB |\n",
            "|       from large pool |    9154 MB |    9154 MB |   34198 MB |   25044 MB |\n",
            "|       from small pool |      58 MB |     132 MB |     796 MB |     738 MB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Non-releasable memory |     948 MB |    1849 MB |    1798 GB |    1798 GB |\n",
            "|       from large pool |     940 MB |    1840 MB |    1719 GB |    1718 GB |\n",
            "|       from small pool |       7 MB |      18 MB |      79 GB |      79 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Allocations           |     593    |     595    |     812 K  |     811 K  |\n",
            "|       from large pool |     162    |     163    |     274 K  |     274 K  |\n",
            "|       from small pool |     431    |     516    |     537 K  |     537 K  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Active allocs         |     593    |     595    |     812 K  |     811 K  |\n",
            "|       from large pool |     162    |     163    |     274 K  |     274 K  |\n",
            "|       from small pool |     431    |     516    |     537 K  |     537 K  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| GPU reserved segments |      62    |      99    |     459    |     397    |\n",
            "|       from large pool |      33    |      33    |      61    |      28    |\n",
            "|       from small pool |      29    |      66    |     398    |     369    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Non-releasable allocs |      54    |      54    |  362042    |  361988    |\n",
            "|       from large pool |      27    |      27    |  169540    |  169513    |\n",
            "|       from small pool |      27    |      44    |  192502    |  192475    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Oversize allocations  |       0    |       0    |       0    |       0    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Oversize GPU segments |       0    |       0    |       0    |       0    |\n",
            "|===========================================================================|\n",
            "\n",
            "2022-03-10 06:07:40 | WARNING | fairseq.trainer | attempting to recover from OOM in forward/backward pass\n",
            "epoch 002:  35% 244/694 [00:42<01:03,  7.10it/s, loss=4.372, ppl=20.7, wps=14374.8, ups=6.17, wpb=2330.8, bsz=128, num_updates=900, lr=0.0005, gnorm=0.906, train_wall=16, gb_free=10.4, wall=174]2022-03-10 06:08:17 | WARNING | fairseq.trainer | OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 1.38 GiB (GPU 0; 11.17 GiB total capacity; 8.39 GiB already allocated; 1.37 GiB free; 9.29 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
            "2022-03-10 06:08:17 | WARNING | fairseq.trainer | |===========================================================================|\n",
            "|                  PyTorch CUDA memory summary, device ID 0                 |\n",
            "|---------------------------------------------------------------------------|\n",
            "|            CUDA OOMs: 8            |        cudaMalloc retries: 15        |\n",
            "|===========================================================================|\n",
            "|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Allocated memory      |    7181 MB |    8593 MB |    2181 GB |    2174 GB |\n",
            "|       from large pool |    7132 MB |    8545 MB |    2086 GB |    2079 GB |\n",
            "|       from small pool |      48 MB |      60 MB |      94 GB |      94 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Active memory         |    7181 MB |    8593 MB |    2181 GB |    2174 GB |\n",
            "|       from large pool |    7132 MB |    8545 MB |    2086 GB |    2079 GB |\n",
            "|       from small pool |      48 MB |      60 MB |      94 GB |      94 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| GPU reserved memory   |    9508 MB |   10706 MB |   36488 MB |   26980 MB |\n",
            "|       from large pool |    9450 MB |   10568 MB |   35612 MB |   26162 MB |\n",
            "|       from small pool |      58 MB |     138 MB |     876 MB |     818 MB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Non-releasable memory |     912 MB |    1001 MB |    2284 GB |    2283 GB |\n",
            "|       from large pool |     903 MB |     992 MB |    2180 GB |    2179 GB |\n",
            "|       from small pool |       9 MB |      13 MB |     103 GB |     103 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Allocations           |     592    |     595    |    1051 K  |    1051 K  |\n",
            "|       from large pool |     160    |     164    |     354 K  |     354 K  |\n",
            "|       from small pool |     432    |     516    |     696 K  |     696 K  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Active allocs         |     592    |     595    |    1051 K  |    1051 K  |\n",
            "|       from large pool |     160    |     164    |     354 K  |     354 K  |\n",
            "|       from small pool |     432    |     516    |     696 K  |     696 K  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| GPU reserved segments |      62    |     103    |     500    |     438    |\n",
            "|       from large pool |      33    |      34    |      62    |      29    |\n",
            "|       from small pool |      29    |      69    |     438    |     409    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Non-releasable allocs |      50    |      52    |  468948    |  468898    |\n",
            "|       from large pool |      18    |      19    |  219334    |  219316    |\n",
            "|       from small pool |      32    |      47    |  249614    |  249582    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Oversize allocations  |       0    |       0    |       0    |       0    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Oversize GPU segments |       0    |       0    |       0    |       0    |\n",
            "|===========================================================================|\n",
            "\n",
            "2022-03-10 06:08:17 | WARNING | fairseq.trainer | attempting to recover from OOM in forward/backward pass\n",
            "epoch 002:  83% 574/694 [01:45<00:27,  4.39it/s, loss=4.179, ppl=18.11, wps=14105.7, ups=5.35, wpb=2638.9, bsz=128, num_updates=1200, lr=0.0005, gnorm=0.905, train_wall=18, gb_free=10.5, wall=230]2022-03-10 06:09:20 | WARNING | fairseq.trainer | OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 1.78 GiB (GPU 0; 11.17 GiB total capacity; 9.33 GiB already allocated; 1.04 GiB free; 9.62 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
            "2022-03-10 06:09:20 | WARNING | fairseq.trainer | |===========================================================================|\n",
            "|                  PyTorch CUDA memory summary, device ID 0                 |\n",
            "|---------------------------------------------------------------------------|\n",
            "|            CUDA OOMs: 9            |        cudaMalloc retries: 21        |\n",
            "|===========================================================================|\n",
            "|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Allocated memory      |    9550 MB |    9626 MB |    2981 GB |    2972 GB |\n",
            "|       from large pool |    9501 MB |    9576 MB |    2853 GB |    2843 GB |\n",
            "|       from small pool |      49 MB |      60 MB |     128 GB |     128 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Active memory         |    9550 MB |    9626 MB |    2981 GB |    2972 GB |\n",
            "|       from large pool |    9501 MB |    9576 MB |    2853 GB |    2843 GB |\n",
            "|       from small pool |      49 MB |      60 MB |     128 GB |     128 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| GPU reserved memory   |    9846 MB |    9846 MB |   50494 MB |   40648 MB |\n",
            "|       from large pool |    9788 MB |    9788 MB |   49296 MB |   39508 MB |\n",
            "|       from small pool |      58 MB |      78 MB |    1198 MB |    1140 MB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Non-releasable memory |  302677 KB |    1374 MB |    3128 GB |    3128 GB |\n",
            "|       from large pool |  293719 KB |    1364 MB |    2988 GB |    2988 GB |\n",
            "|       from small pool |    8958 KB |      16 MB |     140 GB |     140 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Allocations           |     593    |     595    |    1422 K  |    1421 K  |\n",
            "|       from large pool |     160    |     164    |     479 K  |     479 K  |\n",
            "|       from small pool |     433    |     516    |     942 K  |     941 K  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Active allocs         |     593    |     595    |    1422 K  |    1421 K  |\n",
            "|       from large pool |     160    |     164    |     479 K  |     479 K  |\n",
            "|       from small pool |     433    |     516    |     942 K  |     941 K  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| GPU reserved segments |      62    |      72    |     672    |     610    |\n",
            "|       from large pool |      33    |      33    |      73    |      40    |\n",
            "|       from small pool |      29    |      39    |     599    |     570    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Non-releasable allocs |      43    |      44    |  634892    |  634849    |\n",
            "|       from large pool |      18    |      18    |  296330    |  296312    |\n",
            "|       from small pool |      25    |      38    |  338562    |  338537    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Oversize allocations  |       0    |       0    |       0    |       0    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Oversize GPU segments |       0    |       0    |       0    |       0    |\n",
            "|===========================================================================|\n",
            "\n",
            "2022-03-10 06:09:20 | WARNING | fairseq.trainer | attempting to recover from OOM in forward/backward pass\n",
            "epoch 002: 100% 693/694 [02:08<00:00,  2.85it/s, loss=4.023, ppl=16.25, wps=13105.8, ups=5.2, wpb=2521.3, bsz=128, num_updates=1300, lr=0.0005, gnorm=0.877, train_wall=18, gb_free=10.3, wall=249]2022-03-10 06:09:43 | WARNING | fairseq.trainer | OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 1.43 GiB (GPU 0; 11.17 GiB total capacity; 8.46 GiB already allocated; 903.81 MiB free; 9.77 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
            "2022-03-10 06:09:43 | WARNING | fairseq.trainer | |===========================================================================|\n",
            "|                  PyTorch CUDA memory summary, device ID 0                 |\n",
            "|---------------------------------------------------------------------------|\n",
            "|            CUDA OOMs: 10           |        cudaMalloc retries: 23        |\n",
            "|===========================================================================|\n",
            "|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Allocated memory      |    7196 MB |    8662 MB |    3282 GB |    3275 GB |\n",
            "|       from large pool |    7148 MB |    8614 MB |    3142 GB |    3135 GB |\n",
            "|       from small pool |      48 MB |      60 MB |     139 GB |     139 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Active memory         |    7196 MB |    8662 MB |    3282 GB |    3275 GB |\n",
            "|       from large pool |    7148 MB |    8614 MB |    3142 GB |    3135 GB |\n",
            "|       from small pool |      48 MB |      60 MB |     139 GB |     139 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| GPU reserved memory   |   10004 MB |   10004 MB |   52040 MB |   42036 MB |\n",
            "|       from large pool |    9946 MB |    9946 MB |   50762 MB |   40816 MB |\n",
            "|       from small pool |      58 MB |     138 MB |    1278 MB |    1220 MB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Non-releasable memory |    1341 MB |    1341 MB |    3447 GB |    3445 GB |\n",
            "|       from large pool |    1331 MB |    1331 MB |    3294 GB |    3293 GB |\n",
            "|       from small pool |       9 MB |      16 MB |     152 GB |     152 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Allocations           |     592    |     595    |    1555 K  |    1555 K  |\n",
            "|       from large pool |     160    |     164    |     526 K  |     525 K  |\n",
            "|       from small pool |     432    |     516    |    1029 K  |    1029 K  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Active allocs         |     592    |     595    |    1555 K  |    1555 K  |\n",
            "|       from large pool |     160    |     164    |     526 K  |     525 K  |\n",
            "|       from small pool |     432    |     516    |    1029 K  |    1029 K  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| GPU reserved segments |      62    |     102    |     713    |     651    |\n",
            "|       from large pool |      33    |      33    |      74    |      41    |\n",
            "|       from small pool |      29    |      69    |     639    |     610    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Non-releasable allocs |      50    |      50    |  694124    |  694074    |\n",
            "|       from large pool |      21    |      21    |  324424    |  324403    |\n",
            "|       from small pool |      29    |      40    |  369700    |  369671    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Oversize allocations  |       0    |       0    |       0    |       0    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Oversize GPU segments |       0    |       0    |       0    |       0    |\n",
            "|===========================================================================|\n",
            "\n",
            "2022-03-10 06:09:43 | WARNING | fairseq.trainer | attempting to recover from OOM in forward/backward pass\n",
            "2022-03-10 06:09:43 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "\n",
            "epoch 002 | valid on 'valid' subset:   0% 0/8 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 002 | valid on 'valid' subset:  12% 1/8 [00:00<00:01,  6.36it/s]\u001b[A\n",
            "epoch 002 | valid on 'valid' subset:  25% 2/8 [00:00<00:00,  7.80it/s]\u001b[A\n",
            "epoch 002 | valid on 'valid' subset:  50% 4/8 [00:00<00:00, 11.50it/s]\u001b[A\n",
            "epoch 002 | valid on 'valid' subset:  75% 6/8 [00:00<00:00, 10.28it/s]\u001b[A\n",
            "epoch 002 | valid on 'valid' subset: 100% 8/8 [00:00<00:00,  7.96it/s]\u001b[A\n",
            "                                                                      \u001b[A2022-03-10 06:09:44 | INFO | valid | epoch 002 | valid on 'valid' subset | loss 4.13 | ppl 17.51 | wps 25009.3 | wpb 2673.1 | bsz 125 | num_updates 1378 | best_loss 4.13\n",
            "2022-03-10 06:09:44 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 2 @ 1378 updates\n",
            "2022-03-10 06:09:44 | INFO | fairseq.trainer | Saving checkpoint to /content/checkpoints/checkpoint2.pt\n",
            "2022-03-10 06:09:44 | INFO | fairseq.trainer | Finished saving checkpoint to /content/checkpoints/checkpoint2.pt\n",
            "epoch 002: 100% 694/694 [02:12<00:00,  1.27s/it, loss=4.023, ppl=16.25, wps=13105.8, ups=5.2, wpb=2521.3, bsz=128, num_updates=1300, lr=0.0005, gnorm=0.877, train_wall=18, gb_free=10.3, wall=249]2022-03-10 06:09:46 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/checkpoint2.pt (epoch 2 @ 1378 updates, score 4.13) (writing took 1.8995560639999667 seconds)\n",
            "2022-03-10 06:09:46 | INFO | fairseq_cli.train | end of epoch 2 (average epoch stats below)\n",
            "2022-03-10 06:09:46 | INFO | train | epoch 002 | loss 4.273 | ppl 19.33 | wps 13367.3 | ups 5.21 | wpb 2564 | bsz 127.9 | num_updates 1378 | lr 0.0005 | gnorm 0.895 | train_wall 123 | gb_free 10.3 | wall 269\n",
            "epoch 003:   0% 0/694 [00:00<?, ?it/s]2022-03-10 06:09:46 | INFO | fairseq.data.iterators | grouped total_num_itrs = 694\n",
            "2022-03-10 06:09:46 | INFO | fairseq.trainer | begin training epoch 3\n",
            "2022-03-10 06:09:46 | INFO | fairseq_cli.train | Start iterating over samples\n",
            "epoch 003:  12% 85/694 [00:18<02:31,  4.02it/s, loss=4.1, ppl=17.15, wps=11999.3, ups=3.96, wpb=3028.5, bsz=128, num_updates=1400, lr=0.0005, gnorm=0.878, train_wall=21, gb_free=8.6, wall=274]2022-03-10 06:10:05 | WARNING | fairseq.trainer | OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 1.43 GiB (GPU 0; 11.17 GiB total capacity; 8.46 GiB already allocated; 903.81 MiB free; 9.77 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
            "2022-03-10 06:10:05 | WARNING | fairseq.trainer | |===========================================================================|\n",
            "|                  PyTorch CUDA memory summary, device ID 0                 |\n",
            "|---------------------------------------------------------------------------|\n",
            "|            CUDA OOMs: 11           |        cudaMalloc retries: 26        |\n",
            "|===========================================================================|\n",
            "|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Allocated memory      |    7196 MB |    8662 MB |    3553 GB |    3546 GB |\n",
            "|       from large pool |    7148 MB |    8614 MB |    3405 GB |    3398 GB |\n",
            "|       from small pool |      48 MB |      60 MB |     148 GB |     148 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Active memory         |    7196 MB |    8662 MB |    3553 GB |    3546 GB |\n",
            "|       from large pool |    7148 MB |    8614 MB |    3405 GB |    3398 GB |\n",
            "|       from small pool |      48 MB |      60 MB |     148 GB |     148 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| GPU reserved memory   |   10004 MB |   10004 MB |   55972 MB |   45968 MB |\n",
            "|       from large pool |    9946 MB |    9946 MB |   54548 MB |   44602 MB |\n",
            "|       from small pool |      58 MB |     134 MB |    1424 MB |    1366 MB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Non-releasable memory |    1341 MB |    1341 MB |    3740 GB |    3738 GB |\n",
            "|       from large pool |    1331 MB |    1331 MB |    3578 GB |    3576 GB |\n",
            "|       from small pool |       9 MB |      15 MB |     162 GB |     162 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Allocations           |     592    |     595    |    1654 K  |    1653 K  |\n",
            "|       from large pool |     160    |     164    |     560 K  |     560 K  |\n",
            "|       from small pool |     432    |     516    |    1093 K  |    1093 K  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Active allocs         |     592    |     595    |    1654 K  |    1653 K  |\n",
            "|       from large pool |     160    |     164    |     560 K  |     560 K  |\n",
            "|       from small pool |     432    |     516    |    1093 K  |    1093 K  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| GPU reserved segments |      62    |     100    |     789    |     727    |\n",
            "|       from large pool |      33    |      33    |      77    |      44    |\n",
            "|       from small pool |      29    |      67    |     712    |     683    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Non-releasable allocs |      48    |      48    |  737826    |  737778    |\n",
            "|       from large pool |      21    |      21    |  344969    |  344948    |\n",
            "|       from small pool |      27    |      40    |  392857    |  392830    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Oversize allocations  |       0    |       0    |       0    |       0    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Oversize GPU segments |       0    |       0    |       0    |       0    |\n",
            "|===========================================================================|\n",
            "\n",
            "2022-03-10 06:10:05 | WARNING | fairseq.trainer | attempting to recover from OOM in forward/backward pass\n",
            "epoch 003:  57% 395/694 [01:11<00:40,  7.32it/s, loss=3.725, ppl=13.22, wps=13948.1, ups=5.66, wpb=2465.7, bsz=127.2, num_updates=1700, lr=0.0005, gnorm=0.911, train_wall=17, gb_free=10.6, wall=329]2022-03-10 06:10:58 | WARNING | fairseq.trainer | OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 1.38 GiB (GPU 0; 11.17 GiB total capacity; 8.39 GiB already allocated; 1.07 GiB free; 9.59 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
            "2022-03-10 06:10:58 | WARNING | fairseq.trainer | |===========================================================================|\n",
            "|                  PyTorch CUDA memory summary, device ID 0                 |\n",
            "|---------------------------------------------------------------------------|\n",
            "|            CUDA OOMs: 12           |        cudaMalloc retries: 29        |\n",
            "|===========================================================================|\n",
            "|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Allocated memory      |    7181 MB |    8593 MB |    4212 GB |    4205 GB |\n",
            "|       from large pool |    7132 MB |    8545 MB |    4032 GB |    4025 GB |\n",
            "|       from small pool |      48 MB |      60 MB |     180 GB |     180 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Active memory         |    7181 MB |    8593 MB |    4212 GB |    4205 GB |\n",
            "|       from large pool |    7132 MB |    8545 MB |    4032 GB |    4025 GB |\n",
            "|       from small pool |      48 MB |      60 MB |     180 GB |     180 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| GPU reserved memory   |    9816 MB |    9816 MB |   60800 MB |   50984 MB |\n",
            "|       from large pool |    9756 MB |    9756 MB |   59216 MB |   49460 MB |\n",
            "|       from small pool |      60 MB |     138 MB |    1584 MB |    1524 MB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Non-releasable memory |    1220 MB |    1222 MB |    4427 GB |    4426 GB |\n",
            "|       from large pool |    1209 MB |    1210 MB |    4230 GB |    4229 GB |\n",
            "|       from small pool |      11 MB |      20 MB |     197 GB |     197 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Allocations           |     592    |     595    |    2001 K  |    2000 K  |\n",
            "|       from large pool |     160    |     164    |     675 K  |     674 K  |\n",
            "|       from small pool |     432    |     516    |    1325 K  |    1325 K  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Active allocs         |     592    |     595    |    2001 K  |    2000 K  |\n",
            "|       from large pool |     160    |     164    |     675 K  |     674 K  |\n",
            "|       from small pool |     432    |     516    |    1325 K  |    1325 K  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| GPU reserved segments |      63    |     102    |     873    |     810    |\n",
            "|       from large pool |      33    |      33    |      81    |      48    |\n",
            "|       from small pool |      30    |      69    |     792    |     762    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Non-releasable allocs |      48    |      49    |     893 K  |     893 K  |\n",
            "|       from large pool |      18    |      19    |     416 K  |     416 K  |\n",
            "|       from small pool |      30    |      38    |     477 K  |     477 K  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Oversize allocations  |       0    |       0    |       0    |       0    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Oversize GPU segments |       0    |       0    |       0    |       0    |\n",
            "|===========================================================================|\n",
            "\n",
            "2022-03-10 06:10:58 | WARNING | fairseq.trainer | attempting to recover from OOM in forward/backward pass\n",
            "epoch 003:  58% 401/694 [01:13<00:48,  6.06it/s, loss=3.725, ppl=13.22, wps=13948.1, ups=5.66, wpb=2465.7, bsz=127.2, num_updates=1700, lr=0.0005, gnorm=0.911, train_wall=17, gb_free=10.6, wall=329]2022-03-10 06:11:00 | WARNING | fairseq.trainer | OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 1.78 GiB (GPU 0; 11.17 GiB total capacity; 9.33 GiB already allocated; 687.81 MiB free; 9.98 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
            "2022-03-10 06:11:00 | WARNING | fairseq.trainer | |===========================================================================|\n",
            "|                  PyTorch CUDA memory summary, device ID 0                 |\n",
            "|---------------------------------------------------------------------------|\n",
            "|            CUDA OOMs: 13           |        cudaMalloc retries: 30        |\n",
            "|===========================================================================|\n",
            "|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Allocated memory      |    9550 MB |    9625 MB |    4232 GB |    4222 GB |\n",
            "|       from large pool |    9500 MB |    9576 MB |    4050 GB |    4041 GB |\n",
            "|       from small pool |      49 MB |      60 MB |     181 GB |     181 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Active memory         |    9550 MB |    9625 MB |    4232 GB |    4222 GB |\n",
            "|       from large pool |    9500 MB |    9576 MB |    4050 GB |    4041 GB |\n",
            "|       from small pool |      49 MB |      60 MB |     181 GB |     181 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| GPU reserved memory   |   10220 MB |   10288 MB |   62686 MB |   52466 MB |\n",
            "|       from large pool |   10162 MB |   10162 MB |   61036 MB |   50874 MB |\n",
            "|       from small pool |      58 MB |     126 MB |    1650 MB |    1592 MB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Non-releasable memory |  685977 KB |    1937 MB |    4445 GB |    4444 GB |\n",
            "|       from large pool |  677019 KB |    1927 MB |    4247 GB |    4246 GB |\n",
            "|       from small pool |    8958 KB |      15 MB |     198 GB |     198 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Allocations           |     593    |     595    |    2006 K  |    2006 K  |\n",
            "|       from large pool |     160    |     164    |     677 K  |     676 K  |\n",
            "|       from small pool |     433    |     516    |    1329 K  |    1329 K  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Active allocs         |     593    |     595    |    2006 K  |    2006 K  |\n",
            "|       from large pool |     160    |     164    |     677 K  |     676 K  |\n",
            "|       from small pool |     433    |     516    |    1329 K  |    1329 K  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| GPU reserved segments |      62    |      96    |     907    |     845    |\n",
            "|       from large pool |      33    |      33    |      82    |      49    |\n",
            "|       from small pool |      29    |      63    |     825    |     796    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Non-releasable allocs |      47    |      48    |     896 K  |     896 K  |\n",
            "|       from large pool |      19    |      19    |     417 K  |     417 K  |\n",
            "|       from small pool |      28    |      40    |     478 K  |     478 K  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Oversize allocations  |       0    |       0    |       0    |       0    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Oversize GPU segments |       0    |       0    |       0    |       0    |\n",
            "|===========================================================================|\n",
            "\n",
            "2022-03-10 06:11:00 | WARNING | fairseq.trainer | attempting to recover from OOM in forward/backward pass\n",
            "epoch 003:  64% 445/694 [01:21<00:49,  5.01it/s, loss=3.534, ppl=11.58, wps=14166.3, ups=5.79, wpb=2446, bsz=128, num_updates=1800, lr=0.0005, gnorm=0.877, train_wall=16, gb_free=10.5, wall=347]2022-03-10 06:11:08 | WARNING | fairseq.trainer | OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 1.56 GiB (GPU 0; 11.17 GiB total capacity; 9.43 GiB already allocated; 687.81 MiB free; 9.98 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
            "2022-03-10 06:11:08 | WARNING | fairseq.trainer | |===========================================================================|\n",
            "|                  PyTorch CUDA memory summary, device ID 0                 |\n",
            "|---------------------------------------------------------------------------|\n",
            "|            CUDA OOMs: 14           |        cudaMalloc retries: 31        |\n",
            "|===========================================================================|\n",
            "|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Allocated memory      |    8058 MB |    9655 MB |    4340 GB |    4332 GB |\n",
            "|       from large pool |    8009 MB |    9606 MB |    4155 GB |    4147 GB |\n",
            "|       from small pool |      48 MB |      60 MB |     185 GB |     185 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Active memory         |    8058 MB |    9655 MB |    4340 GB |    4332 GB |\n",
            "|       from large pool |    8009 MB |    9606 MB |    4155 GB |    4147 GB |\n",
            "|       from small pool |      48 MB |      60 MB |     185 GB |     185 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| GPU reserved memory   |   10220 MB |   10292 MB |   62758 MB |   52538 MB |\n",
            "|       from large pool |   10162 MB |   10162 MB |   61036 MB |   50874 MB |\n",
            "|       from small pool |      58 MB |     130 MB |    1722 MB |    1664 MB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Non-releasable memory |    2161 MB |    2161 MB |    4566 GB |    4564 GB |\n",
            "|       from large pool |    2152 MB |    2152 MB |    4364 GB |    4361 GB |\n",
            "|       from small pool |       9 MB |      18 MB |     202 GB |     202 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Allocations           |     592    |     595    |    2055 K  |    2055 K  |\n",
            "|       from large pool |     160    |     164    |     694 K  |     694 K  |\n",
            "|       from small pool |     432    |     516    |    1361 K  |    1361 K  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Active allocs         |     592    |     595    |    2055 K  |    2055 K  |\n",
            "|       from large pool |     160    |     164    |     694 K  |     694 K  |\n",
            "|       from small pool |     432    |     516    |    1361 K  |    1361 K  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| GPU reserved segments |      62    |      98    |     943    |     881    |\n",
            "|       from large pool |      33    |      33    |      82    |      49    |\n",
            "|       from small pool |      29    |      65    |     861    |     832    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Non-releasable allocs |      48    |      49    |     918 K  |     918 K  |\n",
            "|       from large pool |      19    |      19    |     428 K  |     428 K  |\n",
            "|       from small pool |      29    |      43    |     489 K  |     489 K  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Oversize allocations  |       0    |       0    |       0    |       0    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Oversize GPU segments |       0    |       0    |       0    |       0    |\n",
            "|===========================================================================|\n",
            "\n",
            "2022-03-10 06:11:08 | WARNING | fairseq.trainer | attempting to recover from OOM in forward/backward pass\n",
            "epoch 003:  91% 631/694 [01:55<00:11,  5.42it/s, loss=3.626, ppl=12.35, wps=14493.4, ups=5.59, wpb=2594.1, bsz=128, num_updates=2000, lr=0.0005, gnorm=0.908, train_wall=18, gb_free=10.5, wall=384]2022-03-10 06:11:42 | WARNING | fairseq.trainer | OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 1.06 GiB (GPU 0; 11.17 GiB total capacity; 6.74 GiB already allocated; 685.81 MiB free; 9.98 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
            "2022-03-10 06:11:42 | WARNING | fairseq.trainer | |===========================================================================|\n",
            "|                  PyTorch CUDA memory summary, device ID 0                 |\n",
            "|---------------------------------------------------------------------------|\n",
            "|            CUDA OOMs: 15           |        cudaMalloc retries: 32        |\n",
            "|===========================================================================|\n",
            "|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Allocated memory      |    5820 MB |    6905 MB |    4780 GB |    4774 GB |\n",
            "|       from large pool |    5772 MB |    6856 MB |    4577 GB |    4571 GB |\n",
            "|       from small pool |      48 MB |      60 MB |     202 GB |     202 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Active memory         |    5820 MB |    6905 MB |    4780 GB |    4774 GB |\n",
            "|       from large pool |    5772 MB |    6856 MB |    4577 GB |    4571 GB |\n",
            "|       from small pool |      48 MB |      60 MB |     202 GB |     202 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| GPU reserved memory   |   10222 MB |   10300 MB |   62838 MB |   52616 MB |\n",
            "|       from large pool |   10162 MB |   10162 MB |   61036 MB |   50874 MB |\n",
            "|       from small pool |      60 MB |     138 MB |    1802 MB |    1742 MB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Non-releasable memory |    2581 MB |    3316 MB |    5042 GB |    5040 GB |\n",
            "|       from large pool |    2569 MB |    3305 MB |    4820 GB |    4817 GB |\n",
            "|       from small pool |      11 MB |      19 MB |     222 GB |     222 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Allocations           |     592    |     595    |    2264 K  |    2264 K  |\n",
            "|       from large pool |     160    |     164    |     766 K  |     765 K  |\n",
            "|       from small pool |     432    |     516    |    1498 K  |    1498 K  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Active allocs         |     592    |     595    |    2264 K  |    2264 K  |\n",
            "|       from large pool |     160    |     164    |     766 K  |     765 K  |\n",
            "|       from small pool |     432    |     516    |    1498 K  |    1498 K  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| GPU reserved segments |      63    |     102    |     983    |     920    |\n",
            "|       from large pool |      33    |      33    |      82    |      49    |\n",
            "|       from small pool |      30    |      69    |     901    |     871    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Non-releasable allocs |      54    |      56    |    1011 K  |    1011 K  |\n",
            "|       from large pool |      24    |      25    |     472 K  |     472 K  |\n",
            "|       from small pool |      30    |      49    |     538 K  |     538 K  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Oversize allocations  |       0    |       0    |       0    |       0    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Oversize GPU segments |       0    |       0    |       0    |       0    |\n",
            "|===========================================================================|\n",
            "\n",
            "2022-03-10 06:11:42 | WARNING | fairseq.trainer | attempting to recover from OOM in forward/backward pass\n",
            "epoch 003:  96% 668/694 [02:03<00:04,  6.44it/s, loss=3.626, ppl=12.35, wps=14493.4, ups=5.59, wpb=2594.1, bsz=128, num_updates=2000, lr=0.0005, gnorm=0.908, train_wall=18, gb_free=10.5, wall=384]2022-03-10 06:11:50 | WARNING | fairseq.trainer | OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 1.93 GiB (GPU 0; 11.17 GiB total capacity; 10.02 GiB already allocated; 501.81 MiB free; 10.16 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
            "2022-03-10 06:11:50 | WARNING | fairseq.trainer | |===========================================================================|\n",
            "|                  PyTorch CUDA memory summary, device ID 0                 |\n",
            "|---------------------------------------------------------------------------|\n",
            "|            CUDA OOMs: 16           |        cudaMalloc retries: 35        |\n",
            "|===========================================================================|\n",
            "|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Allocated memory      |   10258 MB |   10287 MB |    4869 GB |    4859 GB |\n",
            "|       from large pool |   10208 MB |   10237 MB |    4663 GB |    4653 GB |\n",
            "|       from small pool |      50 MB |      60 MB |     206 GB |     206 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Active memory         |   10258 MB |   10287 MB |    4869 GB |    4859 GB |\n",
            "|       from large pool |   10208 MB |   10237 MB |    4663 GB |    4653 GB |\n",
            "|       from small pool |      50 MB |      60 MB |     206 GB |     206 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| GPU reserved memory   |   10406 MB |   10486 MB |   75548 MB |   65142 MB |\n",
            "|       from large pool |   10348 MB |   10348 MB |   73648 MB |   63300 MB |\n",
            "|       from small pool |      58 MB |     138 MB |    1900 MB |    1842 MB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Non-releasable memory |  150613 KB |  597509 KB |    5129 GB |    5129 GB |\n",
            "|       from large pool |  142502 KB |  587932 KB |    4903 GB |    4903 GB |\n",
            "|       from small pool |    8111 KB |   18822 KB |     225 GB |     225 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Allocations           |     592    |     595    |    2305 K  |    2304 K  |\n",
            "|       from large pool |     162    |     164    |     780 K  |     780 K  |\n",
            "|       from small pool |     430    |     516    |    1525 K  |    1524 K  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Active allocs         |     592    |     595    |    2305 K  |    2304 K  |\n",
            "|       from large pool |     162    |     164    |     780 K  |     780 K  |\n",
            "|       from small pool |     430    |     516    |    1525 K  |    1524 K  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| GPU reserved segments |      96    |     136    |    1088    |     992    |\n",
            "|       from large pool |      67    |      67    |     138    |      71    |\n",
            "|       from small pool |      29    |      69    |     950    |     921    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Non-releasable allocs |      62    |      62    |    1029 K  |    1029 K  |\n",
            "|       from large pool |      32    |      32    |     481 K  |     481 K  |\n",
            "|       from small pool |      30    |      47    |     548 K  |     548 K  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Oversize allocations  |       0    |       0    |       0    |       0    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Oversize GPU segments |       0    |       0    |       0    |       0    |\n",
            "|===========================================================================|\n",
            "\n",
            "2022-03-10 06:11:50 | WARNING | fairseq.trainer | attempting to recover from OOM in forward/backward pass\n",
            "epoch 003: 100% 693/694 [02:08<00:00,  6.28it/s, loss=3.626, ppl=12.35, wps=14493.4, ups=5.59, wpb=2594.1, bsz=128, num_updates=2000, lr=0.0005, gnorm=0.908, train_wall=18, gb_free=10.5, wall=384]2022-03-10 06:11:54 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "\n",
            "epoch 003 | valid on 'valid' subset:   0% 0/8 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 003 | valid on 'valid' subset:  12% 1/8 [00:00<00:00,  7.27it/s]\u001b[A\n",
            "epoch 003 | valid on 'valid' subset:  38% 3/8 [00:00<00:00, 13.13it/s]\u001b[A\n",
            "epoch 003 | valid on 'valid' subset:  62% 5/8 [00:00<00:00, 12.36it/s]\u001b[A\n",
            "epoch 003 | valid on 'valid' subset:  88% 7/8 [00:00<00:00, 10.26it/s]\u001b[A\n",
            "                                                                      \u001b[A2022-03-10 06:11:55 | INFO | valid | epoch 003 | valid on 'valid' subset | loss 3.888 | ppl 14.81 | wps 26362.8 | wpb 2673.1 | bsz 125 | num_updates 2066 | best_loss 3.888\n",
            "2022-03-10 06:11:55 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 3 @ 2066 updates\n",
            "2022-03-10 06:11:55 | INFO | fairseq.trainer | Saving checkpoint to /content/checkpoints/checkpoint3.pt\n",
            "2022-03-10 06:11:56 | INFO | fairseq.trainer | Finished saving checkpoint to /content/checkpoints/checkpoint3.pt\n",
            "2022-03-10 06:11:58 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/checkpoint3.pt (epoch 3 @ 2066 updates, score 3.888) (writing took 2.216969601999949 seconds)\n",
            "2022-03-10 06:11:58 | INFO | fairseq_cli.train | end of epoch 3 (average epoch stats below)\n",
            "2022-03-10 06:11:58 | INFO | train | epoch 003 | loss 3.663 | ppl 12.66 | wps 13327.6 | ups 5.22 | wpb 2553.3 | bsz 127.9 | num_updates 2066 | lr 0.0005 | gnorm 0.892 | train_wall 123 | gb_free 10.6 | wall 401\n",
            "epoch 004:   0% 0/694 [00:00<?, ?it/s]2022-03-10 06:11:58 | INFO | fairseq.data.iterators | grouped total_num_itrs = 694\n",
            "2022-03-10 06:11:58 | INFO | fairseq.trainer | begin training epoch 4\n",
            "2022-03-10 06:11:58 | INFO | fairseq_cli.train | Start iterating over samples\n",
            "epoch 004:   7% 49/694 [00:10<05:58,  1.80it/s, loss=3.395, ppl=10.52, wps=10884.7, ups=4.28, wpb=2544.8, bsz=128, num_updates=2100, lr=0.0005, gnorm=0.871, train_wall=19, gb_free=9.7, wall=407]2022-03-10 06:12:09 | WARNING | fairseq.trainer | OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 1.93 GiB (GPU 0; 11.17 GiB total capacity; 8.11 GiB already allocated; 1.29 GiB free; 9.36 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
            "2022-03-10 06:12:09 | WARNING | fairseq.trainer | |===========================================================================|\n",
            "|                  PyTorch CUDA memory summary, device ID 0                 |\n",
            "|---------------------------------------------------------------------------|\n",
            "|            CUDA OOMs: 17           |        cudaMalloc retries: 39        |\n",
            "|===========================================================================|\n",
            "|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Allocated memory      |    8266 MB |    8304 MB |    5089 GB |    5080 GB |\n",
            "|       from large pool |    8216 MB |    8254 MB |    4874 GB |    4866 GB |\n",
            "|       from small pool |      50 MB |      61 MB |     214 GB |     214 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Active memory         |    8266 MB |    8304 MB |    5089 GB |    5080 GB |\n",
            "|       from large pool |    8216 MB |    8254 MB |    4874 GB |    4866 GB |\n",
            "|       from small pool |      50 MB |      61 MB |     214 GB |     214 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| GPU reserved memory   |    9586 MB |    9592 MB |   85830 MB |   76244 MB |\n",
            "|       from large pool |    9528 MB |    9528 MB |   83758 MB |   74230 MB |\n",
            "|       from small pool |      58 MB |      64 MB |    2072 MB |    2014 MB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Non-releasable memory |    1319 MB |    2347 MB |    5354 GB |    5353 GB |\n",
            "|       from large pool |    1311 MB |    2339 MB |    5119 GB |    5118 GB |\n",
            "|       from small pool |       7 MB |      12 MB |     235 GB |     235 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Allocations           |     593    |     595    |    2391 K  |    2390 K  |\n",
            "|       from large pool |     162    |     163    |     808 K  |     808 K  |\n",
            "|       from small pool |     431    |     516    |    1582 K  |    1581 K  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Active allocs         |     593    |     595    |    2391 K  |    2390 K  |\n",
            "|       from large pool |     162    |     163    |     808 K  |     808 K  |\n",
            "|       from small pool |     431    |     516    |    1582 K  |    1581 K  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| GPU reserved segments |      81    |      84    |    1184    |    1103    |\n",
            "|       from large pool |      52    |      52    |     148    |      96    |\n",
            "|       from small pool |      29    |      32    |    1036    |    1007    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Non-releasable allocs |      62    |      67    |    1068 K  |    1068 K  |\n",
            "|       from large pool |      37    |      41    |     499 K  |     499 K  |\n",
            "|       from small pool |      25    |      29    |     569 K  |     569 K  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Oversize allocations  |       0    |       0    |       0    |       0    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Oversize GPU segments |       0    |       0    |       0    |       0    |\n",
            "|===========================================================================|\n",
            "\n",
            "2022-03-10 06:12:09 | WARNING | fairseq.trainer | attempting to recover from OOM in forward/backward pass\n",
            "epoch 004:  24% 170/694 [00:31<01:31,  5.75it/s, loss=3.278, ppl=9.7, wps=13018, ups=5.02, wpb=2590.7, bsz=128, num_updates=2200, lr=0.0005, gnorm=0.879, train_wall=19, gb_free=10.2, wall=427]2022-03-10 06:12:30 | WARNING | fairseq.trainer | OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 1.56 GiB (GPU 0; 11.17 GiB total capacity; 9.43 GiB already allocated; 603.81 MiB free; 10.06 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
            "2022-03-10 06:12:30 | WARNING | fairseq.trainer | |===========================================================================|\n",
            "|                  PyTorch CUDA memory summary, device ID 0                 |\n",
            "|---------------------------------------------------------------------------|\n",
            "|            CUDA OOMs: 18           |        cudaMalloc retries: 42        |\n",
            "|===========================================================================|\n",
            "|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Allocated memory      |    8062 MB |    9659 MB |    5352 GB |    5344 GB |\n",
            "|       from large pool |    8014 MB |    9610 MB |    5126 GB |    5118 GB |\n",
            "|       from small pool |      48 MB |      60 MB |     226 GB |     226 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Active memory         |    8062 MB |    9659 MB |    5352 GB |    5344 GB |\n",
            "|       from large pool |    8014 MB |    9610 MB |    5126 GB |    5118 GB |\n",
            "|       from small pool |      48 MB |      60 MB |     226 GB |     226 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| GPU reserved memory   |   10304 MB |   10304 MB |   93064 MB |   82760 MB |\n",
            "|       from large pool |   10246 MB |   10246 MB |   90878 MB |   80632 MB |\n",
            "|       from small pool |      58 MB |     108 MB |    2186 MB |    2128 MB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Non-releasable memory |  658468 KB |    1784 MB |    5648 GB |    5647 GB |\n",
            "|       from large pool |  649067 KB |    1774 MB |    5400 GB |    5399 GB |\n",
            "|       from small pool |    9401 KB |      13 MB |     247 GB |     247 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Allocations           |     592    |     595    |    2526 K  |    2525 K  |\n",
            "|       from large pool |     160    |     164    |     855 K  |     855 K  |\n",
            "|       from small pool |     432    |     516    |    1670 K  |    1670 K  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Active allocs         |     592    |     595    |    2526 K  |    2525 K  |\n",
            "|       from large pool |     160    |     164    |     855 K  |     855 K  |\n",
            "|       from small pool |     432    |     516    |    1670 K  |    1670 K  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| GPU reserved segments |      80    |     105    |    1246    |    1166    |\n",
            "|       from large pool |      51    |      51    |     153    |     102    |\n",
            "|       from small pool |      29    |      54    |    1093    |    1064    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Non-releasable allocs |      53    |      54    |    1130 K  |    1130 K  |\n",
            "|       from large pool |      24    |      25    |     529 K  |     529 K  |\n",
            "|       from small pool |      29    |      34    |     600 K  |     600 K  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Oversize allocations  |       0    |       0    |       0    |       0    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Oversize GPU segments |       0    |       0    |       0    |       0    |\n",
            "|===========================================================================|\n",
            "\n",
            "2022-03-10 06:12:30 | WARNING | fairseq.trainer | attempting to recover from OOM in forward/backward pass\n",
            "epoch 004:  29% 204/694 [00:39<02:26,  3.34it/s, loss=3.278, ppl=9.7, wps=13018, ups=5.02, wpb=2590.7, bsz=128, num_updates=2200, lr=0.0005, gnorm=0.879, train_wall=19, gb_free=10.2, wall=427]2022-03-10 06:12:37 | WARNING | fairseq.trainer | OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 1.43 GiB (GPU 0; 11.17 GiB total capacity; 8.46 GiB already allocated; 977.81 MiB free; 9.70 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
            "2022-03-10 06:12:37 | WARNING | fairseq.trainer | |===========================================================================|\n",
            "|                  PyTorch CUDA memory summary, device ID 0                 |\n",
            "|---------------------------------------------------------------------------|\n",
            "|            CUDA OOMs: 19           |        cudaMalloc retries: 44        |\n",
            "|===========================================================================|\n",
            "|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Allocated memory      |    7196 MB |    8662 MB |    5445 GB |    5438 GB |\n",
            "|       from large pool |    7148 MB |    8614 MB |    5216 GB |    5209 GB |\n",
            "|       from small pool |      48 MB |      60 MB |     229 GB |     229 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Active memory         |    7196 MB |    8662 MB |    5445 GB |    5438 GB |\n",
            "|       from large pool |    7148 MB |    8614 MB |    5216 GB |    5209 GB |\n",
            "|       from small pool |      48 MB |      60 MB |     229 GB |     229 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| GPU reserved memory   |    9930 MB |    9930 MB |   95672 MB |   85742 MB |\n",
            "|       from large pool |    9872 MB |    9872 MB |   93410 MB |   83538 MB |\n",
            "|       from small pool |      58 MB |     134 MB |    2262 MB |    2204 MB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Non-releasable memory |    1267 MB |    1267 MB |    5747 GB |    5746 GB |\n",
            "|       from large pool |    1257 MB |    1257 MB |    5496 GB |    5494 GB |\n",
            "|       from small pool |       9 MB |      12 MB |     251 GB |     251 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Allocations           |     592    |     595    |    2564 K  |    2563 K  |\n",
            "|       from large pool |     160    |     164    |     868 K  |     868 K  |\n",
            "|       from small pool |     432    |     516    |    1695 K  |    1694 K  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Active allocs         |     592    |     595    |    2564 K  |    2563 K  |\n",
            "|       from large pool |     160    |     164    |     868 K  |     868 K  |\n",
            "|       from small pool |     432    |     516    |    1695 K  |    1694 K  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| GPU reserved segments |      80    |     118    |    1286    |    1206    |\n",
            "|       from large pool |      51    |      51    |     155    |     104    |\n",
            "|       from small pool |      29    |      67    |    1131    |    1102    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Non-releasable allocs |      61    |      61    |    1147 K  |    1147 K  |\n",
            "|       from large pool |      34    |      34    |     537 K  |     537 K  |\n",
            "|       from small pool |      27    |      32    |     609 K  |     609 K  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Oversize allocations  |       0    |       0    |       0    |       0    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Oversize GPU segments |       0    |       0    |       0    |       0    |\n",
            "|===========================================================================|\n",
            "\n",
            "2022-03-10 06:12:37 | WARNING | fairseq.trainer | attempting to recover from OOM in forward/backward pass\n",
            "epoch 004:  35% 246/694 [00:47<01:07,  6.68it/s, loss=3.286, ppl=9.75, wps=13846.5, ups=5.12, wpb=2706.7, bsz=128, num_updates=2300, lr=0.0005, gnorm=0.859, train_wall=18, gb_free=10.6, wall=446]2022-03-10 06:12:45 | WARNING | fairseq.trainer | OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 1.78 GiB (GPU 0; 11.17 GiB total capacity; 9.33 GiB already allocated; 661.81 MiB free; 10.01 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
            "2022-03-10 06:12:45 | WARNING | fairseq.trainer | |===========================================================================|\n",
            "|                  PyTorch CUDA memory summary, device ID 0                 |\n",
            "|---------------------------------------------------------------------------|\n",
            "|            CUDA OOMs: 20           |        cudaMalloc retries: 45        |\n",
            "|===========================================================================|\n",
            "|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Allocated memory      |    9551 MB |    9627 MB |    5546 GB |    5537 GB |\n",
            "|       from large pool |    9502 MB |    9578 MB |    5313 GB |    5303 GB |\n",
            "|       from small pool |      49 MB |      60 MB |     233 GB |     233 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Active memory         |    9551 MB |    9627 MB |    5546 GB |    5537 GB |\n",
            "|       from large pool |    9502 MB |    9578 MB |    5313 GB |    5303 GB |\n",
            "|       from small pool |      49 MB |      60 MB |     233 GB |     233 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| GPU reserved memory   |   10246 MB |   10364 MB |   97572 MB |   87326 MB |\n",
            "|       from large pool |   10188 MB |   10226 MB |   95230 MB |   85042 MB |\n",
            "|       from small pool |      58 MB |     138 MB |    2342 MB |    2284 MB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Non-releasable memory |  710665 KB |    1506 MB |    5863 GB |    5862 GB |\n",
            "|       from large pool |  701707 KB |    1497 MB |    5608 GB |    5607 GB |\n",
            "|       from small pool |    8958 KB |      20 MB |     255 GB |     255 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Allocations           |     593    |     595    |    2610 K  |    2610 K  |\n",
            "|       from large pool |     160    |     164    |     885 K  |     885 K  |\n",
            "|       from small pool |     433    |     516    |    1725 K  |    1725 K  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Active allocs         |     593    |     595    |    2610 K  |    2610 K  |\n",
            "|       from large pool |     160    |     164    |     885 K  |     885 K  |\n",
            "|       from small pool |     433    |     516    |    1725 K  |    1725 K  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| GPU reserved segments |      79    |     120    |    1327    |    1248    |\n",
            "|       from large pool |      50    |      51    |     156    |     106    |\n",
            "|       from small pool |      29    |      69    |    1171    |    1142    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Non-releasable allocs |      60    |      61    |    1168 K  |    1168 K  |\n",
            "|       from large pool |      33    |      34    |     548 K  |     548 K  |\n",
            "|       from small pool |      27    |      41    |     619 K  |     619 K  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Oversize allocations  |       0    |       0    |       0    |       0    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Oversize GPU segments |       0    |       0    |       0    |       0    |\n",
            "|===========================================================================|\n",
            "\n",
            "2022-03-10 06:12:45 | WARNING | fairseq.trainer | attempting to recover from OOM in forward/backward pass\n",
            "epoch 004:  54% 377/694 [01:12<00:57,  5.48it/s, loss=3.385, ppl=10.45, wps=13901.9, ups=4.85, wpb=2869, bsz=128, num_updates=2400, lr=0.0005, gnorm=0.892, train_wall=20, gb_free=8.7, wall=467]2022-03-10 06:13:11 | WARNING | fairseq.trainer | OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 1.38 GiB (GPU 0; 11.17 GiB total capacity; 8.39 GiB already allocated; 607.81 MiB free; 10.06 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
            "2022-03-10 06:13:11 | WARNING | fairseq.trainer | |===========================================================================|\n",
            "|                  PyTorch CUDA memory summary, device ID 0                 |\n",
            "|---------------------------------------------------------------------------|\n",
            "|            CUDA OOMs: 21           |        cudaMalloc retries: 47        |\n",
            "|===========================================================================|\n",
            "|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Allocated memory      |    7180 MB |    8592 MB |    5880 GB |    5873 GB |\n",
            "|       from large pool |    7131 MB |    8544 MB |    5634 GB |    5627 GB |\n",
            "|       from small pool |      48 MB |      60 MB |     245 GB |     245 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Active memory         |    7180 MB |    8592 MB |    5880 GB |    5873 GB |\n",
            "|       from large pool |    7131 MB |    8544 MB |    5634 GB |    5627 GB |\n",
            "|       from small pool |      48 MB |      60 MB |     245 GB |     245 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| GPU reserved memory   |   10300 MB |   10374 MB |   98846 MB |   88546 MB |\n",
            "|       from large pool |   10240 MB |   10240 MB |   96348 MB |   86108 MB |\n",
            "|       from small pool |      60 MB |     134 MB |    2498 MB |    2438 MB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Non-releasable memory |    1141 MB |    1707 MB |    6228 GB |    6227 GB |\n",
            "|       from large pool |    1130 MB |    1695 MB |    5959 GB |    5958 GB |\n",
            "|       from small pool |      11 MB |      20 MB |     269 GB |     269 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Allocations           |     592    |     595    |    2757 K  |    2757 K  |\n",
            "|       from large pool |     160    |     164    |     936 K  |     935 K  |\n",
            "|       from small pool |     432    |     516    |    1821 K  |    1821 K  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Active allocs         |     592    |     595    |    2757 K  |    2757 K  |\n",
            "|       from large pool |     160    |     164    |     936 K  |     935 K  |\n",
            "|       from small pool |     432    |     516    |    1821 K  |    1821 K  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| GPU reserved segments |      80    |     117    |    1406    |    1326    |\n",
            "|       from large pool |      50    |      50    |     157    |     107    |\n",
            "|       from small pool |      30    |      67    |    1249    |    1219    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Non-releasable allocs |      64    |      65    |    1234 K  |    1234 K  |\n",
            "|       from large pool |      36    |      37    |     580 K  |     580 K  |\n",
            "|       from small pool |      28    |      46    |     654 K  |     654 K  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Oversize allocations  |       0    |       0    |       0    |       0    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Oversize GPU segments |       0    |       0    |       0    |       0    |\n",
            "|===========================================================================|\n",
            "\n",
            "2022-03-10 06:13:11 | WARNING | fairseq.trainer | attempting to recover from OOM in forward/backward pass\n",
            "epoch 004: 100% 692/694 [02:08<00:00,  6.95it/s, loss=3.308, ppl=9.91, wps=14115.2, ups=5.63, wpb=2507.1, bsz=127.2, num_updates=2700, lr=0.0005, gnorm=0.963, train_wall=17, gb_free=10.6, wall=522]2022-03-10 06:14:07 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "\n",
            "epoch 004 | valid on 'valid' subset:   0% 0/8 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 004 | valid on 'valid' subset:  12% 1/8 [00:00<00:00,  7.24it/s]\u001b[A\n",
            "epoch 004 | valid on 'valid' subset:  38% 3/8 [00:00<00:00, 13.07it/s]\u001b[A\n",
            "epoch 004 | valid on 'valid' subset:  62% 5/8 [00:00<00:00, 12.38it/s]\u001b[A\n",
            "epoch 004 | valid on 'valid' subset:  88% 7/8 [00:00<00:00, 10.38it/s]\u001b[A\n",
            "                                                                      \u001b[A2022-03-10 06:14:07 | INFO | valid | epoch 004 | valid on 'valid' subset | loss 3.77 | ppl 13.64 | wps 26768.7 | wpb 2673.1 | bsz 125 | num_updates 2755 | best_loss 3.77\n",
            "2022-03-10 06:14:07 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 4 @ 2755 updates\n",
            "2022-03-10 06:14:07 | INFO | fairseq.trainer | Saving checkpoint to /content/checkpoints/checkpoint4.pt\n",
            "2022-03-10 06:14:08 | INFO | fairseq.trainer | Finished saving checkpoint to /content/checkpoints/checkpoint4.pt\n",
            "epoch 005:   0% 0/694 [00:00<?, ?it/s]2022-03-10 06:14:10 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/checkpoint4.pt (epoch 4 @ 2755 updates, score 3.77) (writing took 2.1188503500000024 seconds)\n",
            "2022-03-10 06:14:10 | INFO | fairseq_cli.train | end of epoch 4 (average epoch stats below)\n",
            "2022-03-10 06:14:10 | INFO | train | epoch 004 | loss 3.289 | ppl 9.77 | wps 13381.9 | ups 5.22 | wpb 2564 | bsz 127.9 | num_updates 2755 | lr 0.0005 | gnorm 0.923 | train_wall 123 | gb_free 9.6 | wall 533\n",
            "2022-03-10 06:14:10 | INFO | fairseq.data.iterators | grouped total_num_itrs = 694\n",
            "2022-03-10 06:14:10 | INFO | fairseq.trainer | begin training epoch 5\n",
            "2022-03-10 06:14:10 | INFO | fairseq_cli.train | Start iterating over samples\n",
            "epoch 005:  26% 178/694 [00:34<01:23,  6.20it/s, loss=3.035, ppl=8.2, wps=14138.9, ups=5.25, wpb=2691.1, bsz=128, num_updates=2900, lr=0.0005, gnorm=0.94, train_wall=19, gb_free=10.7, wall=560]2022-03-10 06:14:44 | WARNING | fairseq.trainer | OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 1.43 GiB (GPU 0; 11.17 GiB total capacity; 8.46 GiB already allocated; 929.81 MiB free; 9.74 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
            "2022-03-10 06:14:44 | WARNING | fairseq.trainer | |===========================================================================|\n",
            "|                  PyTorch CUDA memory summary, device ID 0                 |\n",
            "|---------------------------------------------------------------------------|\n",
            "|            CUDA OOMs: 22           |        cudaMalloc retries: 51        |\n",
            "|===========================================================================|\n",
            "|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Allocated memory      |    7196 MB |    8662 MB |    7033 GB |    7026 GB |\n",
            "|       from large pool |    7148 MB |    8614 MB |    6737 GB |    6730 GB |\n",
            "|       from small pool |      48 MB |      60 MB |     295 GB |     295 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Active memory         |    7196 MB |    8662 MB |    7033 GB |    7026 GB |\n",
            "|       from large pool |    7148 MB |    8614 MB |    6737 GB |    6730 GB |\n",
            "|       from small pool |      48 MB |      60 MB |     295 GB |     295 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| GPU reserved memory   |    9978 MB |    9978 MB |  107730 MB |   97752 MB |\n",
            "|       from large pool |    9918 MB |    9918 MB |  105008 MB |   95090 MB |\n",
            "|       from small pool |      60 MB |     132 MB |    2722 MB |    2662 MB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Non-releasable memory |    1315 MB |    1497 MB |    7477 GB |    7476 GB |\n",
            "|       from large pool |    1303 MB |    1485 MB |    7154 GB |    7152 GB |\n",
            "|       from small pool |      11 MB |      21 MB |     323 GB |     323 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Allocations           |     592    |     595    |    3315 K  |    3314 K  |\n",
            "|       from large pool |     160    |     164    |    1123 K  |    1123 K  |\n",
            "|       from small pool |     432    |     516    |    2191 K  |    2191 K  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Active allocs         |     592    |     595    |    3315 K  |    3314 K  |\n",
            "|       from large pool |     160    |     164    |    1123 K  |    1123 K  |\n",
            "|       from small pool |     432    |     516    |    2191 K  |    2191 K  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| GPU reserved segments |      80    |     116    |    1525    |    1445    |\n",
            "|       from large pool |      50    |      50    |     164    |     114    |\n",
            "|       from small pool |      30    |      66    |    1361    |    1331    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Non-releasable allocs |      63    |      63    |    1485 K  |    1485 K  |\n",
            "|       from large pool |      34    |      34    |     699 K  |     699 K  |\n",
            "|       from small pool |      29    |      49    |     786 K  |     786 K  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Oversize allocations  |       0    |       0    |       0    |       0    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Oversize GPU segments |       0    |       0    |       0    |       0    |\n",
            "|===========================================================================|\n",
            "\n",
            "2022-03-10 06:14:44 | WARNING | fairseq.trainer | attempting to recover from OOM in forward/backward pass\n",
            "epoch 005:  58% 405/694 [01:13<00:40,  7.07it/s, loss=3.022, ppl=8.12, wps=13994, ups=5.69, wpb=2458.8, bsz=128, num_updates=3100, lr=0.0005, gnorm=0.992, train_wall=17, gb_free=10, wall=597]2022-03-10 06:15:24 | WARNING | fairseq.trainer | OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 1.78 GiB (GPU 0; 11.17 GiB total capacity; 9.33 GiB already allocated; 961.81 MiB free; 9.71 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
            "2022-03-10 06:15:24 | WARNING | fairseq.trainer | |===========================================================================|\n",
            "|                  PyTorch CUDA memory summary, device ID 0                 |\n",
            "|---------------------------------------------------------------------------|\n",
            "|            CUDA OOMs: 23           |        cudaMalloc retries: 54        |\n",
            "|===========================================================================|\n",
            "|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Allocated memory      |    9552 MB |    9627 MB |    7533 GB |    7524 GB |\n",
            "|       from large pool |    9502 MB |    9578 MB |    7213 GB |    7204 GB |\n",
            "|       from small pool |      49 MB |      60 MB |     319 GB |     319 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Active memory         |    9552 MB |    9627 MB |    7533 GB |    7524 GB |\n",
            "|       from large pool |    9502 MB |    9578 MB |    7213 GB |    7204 GB |\n",
            "|       from small pool |      49 MB |      60 MB |     319 GB |     319 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| GPU reserved memory   |    9946 MB |    9984 MB |  112074 MB |  102128 MB |\n",
            "|       from large pool |    9886 MB |    9924 MB |  109196 MB |   99310 MB |\n",
            "|       from small pool |      60 MB |     138 MB |    2878 MB |    2818 MB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Non-releasable memory |  403289 KB |    1571 MB |    8046 GB |    8045 GB |\n",
            "|       from large pool |  392283 KB |    1559 MB |    7696 GB |    7696 GB |\n",
            "|       from small pool |   11006 KB |      17 MB |     349 GB |     349 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Allocations           |     593    |     595    |    3569 K  |    3568 K  |\n",
            "|       from large pool |     160    |     164    |    1207 K  |    1207 K  |\n",
            "|       from small pool |     433    |     516    |    2361 K  |    2361 K  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Active allocs         |     593    |     595    |    3569 K  |    3568 K  |\n",
            "|       from large pool |     160    |     164    |    1207 K  |    1207 K  |\n",
            "|       from small pool |     433    |     516    |    2361 K  |    2361 K  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| GPU reserved segments |      79    |     119    |    1606    |    1527    |\n",
            "|       from large pool |      49    |      50    |     167    |     118    |\n",
            "|       from small pool |      30    |      69    |    1439    |    1409    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Non-releasable allocs |      62    |      63    |    1601 K  |    1601 K  |\n",
            "|       from large pool |      31    |      32    |     752 K  |     752 K  |\n",
            "|       from small pool |      31    |      42    |     849 K  |     849 K  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Oversize allocations  |       0    |       0    |       0    |       0    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Oversize GPU segments |       0    |       0    |       0    |       0    |\n",
            "|===========================================================================|\n",
            "\n",
            "2022-03-10 06:15:24 | WARNING | fairseq.trainer | attempting to recover from OOM in forward/backward pass\n",
            "epoch 005:  62% 428/694 [01:18<00:50,  5.29it/s, loss=3.022, ppl=8.12, wps=13994, ups=5.69, wpb=2458.8, bsz=128, num_updates=3100, lr=0.0005, gnorm=0.992, train_wall=17, gb_free=10, wall=597]2022-03-10 06:15:28 | WARNING | fairseq.trainer | OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 1.38 GiB (GPU 0; 11.17 GiB total capacity; 8.39 GiB already allocated; 855.81 MiB free; 9.82 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
            "2022-03-10 06:15:28 | WARNING | fairseq.trainer | |===========================================================================|\n",
            "|                  PyTorch CUDA memory summary, device ID 0                 |\n",
            "|---------------------------------------------------------------------------|\n",
            "|            CUDA OOMs: 24           |        cudaMalloc retries: 56        |\n",
            "|===========================================================================|\n",
            "|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Allocated memory      |    7180 MB |    8592 MB |    7586 GB |    7579 GB |\n",
            "|       from large pool |    7131 MB |    8544 MB |    7263 GB |    7256 GB |\n",
            "|       from small pool |      48 MB |      60 MB |     322 GB |     322 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Active memory         |    7180 MB |    8592 MB |    7586 GB |    7579 GB |\n",
            "|       from large pool |    7131 MB |    8544 MB |    7263 GB |    7256 GB |\n",
            "|       from small pool |      48 MB |      60 MB |     322 GB |     322 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| GPU reserved memory   |   10052 MB |   10052 MB |  113550 MB |  103498 MB |\n",
            "|       from large pool |    9992 MB |    9992 MB |  110610 MB |  100618 MB |\n",
            "|       from small pool |      60 MB |     122 MB |    2940 MB |    2880 MB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Non-releasable memory |    1457 MB |    1459 MB |    8099 GB |    8098 GB |\n",
            "|       from large pool |    1446 MB |    1447 MB |    7747 GB |    7745 GB |\n",
            "|       from small pool |      11 MB |      20 MB |     352 GB |     352 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Allocations           |     592    |     595    |    3594 K  |    3593 K  |\n",
            "|       from large pool |     160    |     164    |    1215 K  |    1215 K  |\n",
            "|       from small pool |     432    |     516    |    2378 K  |    2378 K  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Active allocs         |     592    |     595    |    3594 K  |    3593 K  |\n",
            "|       from large pool |     160    |     164    |    1215 K  |    1215 K  |\n",
            "|       from small pool |     432    |     516    |    2378 K  |    2378 K  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| GPU reserved segments |      79    |     110    |    1638    |    1559    |\n",
            "|       from large pool |      49    |      49    |     168    |     119    |\n",
            "|       from small pool |      30    |      61    |    1470    |    1440    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Non-releasable allocs |      66    |      67    |    1613 K  |    1613 K  |\n",
            "|       from large pool |      35    |      36    |     757 K  |     757 K  |\n",
            "|       from small pool |      31    |      40    |     855 K  |     855 K  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Oversize allocations  |       0    |       0    |       0    |       0    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Oversize GPU segments |       0    |       0    |       0    |       0    |\n",
            "|===========================================================================|\n",
            "\n",
            "2022-03-10 06:15:28 | WARNING | fairseq.trainer | attempting to recover from OOM in forward/backward pass\n",
            "epoch 005:  71% 494/694 [01:29<00:29,  6.83it/s, loss=2.973, ppl=7.85, wps=13142.3, ups=5.52, wpb=2382.3, bsz=127.2, num_updates=3200, lr=0.0005, gnorm=0.998, train_wall=16, gb_free=10.6, wall=615]2022-03-10 06:15:40 | WARNING | fairseq.trainer | OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 1.56 GiB (GPU 0; 11.17 GiB total capacity; 9.43 GiB already allocated; 671.81 MiB free; 10.00 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
            "2022-03-10 06:15:40 | WARNING | fairseq.trainer | |===========================================================================|\n",
            "|                  PyTorch CUDA memory summary, device ID 0                 |\n",
            "|---------------------------------------------------------------------------|\n",
            "|            CUDA OOMs: 25           |        cudaMalloc retries: 57        |\n",
            "|===========================================================================|\n",
            "|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Allocated memory      |    8062 MB |    9659 MB |    7731 GB |    7723 GB |\n",
            "|       from large pool |    8013 MB |    9610 MB |    7402 GB |    7394 GB |\n",
            "|       from small pool |      48 MB |      60 MB |     329 GB |     329 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Active memory         |    8062 MB |    9659 MB |    7731 GB |    7723 GB |\n",
            "|       from large pool |    8013 MB |    9610 MB |    7402 GB |    7394 GB |\n",
            "|       from small pool |      48 MB |      60 MB |     329 GB |     329 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| GPU reserved memory   |   10236 MB |   10306 MB |  115218 MB |  104982 MB |\n",
            "|       from large pool |   10176 MB |   10176 MB |  112208 MB |  102032 MB |\n",
            "|       from small pool |      60 MB |     130 MB |    3010 MB |    2950 MB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Non-releasable memory |  589399 KB |    1915 MB |    8260 GB |    8259 GB |\n",
            "|       from large pool |  577950 KB |    1903 MB |    7900 GB |    7899 GB |\n",
            "|       from small pool |   11449 KB |      37 MB |     359 GB |     359 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Allocations           |     592    |     595    |    3668 K  |    3668 K  |\n",
            "|       from large pool |     160    |     164    |    1240 K  |    1240 K  |\n",
            "|       from small pool |     432    |     516    |    2428 K  |    2428 K  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Active allocs         |     592    |     595    |    3668 K  |    3668 K  |\n",
            "|       from large pool |     160    |     164    |    1240 K  |    1240 K  |\n",
            "|       from small pool |     432    |     516    |    2428 K  |    2428 K  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| GPU reserved segments |      79    |     114    |    1674    |    1595    |\n",
            "|       from large pool |      49    |      49    |     169    |     120    |\n",
            "|       from small pool |      30    |      65    |    1505    |    1475    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Non-releasable allocs |      57    |      67    |    1646 K  |    1646 K  |\n",
            "|       from large pool |      26    |      27    |     773 K  |     773 K  |\n",
            "|       from small pool |      31    |      62    |     872 K  |     872 K  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Oversize allocations  |       0    |       0    |       0    |       0    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Oversize GPU segments |       0    |       0    |       0    |       0    |\n",
            "|===========================================================================|\n",
            "\n",
            "2022-03-10 06:15:40 | WARNING | fairseq.trainer | attempting to recover from OOM in forward/backward pass\n",
            "epoch 005:  76% 527/694 [01:36<00:28,  5.91it/s, loss=2.973, ppl=7.85, wps=13142.3, ups=5.52, wpb=2382.3, bsz=127.2, num_updates=3200, lr=0.0005, gnorm=0.998, train_wall=16, gb_free=10.6, wall=615]2022-03-10 06:15:47 | WARNING | fairseq.trainer | OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 1.93 GiB (GPU 0; 11.17 GiB total capacity; 10.01 GiB already allocated; 291.81 MiB free; 10.37 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
            "2022-03-10 06:15:47 | WARNING | fairseq.trainer | |===========================================================================|\n",
            "|                  PyTorch CUDA memory summary, device ID 0                 |\n",
            "|---------------------------------------------------------------------------|\n",
            "|            CUDA OOMs: 26           |        cudaMalloc retries: 58        |\n",
            "|===========================================================================|\n",
            "|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Allocated memory      |   10251 MB |   10280 MB |    7824 GB |    7814 GB |\n",
            "|       from large pool |   10201 MB |   10230 MB |    7492 GB |    7482 GB |\n",
            "|       from small pool |      50 MB |      60 MB |     332 GB |     331 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Active memory         |   10251 MB |   10280 MB |    7824 GB |    7814 GB |\n",
            "|       from large pool |   10201 MB |   10230 MB |    7492 GB |    7482 GB |\n",
            "|       from small pool |      50 MB |      60 MB |     332 GB |     331 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| GPU reserved memory   |   10616 MB |   10682 MB |  117262 MB |  106646 MB |\n",
            "|       from large pool |   10556 MB |   10556 MB |  114186 MB |  103630 MB |\n",
            "|       from small pool |      60 MB |     126 MB |    3076 MB |    3016 MB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Non-releasable memory |  372746 KB |    2195 MB |    8363 GB |    8363 GB |\n",
            "|       from large pool |  362587 KB |    2183 MB |    8000 GB |    8000 GB |\n",
            "|       from small pool |   10159 KB |      20 MB |     362 GB |     362 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Allocations           |     592    |     595    |    3704 K  |    3703 K  |\n",
            "|       from large pool |     162    |     164    |    1252 K  |    1252 K  |\n",
            "|       from small pool |     430    |     516    |    2451 K  |    2450 K  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Active allocs         |     592    |     595    |    3704 K  |    3703 K  |\n",
            "|       from large pool |     162    |     164    |    1252 K  |    1252 K  |\n",
            "|       from small pool |     430    |     516    |    2451 K  |    2450 K  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| GPU reserved segments |      79    |     112    |    1708    |    1629    |\n",
            "|       from large pool |      49    |      49    |     170    |     121    |\n",
            "|       from small pool |      30    |      63    |    1538    |    1508    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Non-releasable allocs |      68    |      69    |    1662 K  |    1662 K  |\n",
            "|       from large pool |      37    |      38    |     781 K  |     781 K  |\n",
            "|       from small pool |      31    |      43    |     880 K  |     880 K  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Oversize allocations  |       0    |       0    |       0    |       0    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Oversize GPU segments |       0    |       0    |       0    |       0    |\n",
            "|===========================================================================|\n",
            "\n",
            "2022-03-10 06:15:47 | WARNING | fairseq.trainer | attempting to recover from OOM in forward/backward pass\n",
            "epoch 005: 100% 693/694 [02:08<00:00,  6.15it/s, loss=3.144, ppl=8.84, wps=14222.6, ups=5.06, wpb=2812.4, bsz=128, num_updates=3400, lr=0.0005, gnorm=0.955, train_wall=19, gb_free=9.7, wall=654]2022-03-10 06:16:18 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "\n",
            "epoch 005 | valid on 'valid' subset:   0% 0/8 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 005 | valid on 'valid' subset:  12% 1/8 [00:00<00:01,  6.86it/s]\u001b[A\n",
            "epoch 005 | valid on 'valid' subset:  38% 3/8 [00:00<00:00, 11.62it/s]\u001b[A\n",
            "epoch 005 | valid on 'valid' subset:  62% 5/8 [00:00<00:00, 11.70it/s]\u001b[A\n",
            "epoch 005 | valid on 'valid' subset:  88% 7/8 [00:00<00:00, 10.04it/s]\u001b[A\n",
            "                                                                      \u001b[A2022-03-10 06:16:19 | INFO | valid | epoch 005 | valid on 'valid' subset | loss 3.748 | ppl 13.44 | wps 25596.5 | wpb 2673.1 | bsz 125 | num_updates 3444 | best_loss 3.748\n",
            "2022-03-10 06:16:19 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 5 @ 3444 updates\n",
            "2022-03-10 06:16:19 | INFO | fairseq.trainer | Saving checkpoint to /content/checkpoints/checkpoint5.pt\n",
            "2022-03-10 06:16:20 | INFO | fairseq.trainer | Finished saving checkpoint to /content/checkpoints/checkpoint5.pt\n",
            "2022-03-10 06:16:21 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/checkpoint5.pt (epoch 5 @ 3444 updates, score 3.748) (writing took 1.8725147139998626 seconds)\n",
            "2022-03-10 06:16:21 | INFO | fairseq_cli.train | end of epoch 5 (average epoch stats below)\n",
            "2022-03-10 06:16:21 | INFO | train | epoch 005 | loss 3.012 | ppl 8.07 | wps 13429.1 | ups 5.24 | wpb 2564 | bsz 127.9 | num_updates 3444 | lr 0.0005 | gnorm 0.957 | train_wall 123 | gb_free 10.5 | wall 664\n",
            "epoch 006:   0% 0/694 [00:00<?, ?it/s]2022-03-10 06:16:21 | INFO | fairseq.data.iterators | grouped total_num_itrs = 694\n",
            "2022-03-10 06:16:21 | INFO | fairseq.trainer | begin training epoch 6\n",
            "2022-03-10 06:16:21 | INFO | fairseq_cli.train | Start iterating over samples\n",
            "epoch 006:  15% 104/694 [00:19<01:26,  6.84it/s, loss=2.853, ppl=7.23, wps=12185.4, ups=4.65, wpb=2619.8, bsz=127.2, num_updates=3500, lr=0.0005, gnorm=0.926, train_wall=18, gb_free=10.5, wall=676]2022-03-10 06:16:41 | WARNING | fairseq.trainer | OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 1.43 GiB (GPU 0; 11.17 GiB total capacity; 8.46 GiB already allocated; 293.81 MiB free; 10.37 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
            "2022-03-10 06:16:41 | WARNING | fairseq.trainer | |===========================================================================|\n",
            "|                  PyTorch CUDA memory summary, device ID 0                 |\n",
            "|---------------------------------------------------------------------------|\n",
            "|            CUDA OOMs: 27           |        cudaMalloc retries: 59        |\n",
            "|===========================================================================|\n",
            "|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Allocated memory      |    7196 MB |    8661 MB |    8504 GB |    8497 GB |\n",
            "|       from large pool |    7148 MB |    8613 MB |    8146 GB |    8140 GB |\n",
            "|       from small pool |      48 MB |      60 MB |     357 GB |     357 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Active memory         |    7196 MB |    8661 MB |    8504 GB |    8497 GB |\n",
            "|       from large pool |    7148 MB |    8613 MB |    8146 GB |    8140 GB |\n",
            "|       from small pool |      48 MB |      60 MB |     357 GB |     357 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| GPU reserved memory   |   10614 MB |   10694 MB |  117340 MB |  106726 MB |\n",
            "|       from large pool |   10556 MB |   10556 MB |  114186 MB |  103630 MB |\n",
            "|       from small pool |      58 MB |     138 MB |    3154 MB |    3096 MB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Non-releasable memory |    1439 MB |    1952 MB |    9134 GB |    9132 GB |\n",
            "|       from large pool |    1429 MB |    1942 MB |    8743 GB |    8741 GB |\n",
            "|       from small pool |       9 MB |      18 MB |     391 GB |     391 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Allocations           |     592    |     595    |    4010 K  |    4010 K  |\n",
            "|       from large pool |     160    |     164    |    1359 K  |    1359 K  |\n",
            "|       from small pool |     432    |     516    |    2651 K  |    2650 K  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Active allocs         |     592    |     595    |    4010 K  |    4010 K  |\n",
            "|       from large pool |     160    |     164    |    1359 K  |    1359 K  |\n",
            "|       from small pool |     432    |     516    |    2651 K  |    2650 K  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| GPU reserved segments |      78    |     118    |    1747    |    1669    |\n",
            "|       from large pool |      49    |      49    |     170    |     121    |\n",
            "|       from small pool |      29    |      69    |    1577    |    1548    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Non-releasable allocs |      61    |      62    |    1801 K  |    1801 K  |\n",
            "|       from large pool |      33    |      34    |     849 K  |     849 K  |\n",
            "|       from small pool |      28    |      39    |     952 K  |     952 K  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Oversize allocations  |       0    |       0    |       0    |       0    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Oversize GPU segments |       0    |       0    |       0    |       0    |\n",
            "|===========================================================================|\n",
            "\n",
            "2022-03-10 06:16:41 | WARNING | fairseq.trainer | attempting to recover from OOM in forward/backward pass\n",
            "epoch 006:  31% 217/694 [00:40<01:17,  6.18it/s, loss=2.706, ppl=6.53, wps=13395.6, ups=5.23, wpb=2559.2, bsz=128, num_updates=3600, lr=0.0005, gnorm=0.933, train_wall=18, gb_free=10.4, wall=695]2022-03-10 06:17:03 | WARNING | fairseq.trainer | OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 1.56 GiB (GPU 0; 11.17 GiB total capacity; 9.43 GiB already allocated; 615.81 MiB free; 10.05 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
            "2022-03-10 06:17:03 | WARNING | fairseq.trainer | |===========================================================================|\n",
            "|                  PyTorch CUDA memory summary, device ID 0                 |\n",
            "|---------------------------------------------------------------------------|\n",
            "|            CUDA OOMs: 28           |        cudaMalloc retries: 62        |\n",
            "|===========================================================================|\n",
            "|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Allocated memory      |    8062 MB |    9659 MB |    8773 GB |    8766 GB |\n",
            "|       from large pool |    8013 MB |    9610 MB |    8404 GB |    8396 GB |\n",
            "|       from small pool |      48 MB |      60 MB |     369 GB |     369 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Active memory         |    8062 MB |    9659 MB |    8773 GB |    8766 GB |\n",
            "|       from large pool |    8013 MB |    9610 MB |    8404 GB |    8396 GB |\n",
            "|       from small pool |      48 MB |      60 MB |     369 GB |     369 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| GPU reserved memory   |   10292 MB |   10292 MB |  121704 MB |  111412 MB |\n",
            "|       from large pool |   10234 MB |   10234 MB |  118400 MB |  108166 MB |\n",
            "|       from small pool |      58 MB |     138 MB |    3304 MB |    3246 MB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Non-releasable memory |  646743 KB |    1815 MB |    9421 GB |    9420 GB |\n",
            "|       from large pool |  637342 KB |    1805 MB |    9017 GB |    9016 GB |\n",
            "|       from small pool |    9401 KB |      15 MB |     403 GB |     403 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Allocations           |     592    |     595    |    4137 K  |    4136 K  |\n",
            "|       from large pool |     160    |     164    |    1401 K  |    1401 K  |\n",
            "|       from small pool |     432    |     516    |    2735 K  |    2734 K  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Active allocs         |     592    |     595    |    4137 K  |    4136 K  |\n",
            "|       from large pool |     160    |     164    |    1401 K  |    1401 K  |\n",
            "|       from small pool |     432    |     516    |    2735 K  |    2734 K  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| GPU reserved segments |      78    |     118    |    1825    |    1747    |\n",
            "|       from large pool |      49    |      49    |     173    |     124    |\n",
            "|       from small pool |      29    |      69    |    1652    |    1623    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Non-releasable allocs |      54    |      55    |    1858 K  |    1858 K  |\n",
            "|       from large pool |      25    |      26    |     876 K  |     876 K  |\n",
            "|       from small pool |      29    |      40    |     982 K  |     982 K  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Oversize allocations  |       0    |       0    |       0    |       0    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Oversize GPU segments |       0    |       0    |       0    |       0    |\n",
            "|===========================================================================|\n",
            "\n",
            "2022-03-10 06:17:03 | WARNING | fairseq.trainer | attempting to recover from OOM in forward/backward pass\n",
            "epoch 006:  32% 222/694 [00:42<02:36,  3.02it/s, loss=2.706, ppl=6.53, wps=13395.6, ups=5.23, wpb=2559.2, bsz=128, num_updates=3600, lr=0.0005, gnorm=0.933, train_wall=18, gb_free=10.4, wall=695]2022-03-10 06:17:05 | WARNING | fairseq.trainer | OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 1.93 GiB (GPU 0; 11.17 GiB total capacity; 10.01 GiB already allocated; 235.81 MiB free; 10.42 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
            "2022-03-10 06:17:05 | WARNING | fairseq.trainer | |===========================================================================|\n",
            "|                  PyTorch CUDA memory summary, device ID 0                 |\n",
            "|---------------------------------------------------------------------------|\n",
            "|            CUDA OOMs: 29           |        cudaMalloc retries: 63        |\n",
            "|===========================================================================|\n",
            "|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Allocated memory      |   10253 MB |   10281 MB |    8806 GB |    8796 GB |\n",
            "|       from large pool |   10203 MB |   10231 MB |    8436 GB |    8426 GB |\n",
            "|       from small pool |      50 MB |      60 MB |     369 GB |     369 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Active memory         |   10253 MB |   10281 MB |    8806 GB |    8796 GB |\n",
            "|       from large pool |   10203 MB |   10231 MB |    8436 GB |    8426 GB |\n",
            "|       from small pool |      50 MB |      60 MB |     369 GB |     369 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| GPU reserved memory   |   10672 MB |   10712 MB |  123722 MB |  113050 MB |\n",
            "|       from large pool |   10614 MB |   10614 MB |  120378 MB |  109764 MB |\n",
            "|       from small pool |      58 MB |      98 MB |    3344 MB |    3286 MB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Non-releasable memory |  428547 KB |    1897 MB |    9453 GB |    9452 GB |\n",
            "|       from large pool |  420436 KB |    1887 MB |    9048 GB |    9048 GB |\n",
            "|       from small pool |    8111 KB |      12 MB |     404 GB |     404 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Allocations           |     592    |     595    |    4142 K  |    4141 K  |\n",
            "|       from large pool |     162    |     164    |    1403 K  |    1403 K  |\n",
            "|       from small pool |     430    |     516    |    2738 K  |    2737 K  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Active allocs         |     592    |     595    |    4142 K  |    4141 K  |\n",
            "|       from large pool |     162    |     164    |    1403 K  |    1403 K  |\n",
            "|       from small pool |     430    |     516    |    2738 K  |    2737 K  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| GPU reserved segments |      78    |      98    |    1846    |    1768    |\n",
            "|       from large pool |      49    |      49    |     174    |     125    |\n",
            "|       from small pool |      29    |      49    |    1672    |    1643    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Non-releasable allocs |      65    |      66    |    1861 K  |    1861 K  |\n",
            "|       from large pool |      36    |      38    |     877 K  |     877 K  |\n",
            "|       from small pool |      29    |      31    |     983 K  |     983 K  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Oversize allocations  |       0    |       0    |       0    |       0    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Oversize GPU segments |       0    |       0    |       0    |       0    |\n",
            "|===========================================================================|\n",
            "\n",
            "2022-03-10 06:17:05 | WARNING | fairseq.trainer | attempting to recover from OOM in forward/backward pass\n",
            "epoch 006:  70% 486/694 [01:29<01:00,  3.42it/s, loss=2.665, ppl=6.34, wps=15012.4, ups=6.22, wpb=2412.1, bsz=128, num_updates=3900, lr=0.0005, gnorm=0.993, train_wall=16, gb_free=9.6, wall=748]2022-03-10 06:17:50 | WARNING | fairseq.trainer | OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 1.78 GiB (GPU 0; 11.17 GiB total capacity; 9.33 GiB already allocated; 273.81 MiB free; 10.38 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
            "2022-03-10 06:17:50 | WARNING | fairseq.trainer | |===========================================================================|\n",
            "|                  PyTorch CUDA memory summary, device ID 0                 |\n",
            "|---------------------------------------------------------------------------|\n",
            "|            CUDA OOMs: 30           |        cudaMalloc retries: 64        |\n",
            "|===========================================================================|\n",
            "|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Allocated memory      |    9552 MB |    9627 MB |    9368 GB |    9359 GB |\n",
            "|       from large pool |    9502 MB |    9578 MB |    8971 GB |    8961 GB |\n",
            "|       from small pool |      49 MB |      60 MB |     397 GB |     397 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Active memory         |    9552 MB |    9627 MB |    9368 GB |    9359 GB |\n",
            "|       from large pool |    9502 MB |    9578 MB |    8971 GB |    8961 GB |\n",
            "|       from small pool |      49 MB |      60 MB |     397 GB |     397 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| GPU reserved memory   |   10634 MB |   10752 MB |  123802 MB |  113168 MB |\n",
            "|       from large pool |   10576 MB |   10614 MB |  120378 MB |  109802 MB |\n",
            "|       from small pool |      58 MB |     138 MB |    3424 MB |    3366 MB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Non-releasable memory |    1081 MB |    1894 MB |   10097 GB |   10096 GB |\n",
            "|       from large pool |    1073 MB |    1885 MB |    9663 GB |    9662 GB |\n",
            "|       from small pool |       8 MB |      13 MB |     434 GB |     434 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Allocations           |     593    |     595    |    4436 K  |    4436 K  |\n",
            "|       from large pool |     160    |     164    |    1501 K  |    1501 K  |\n",
            "|       from small pool |     433    |     516    |    2935 K  |    2934 K  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Active allocs         |     593    |     595    |    4436 K  |    4436 K  |\n",
            "|       from large pool |     160    |     164    |    1501 K  |    1501 K  |\n",
            "|       from small pool |     433    |     516    |    2935 K  |    2934 K  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| GPU reserved segments |      77    |     118    |    1886    |    1809    |\n",
            "|       from large pool |      48    |      49    |     174    |     126    |\n",
            "|       from small pool |      29    |      69    |    1712    |    1683    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Non-releasable allocs |      59    |      61    |    1993 K  |    1993 K  |\n",
            "|       from large pool |      30    |      31    |     939 K  |     939 K  |\n",
            "|       from small pool |      29    |      37    |    1054 K  |    1054 K  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Oversize allocations  |       0    |       0    |       0    |       0    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Oversize GPU segments |       0    |       0    |       0    |       0    |\n",
            "|===========================================================================|\n",
            "\n",
            "2022-03-10 06:17:50 | WARNING | fairseq.trainer | attempting to recover from OOM in forward/backward pass\n",
            "epoch 006:  80% 557/694 [01:43<00:26,  5.15it/s, loss=2.665, ppl=6.34, wps=15012.4, ups=6.22, wpb=2412.1, bsz=128, num_updates=3900, lr=0.0005, gnorm=0.993, train_wall=16, gb_free=9.6, wall=748]2022-03-10 06:18:05 | WARNING | fairseq.trainer | OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 1.38 GiB (GPU 0; 11.17 GiB total capacity; 8.39 GiB already allocated; 273.81 MiB free; 10.38 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
            "2022-03-10 06:18:05 | WARNING | fairseq.trainer | |===========================================================================|\n",
            "|                  PyTorch CUDA memory summary, device ID 0                 |\n",
            "|---------------------------------------------------------------------------|\n",
            "|            CUDA OOMs: 31           |        cudaMalloc retries: 65        |\n",
            "|===========================================================================|\n",
            "|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Allocated memory      |    7180 MB |    8592 MB |    9574 GB |    9567 GB |\n",
            "|       from large pool |    7131 MB |    8544 MB |    9170 GB |    9163 GB |\n",
            "|       from small pool |      48 MB |      60 MB |     404 GB |     404 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Active memory         |    7180 MB |    8592 MB |    9574 GB |    9567 GB |\n",
            "|       from large pool |    7131 MB |    8544 MB |    9170 GB |    9163 GB |\n",
            "|       from small pool |      48 MB |      60 MB |     404 GB |     404 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| GPU reserved memory   |   10634 MB |   10704 MB |  123872 MB |  113238 MB |\n",
            "|       from large pool |   10576 MB |   10576 MB |  120378 MB |  109802 MB |\n",
            "|       from small pool |      58 MB |     128 MB |    3494 MB |    3436 MB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Non-releasable memory |    1475 MB |    2041 MB |   10332 GB |   10331 GB |\n",
            "|       from large pool |    1466 MB |    2031 MB |    9891 GB |    9889 GB |\n",
            "|       from small pool |       9 MB |      15 MB |     441 GB |     441 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Allocations           |     592    |     595    |    4517 K  |    4516 K  |\n",
            "|       from large pool |     160    |     164    |    1529 K  |    1529 K  |\n",
            "|       from small pool |     432    |     516    |    2987 K  |    2987 K  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Active allocs         |     592    |     595    |    4517 K  |    4516 K  |\n",
            "|       from large pool |     160    |     164    |    1529 K  |    1529 K  |\n",
            "|       from small pool |     432    |     516    |    2987 K  |    2987 K  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| GPU reserved segments |      77    |     112    |    1921    |    1844    |\n",
            "|       from large pool |      48    |      48    |     174    |     126    |\n",
            "|       from small pool |      29    |      64    |    1747    |    1718    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Non-releasable allocs |      63    |      64    |    2030 K  |    2030 K  |\n",
            "|       from large pool |      34    |      35    |     956 K  |     956 K  |\n",
            "|       from small pool |      29    |      37    |    1073 K  |    1073 K  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Oversize allocations  |       0    |       0    |       0    |       0    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Oversize GPU segments |       0    |       0    |       0    |       0    |\n",
            "|===========================================================================|\n",
            "\n",
            "2022-03-10 06:18:05 | WARNING | fairseq.trainer | attempting to recover from OOM in forward/backward pass\n",
            "epoch 006: 100% 693/694 [02:09<00:00,  5.59it/s, loss=2.875, ppl=7.33, wps=13501, ups=5.15, wpb=2622.6, bsz=128, num_updates=4100, lr=0.0005, gnorm=1.017, train_wall=19, gb_free=10.4, wall=789]2022-03-10 06:18:30 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "\n",
            "epoch 006 | valid on 'valid' subset:   0% 0/8 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 006 | valid on 'valid' subset:  12% 1/8 [00:00<00:01,  6.08it/s]\u001b[A\n",
            "epoch 006 | valid on 'valid' subset:  38% 3/8 [00:00<00:00, 10.29it/s]\u001b[A\n",
            "epoch 006 | valid on 'valid' subset:  62% 5/8 [00:00<00:00, 11.01it/s]\u001b[A\n",
            "epoch 006 | valid on 'valid' subset:  88% 7/8 [00:00<00:00,  9.72it/s]\u001b[A\n",
            "                                                                      \u001b[A2022-03-10 06:18:31 | INFO | valid | epoch 006 | valid on 'valid' subset | loss 3.75 | ppl 13.46 | wps 25150.4 | wpb 2673.1 | bsz 125 | num_updates 4133 | best_loss 3.748\n",
            "2022-03-10 06:18:31 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 6 @ 4133 updates\n",
            "2022-03-10 06:18:31 | INFO | fairseq.trainer | Saving checkpoint to /content/checkpoints/checkpoint6.pt\n",
            "2022-03-10 06:18:32 | INFO | fairseq.trainer | Finished saving checkpoint to /content/checkpoints/checkpoint6.pt\n",
            "2022-03-10 06:18:32 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/checkpoint6.pt (epoch 6 @ 4133 updates, score 3.75) (writing took 1.1075241900000492 seconds)\n",
            "2022-03-10 06:18:32 | INFO | fairseq_cli.train | end of epoch 6 (average epoch stats below)\n",
            "2022-03-10 06:18:32 | INFO | train | epoch 006 | loss 2.776 | ppl 6.85 | wps 13456.8 | ups 5.25 | wpb 2564 | bsz 127.9 | num_updates 4133 | lr 0.0005 | gnorm 0.986 | train_wall 123 | gb_free 10.6 | wall 796\n",
            "2022-03-10 06:18:32 | INFO | fairseq.data.iterators | grouped total_num_itrs = 694\n",
            "epoch 007:   0% 0/694 [00:00<?, ?it/s]2022-03-10 06:18:32 | INFO | fairseq.trainer | begin training epoch 7\n",
            "2022-03-10 06:18:32 | INFO | fairseq_cli.train | Start iterating over samples\n",
            "epoch 007:  23% 163/694 [00:28<01:40,  5.28it/s, loss=2.476, ppl=5.56, wps=12291.8, ups=5.3, wpb=2320.5, bsz=128, num_updates=4200, lr=0.0005, gnorm=0.996, train_wall=16, gb_free=10.4, wall=808]2022-03-10 06:19:01 | WARNING | fairseq.trainer | OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 1.43 GiB (GPU 0; 11.17 GiB total capacity; 8.46 GiB already allocated; 785.81 MiB free; 9.88 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
            "2022-03-10 06:19:01 | WARNING | fairseq.trainer | |===========================================================================|\n",
            "|                  PyTorch CUDA memory summary, device ID 0                 |\n",
            "|---------------------------------------------------------------------------|\n",
            "|            CUDA OOMs: 32           |        cudaMalloc retries: 69        |\n",
            "|===========================================================================|\n",
            "|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Allocated memory      |    7196 MB |    8662 MB |   10247 GB |   10240 GB |\n",
            "|       from large pool |    7148 MB |    8614 MB |    9812 GB |    9805 GB |\n",
            "|       from small pool |      48 MB |      60 MB |     435 GB |     434 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Active memory         |    7196 MB |    8662 MB |   10247 GB |   10240 GB |\n",
            "|       from large pool |    7148 MB |    8614 MB |    9812 GB |    9805 GB |\n",
            "|       from small pool |      48 MB |      60 MB |     435 GB |     434 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| GPU reserved memory   |   10122 MB |   10122 MB |  130068 MB |  119946 MB |\n",
            "|       from large pool |   10064 MB |   10064 MB |  126348 MB |  116284 MB |\n",
            "|       from small pool |      58 MB |     138 MB |    3720 MB |    3662 MB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Non-releasable memory |    1459 MB |    1459 MB |   11082 GB |   11081 GB |\n",
            "|       from large pool |    1449 MB |    1449 MB |   10607 GB |   10606 GB |\n",
            "|       from small pool |       9 MB |      35 MB |     475 GB |     475 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Allocations           |     592    |     595    |    4854 K  |    4854 K  |\n",
            "|       from large pool |     160    |     164    |    1642 K  |    1642 K  |\n",
            "|       from small pool |     432    |     516    |    3212 K  |    3211 K  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Active allocs         |     592    |     595    |    4854 K  |    4854 K  |\n",
            "|       from large pool |     160    |     164    |    1642 K  |    1642 K  |\n",
            "|       from small pool |     432    |     516    |    3212 K  |    3211 K  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| GPU reserved segments |      77    |     117    |    2039    |    1962    |\n",
            "|       from large pool |      48    |      48    |     179    |     131    |\n",
            "|       from small pool |      29    |      69    |    1860    |    1831    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Non-releasable allocs |      62    |      62    |    2184 K  |    2184 K  |\n",
            "|       from large pool |      32    |      32    |    1028 K  |    1028 K  |\n",
            "|       from small pool |      30    |      52    |    1155 K  |    1155 K  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Oversize allocations  |       0    |       0    |       0    |       0    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Oversize GPU segments |       0    |       0    |       0    |       0    |\n",
            "|===========================================================================|\n",
            "\n",
            "2022-03-10 06:19:01 | WARNING | fairseq.trainer | attempting to recover from OOM in forward/backward pass\n",
            "epoch 007:  43% 297/694 [00:53<01:38,  4.03it/s, loss=2.583, ppl=5.99, wps=14197.5, ups=5.37, wpb=2642.6, bsz=128, num_updates=4400, lr=0.0005, gnorm=1.005, train_wall=18, gb_free=10.5, wall=844]2022-03-10 06:19:27 | WARNING | fairseq.trainer | OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 1.38 GiB (GPU 0; 11.17 GiB total capacity; 8.39 GiB already allocated; 1.09 GiB free; 9.56 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
            "2022-03-10 06:19:27 | WARNING | fairseq.trainer | |===========================================================================|\n",
            "|                  PyTorch CUDA memory summary, device ID 0                 |\n",
            "|---------------------------------------------------------------------------|\n",
            "|            CUDA OOMs: 33           |        cudaMalloc retries: 71        |\n",
            "|===========================================================================|\n",
            "|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Allocated memory      |    7180 MB |    8592 MB |   10573 GB |   10566 GB |\n",
            "|       from large pool |    7131 MB |    8544 MB |   10125 GB |   10118 GB |\n",
            "|       from small pool |      48 MB |      60 MB |     448 GB |     448 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Active memory         |    7180 MB |    8592 MB |   10573 GB |   10566 GB |\n",
            "|       from large pool |    7131 MB |    8544 MB |   10125 GB |   10118 GB |\n",
            "|       from small pool |      48 MB |      60 MB |     448 GB |     448 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| GPU reserved memory   |    9788 MB |    9788 MB |  132584 MB |  122796 MB |\n",
            "|       from large pool |    9730 MB |    9730 MB |  128788 MB |  119058 MB |\n",
            "|       from small pool |      58 MB |     134 MB |    3796 MB |    3738 MB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Non-releasable memory |    1193 MB |    1195 MB |   11439 GB |   11438 GB |\n",
            "|       from large pool |    1184 MB |    1185 MB |   10949 GB |   10948 GB |\n",
            "|       from small pool |       9 MB |      13 MB |     489 GB |     489 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Allocations           |     592    |     595    |    5005 K  |    5004 K  |\n",
            "|       from large pool |     160    |     164    |    1694 K  |    1693 K  |\n",
            "|       from small pool |     432    |     516    |    3311 K  |    3310 K  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Active allocs         |     592    |     595    |    5005 K  |    5004 K  |\n",
            "|       from large pool |     160    |     164    |    1694 K  |    1693 K  |\n",
            "|       from small pool |     432    |     516    |    3311 K  |    3310 K  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| GPU reserved segments |      77    |     115    |    2079    |    2002    |\n",
            "|       from large pool |      48    |      48    |     181    |     133    |\n",
            "|       from small pool |      29    |      67    |    1898    |    1869    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Non-releasable allocs |      63    |      64    |    2252 K  |    2252 K  |\n",
            "|       from large pool |      34    |      35    |    1061 K  |    1061 K  |\n",
            "|       from small pool |      29    |      41    |    1190 K  |    1190 K  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Oversize allocations  |       0    |       0    |       0    |       0    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Oversize GPU segments |       0    |       0    |       0    |       0    |\n",
            "|===========================================================================|\n",
            "\n",
            "2022-03-10 06:19:27 | WARNING | fairseq.trainer | attempting to recover from OOM in forward/backward pass\n",
            "epoch 007:  51% 357/694 [01:06<00:52,  6.43it/s, loss=2.583, ppl=5.99, wps=14197.5, ups=5.37, wpb=2642.6, bsz=128, num_updates=4400, lr=0.0005, gnorm=1.005, train_wall=18, gb_free=10.5, wall=844]2022-03-10 06:19:40 | WARNING | fairseq.trainer | OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 1.93 GiB (GPU 0; 11.17 GiB total capacity; 8.11 GiB already allocated; 1.84 GiB free; 8.82 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
            "epoch 007:  52% 359/694 [01:07<01:25,  3.90it/s, loss=2.583, ppl=5.99, wps=14197.5, ups=5.37, wpb=2642.6, bsz=128, num_updates=4400, lr=0.0005, gnorm=1.005, train_wall=18, gb_free=10.5, wall=844]2022-03-10 06:19:40 | WARNING | fairseq.trainer | |===========================================================================|\n",
            "|                  PyTorch CUDA memory summary, device ID 0                 |\n",
            "|---------------------------------------------------------------------------|\n",
            "|            CUDA OOMs: 34           |        cudaMalloc retries: 73        |\n",
            "|===========================================================================|\n",
            "|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Allocated memory      |    8265 MB |    8302 MB |   10740 GB |   10732 GB |\n",
            "|       from large pool |    8215 MB |    8252 MB |   10286 GB |   10278 GB |\n",
            "|       from small pool |      50 MB |      60 MB |     454 GB |     454 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Active memory         |    8265 MB |    8302 MB |   10740 GB |   10732 GB |\n",
            "|       from large pool |    8215 MB |    8252 MB |   10286 GB |   10278 GB |\n",
            "|       from small pool |      50 MB |      60 MB |     454 GB |     454 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| GPU reserved memory   |    9028 MB |   10906 MB |  136214 MB |  127186 MB |\n",
            "|       from large pool |    8970 MB |   10790 MB |  132288 MB |  123318 MB |\n",
            "|       from small pool |      58 MB |     116 MB |    3926 MB |    3868 MB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Non-releasable memory |  781025 KB |    1790 MB |   11620 GB |   11620 GB |\n",
            "|       from large pool |  772915 KB |    1781 MB |   11124 GB |   11123 GB |\n",
            "|       from small pool |    8110 KB |      41 MB |     496 GB |     496 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Allocations           |     593    |     595    |    5072 K  |    5072 K  |\n",
            "|       from large pool |     162    |     163    |    1716 K  |    1716 K  |\n",
            "|       from small pool |     431    |     516    |    3356 K  |    3356 K  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Active allocs         |     593    |     595    |    5072 K  |    5072 K  |\n",
            "|       from large pool |     162    |     163    |    1716 K  |    1716 K  |\n",
            "|       from small pool |     431    |     516    |    3356 K  |    3356 K  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| GPU reserved segments |      77    |     107    |    2147    |    2070    |\n",
            "|       from large pool |      48    |      49    |     184    |     136    |\n",
            "|       from small pool |      29    |      58    |    1963    |    1934    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Non-releasable allocs |      62    |      65    |    2282 K  |    2282 K  |\n",
            "|       from large pool |      35    |      37    |    1075 K  |    1075 K  |\n",
            "|       from small pool |      27    |      54    |    1206 K  |    1206 K  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Oversize allocations  |       0    |       0    |       0    |       0    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Oversize GPU segments |       0    |       0    |       0    |       0    |\n",
            "|===========================================================================|\n",
            "\n",
            "2022-03-10 06:19:40 | WARNING | fairseq.trainer | attempting to recover from OOM in forward/backward pass\n",
            "epoch 007:  60% 416/694 [01:19<01:23,  3.33it/s, loss=2.704, ppl=6.52, wps=12337.3, ups=4.69, wpb=2633, bsz=127.2, num_updates=4500, lr=0.0005, gnorm=1.077, train_wall=20, gb_free=9.9, wall=865]2022-03-10 06:19:52 | WARNING | fairseq.trainer | OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 1.56 GiB (GPU 0; 11.17 GiB total capacity; 9.43 GiB already allocated; 105.81 MiB free; 10.55 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
            "2022-03-10 06:19:52 | WARNING | fairseq.trainer | |===========================================================================|\n",
            "|                  PyTorch CUDA memory summary, device ID 0                 |\n",
            "|---------------------------------------------------------------------------|\n",
            "|            CUDA OOMs: 35           |        cudaMalloc retries: 75        |\n",
            "|===========================================================================|\n",
            "|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Allocated memory      |    8062 MB |    9658 MB |   10907 GB |   10899 GB |\n",
            "|       from large pool |    8013 MB |    9609 MB |   10447 GB |   10439 GB |\n",
            "|       from small pool |      48 MB |      60 MB |     459 GB |     459 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Active memory         |    8062 MB |    9658 MB |   10907 GB |   10899 GB |\n",
            "|       from large pool |    8013 MB |    9609 MB |   10447 GB |   10439 GB |\n",
            "|       from small pool |      48 MB |      60 MB |     459 GB |     459 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| GPU reserved memory   |   10802 MB |   10802 MB |  140570 MB |  129768 MB |\n",
            "|       from large pool |   10744 MB |   10744 MB |  136562 MB |  125818 MB |\n",
            "|       from small pool |      58 MB |     140 MB |    4008 MB |    3950 MB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Non-releasable memory |    1141 MB |    1469 MB |   11804 GB |   11802 GB |\n",
            "|       from large pool |    1132 MB |    1459 MB |   11301 GB |   11300 GB |\n",
            "|       from small pool |       9 MB |      17 MB |     502 GB |     502 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Allocations           |     592    |     595    |    5137 K  |    5137 K  |\n",
            "|       from large pool |     160    |     164    |    1739 K  |    1738 K  |\n",
            "|       from small pool |     432    |     516    |    3398 K  |    3398 K  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Active allocs         |     592    |     595    |    5137 K  |    5137 K  |\n",
            "|       from large pool |     160    |     164    |    1739 K  |    1738 K  |\n",
            "|       from small pool |     432    |     516    |    3398 K  |    3398 K  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| GPU reserved segments |      78    |     119    |    2191    |    2113    |\n",
            "|       from large pool |      49    |      49    |     187    |     138    |\n",
            "|       from small pool |      29    |      70    |    2004    |    1975    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Non-releasable allocs |      54    |      55    |    2312 K  |    2312 K  |\n",
            "|       from large pool |      27    |      28    |    1090 K  |    1090 K  |\n",
            "|       from small pool |      27    |      41    |    1221 K  |    1221 K  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Oversize allocations  |       0    |       0    |       0    |       0    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Oversize GPU segments |       0    |       0    |       0    |       0    |\n",
            "|===========================================================================|\n",
            "\n",
            "2022-03-10 06:19:52 | WARNING | fairseq.trainer | attempting to recover from OOM in forward/backward pass\n",
            "epoch 007:  82% 572/694 [01:49<00:24,  4.97it/s, loss=2.598, ppl=6.05, wps=14124.9, ups=5.59, wpb=2527.1, bsz=128, num_updates=4700, lr=0.0005, gnorm=1.025, train_wall=17, gb_free=8.9, wall=905]2022-03-10 06:20:22 | WARNING | fairseq.trainer | OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 1.78 GiB (GPU 0; 11.17 GiB total capacity; 7.62 GiB already allocated; 1.51 GiB free; 9.14 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
            "2022-03-10 06:20:22 | WARNING | fairseq.trainer | |===========================================================================|\n",
            "|                  PyTorch CUDA memory summary, device ID 0                 |\n",
            "|---------------------------------------------------------------------------|\n",
            "|            CUDA OOMs: 36           |        cudaMalloc retries: 78        |\n",
            "|===========================================================================|\n",
            "|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Allocated memory      |    7807 MB |    7820 MB |   11285 GB |   11277 GB |\n",
            "|       from large pool |    7758 MB |    7771 MB |   10809 GB |   10802 GB |\n",
            "|       from small pool |      49 MB |      60 MB |     475 GB |     475 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Active memory         |    7807 MB |    7820 MB |   11285 GB |   11277 GB |\n",
            "|       from large pool |    7758 MB |    7771 MB |   10809 GB |   10802 GB |\n",
            "|       from small pool |      49 MB |      60 MB |     475 GB |     475 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| GPU reserved memory   |    9364 MB |    9442 MB |  145198 MB |  135834 MB |\n",
            "|       from large pool |    9304 MB |    9304 MB |  140970 MB |  131666 MB |\n",
            "|       from small pool |      60 MB |     138 MB |    4228 MB |    4168 MB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Non-releasable memory |    1556 MB |    1714 MB |   12218 GB |   12216 GB |\n",
            "|       from large pool |    1545 MB |    1703 MB |   11698 GB |   11697 GB |\n",
            "|       from small pool |      10 MB |      14 MB |     519 GB |     519 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Allocations           |     592    |     595    |    5312 K  |    5311 K  |\n",
            "|       from large pool |     162    |     164    |    1798 K  |    1798 K  |\n",
            "|       from small pool |     430    |     515    |    3514 K  |    3513 K  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Active allocs         |     592    |     595    |    5312 K  |    5311 K  |\n",
            "|       from large pool |     162    |     164    |    1798 K  |    1798 K  |\n",
            "|       from small pool |     430    |     515    |    3514 K  |    3513 K  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| GPU reserved segments |      78    |     117    |    2305    |    2227    |\n",
            "|       from large pool |      48    |      48    |     191    |     143    |\n",
            "|       from small pool |      30    |      69    |    2114    |    2084    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Non-releasable allocs |      61    |      61    |    2391 K  |    2391 K  |\n",
            "|       from large pool |      29    |      30    |    1127 K  |    1127 K  |\n",
            "|       from small pool |      32    |      34    |    1263 K  |    1263 K  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Oversize allocations  |       0    |       0    |       0    |       0    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Oversize GPU segments |       0    |       0    |       0    |       0    |\n",
            "|===========================================================================|\n",
            "\n",
            "2022-03-10 06:20:22 | WARNING | fairseq.trainer | attempting to recover from OOM in forward/backward pass\n",
            "epoch 007: 100% 693/694 [02:10<00:00,  8.08it/s, loss=2.541, ppl=5.82, wps=14231.3, ups=5.71, wpb=2492.5, bsz=128, num_updates=4800, lr=0.0005, gnorm=1.028, train_wall=17, gb_free=10.3, wall=922]2022-03-10 06:20:43 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "\n",
            "epoch 007 | valid on 'valid' subset:   0% 0/8 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 007 | valid on 'valid' subset:  12% 1/8 [00:00<00:01,  6.62it/s]\u001b[A\n",
            "epoch 007 | valid on 'valid' subset:  38% 3/8 [00:00<00:00, 12.02it/s]\u001b[A\n",
            "epoch 007 | valid on 'valid' subset:  62% 5/8 [00:00<00:00, 11.83it/s]\u001b[A\n",
            "epoch 007 | valid on 'valid' subset:  88% 7/8 [00:00<00:00, 10.05it/s]\u001b[A\n",
            "                                                                      \u001b[A2022-03-10 06:20:44 | INFO | valid | epoch 007 | valid on 'valid' subset | loss 3.829 | ppl 14.21 | wps 26151.4 | wpb 2673.1 | bsz 125 | num_updates 4822 | best_loss 3.748\n",
            "2022-03-10 06:20:44 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 7 @ 4822 updates\n",
            "2022-03-10 06:20:44 | INFO | fairseq.trainer | Saving checkpoint to /content/checkpoints/checkpoint7.pt\n",
            "2022-03-10 06:20:44 | INFO | fairseq.trainer | Finished saving checkpoint to /content/checkpoints/checkpoint7.pt\n",
            "2022-03-10 06:20:45 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/checkpoint7.pt (epoch 7 @ 4822 updates, score 3.829) (writing took 1.3411831029998211 seconds)\n",
            "2022-03-10 06:20:45 | INFO | fairseq_cli.train | end of epoch 7 (average epoch stats below)\n",
            "2022-03-10 06:20:45 | INFO | train | epoch 007 | loss 2.585 | ppl 6 | wps 13325 | ups 5.2 | wpb 2564 | bsz 127.9 | num_updates 4822 | lr 0.0005 | gnorm 1.018 | train_wall 124 | gb_free 10 | wall 928\n",
            "epoch 008:   0% 0/694 [00:00<?, ?it/s]2022-03-10 06:20:45 | INFO | fairseq.data.iterators | grouped total_num_itrs = 694\n",
            "2022-03-10 06:20:45 | INFO | fairseq.trainer | begin training epoch 8\n",
            "2022-03-10 06:20:45 | INFO | fairseq_cli.train | Start iterating over samples\n",
            "epoch 008:  11% 74/694 [00:12<01:33,  6.64it/s]2022-03-10 06:20:58 | WARNING | fairseq.trainer | OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 1.78 GiB (GPU 0; 11.17 GiB total capacity; 7.62 GiB already allocated; 1.51 GiB free; 9.14 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
            "2022-03-10 06:20:58 | WARNING | fairseq.trainer | |===========================================================================|\n",
            "|                  PyTorch CUDA memory summary, device ID 0                 |\n",
            "|---------------------------------------------------------------------------|\n",
            "|            CUDA OOMs: 37           |        cudaMalloc retries: 79        |\n",
            "|===========================================================================|\n",
            "|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Allocated memory      |    7807 MB |    7820 MB |   11693 GB |   11686 GB |\n",
            "|       from large pool |    7758 MB |    7771 MB |   11196 GB |   11189 GB |\n",
            "|       from small pool |      49 MB |      60 MB |     496 GB |     496 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Active memory         |    7807 MB |    7820 MB |   11693 GB |   11686 GB |\n",
            "|       from large pool |    7758 MB |    7771 MB |   11196 GB |   11189 GB |\n",
            "|       from small pool |      49 MB |      60 MB |     496 GB |     496 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| GPU reserved memory   |    9362 MB |    9442 MB |  145276 MB |  135914 MB |\n",
            "|       from large pool |    9304 MB |    9304 MB |  140970 MB |  131666 MB |\n",
            "|       from small pool |      58 MB |     138 MB |    4306 MB |    4248 MB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Non-releasable memory |    1554 MB |    1712 MB |   12672 GB |   12671 GB |\n",
            "|       from large pool |    1545 MB |    1703 MB |   12129 GB |   12128 GB |\n",
            "|       from small pool |       8 MB |      30 MB |     543 GB |     543 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Allocations           |     592    |     595    |    5534 K  |    5533 K  |\n",
            "|       from large pool |     162    |     164    |    1871 K  |    1871 K  |\n",
            "|       from small pool |     430    |     516    |    3663 K  |    3662 K  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Active allocs         |     592    |     595    |    5534 K  |    5533 K  |\n",
            "|       from large pool |     162    |     164    |    1871 K  |    1871 K  |\n",
            "|       from small pool |     430    |     516    |    3663 K  |    3662 K  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| GPU reserved segments |      77    |     117    |    2344    |    2267    |\n",
            "|       from large pool |      48    |      48    |     191    |     143    |\n",
            "|       from small pool |      29    |      69    |    2153    |    2124    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Non-releasable allocs |      60    |      70    |    2492 K  |    2492 K  |\n",
            "|       from large pool |      29    |      30    |    1173 K  |    1173 K  |\n",
            "|       from small pool |      31    |      65    |    1319 K  |    1319 K  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Oversize allocations  |       0    |       0    |       0    |       0    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Oversize GPU segments |       0    |       0    |       0    |       0    |\n",
            "|===========================================================================|\n",
            "\n",
            "2022-03-10 06:20:58 | WARNING | fairseq.trainer | attempting to recover from OOM in forward/backward pass\n",
            "epoch 008:  17% 118/694 [00:18<01:24,  6.80it/s, loss=2.24, ppl=4.72, wps=11937, ups=5.28, wpb=2259.2, bsz=128, num_updates=4900, lr=0.0005, gnorm=1.026, train_wall=16, gb_free=10.3, wall=941]2022-03-10 06:21:04 | WARNING | fairseq.trainer | OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 1.43 GiB (GPU 0; 11.17 GiB total capacity; 8.46 GiB already allocated; 1.17 GiB free; 9.48 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
            "2022-03-10 06:21:04 | WARNING | fairseq.trainer | |===========================================================================|\n",
            "|                  PyTorch CUDA memory summary, device ID 0                 |\n",
            "|---------------------------------------------------------------------------|\n",
            "|            CUDA OOMs: 38           |        cudaMalloc retries: 80        |\n",
            "|===========================================================================|\n",
            "|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Allocated memory      |    7196 MB |    8662 MB |   11769 GB |   11762 GB |\n",
            "|       from large pool |    7148 MB |    8614 MB |   11267 GB |   11260 GB |\n",
            "|       from small pool |      48 MB |      60 MB |     502 GB |     502 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Active memory         |    7196 MB |    8662 MB |   11769 GB |   11762 GB |\n",
            "|       from large pool |    7148 MB |    8614 MB |   11267 GB |   11260 GB |\n",
            "|       from small pool |      48 MB |      60 MB |     502 GB |     502 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| GPU reserved memory   |    9710 MB |   10902 MB |  146816 MB |  137106 MB |\n",
            "|       from large pool |    9652 MB |   10770 MB |  142436 MB |  132784 MB |\n",
            "|       from small pool |      58 MB |     132 MB |    4380 MB |    4322 MB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Non-releasable memory |    1047 MB |    1229 MB |   12758 GB |   12757 GB |\n",
            "|       from large pool |    1037 MB |    1219 MB |   12210 GB |   12209 GB |\n",
            "|       from small pool |       9 MB |      18 MB |     548 GB |     548 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Allocations           |     592    |     595    |    5581 K  |    5581 K  |\n",
            "|       from large pool |     160    |     164    |    1885 K  |    1885 K  |\n",
            "|       from small pool |     432    |     516    |    3695 K  |    3695 K  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Active allocs         |     592    |     595    |    5581 K  |    5581 K  |\n",
            "|       from large pool |     160    |     164    |    1885 K  |    1885 K  |\n",
            "|       from small pool |     432    |     516    |    3695 K  |    3695 K  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| GPU reserved segments |      77    |     115    |    2382    |    2305    |\n",
            "|       from large pool |      48    |      49    |     192    |     144    |\n",
            "|       from small pool |      29    |      66    |    2190    |    2161    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Non-releasable allocs |      61    |      61    |    2514 K  |    2514 K  |\n",
            "|       from large pool |      32    |      32    |    1182 K  |    1182 K  |\n",
            "|       from small pool |      29    |      41    |    1331 K  |    1331 K  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Oversize allocations  |       0    |       0    |       0    |       0    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Oversize GPU segments |       0    |       0    |       0    |       0    |\n",
            "|===========================================================================|\n",
            "\n",
            "2022-03-10 06:21:04 | WARNING | fairseq.trainer | attempting to recover from OOM in forward/backward pass\n",
            "epoch 008:  51% 354/694 [01:04<00:48,  6.96it/s, loss=2.552, ppl=5.86, wps=13124.3, ups=4.89, wpb=2681.6, bsz=127.2, num_updates=5100, lr=0.0005, gnorm=1.089, train_wall=20, gb_free=10.2, wall=980]2022-03-10 06:21:51 | WARNING | fairseq.trainer | OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 1.38 GiB (GPU 0; 11.17 GiB total capacity; 8.39 GiB already allocated; 1.32 GiB free; 9.33 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
            "2022-03-10 06:21:51 | WARNING | fairseq.trainer | |===========================================================================|\n",
            "|                  PyTorch CUDA memory summary, device ID 0                 |\n",
            "|---------------------------------------------------------------------------|\n",
            "|            CUDA OOMs: 39           |        cudaMalloc retries: 83        |\n",
            "|===========================================================================|\n",
            "|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Allocated memory      |    7180 MB |    8592 MB |   12362 GB |   12355 GB |\n",
            "|       from large pool |    7131 MB |    8544 MB |   11837 GB |   11830 GB |\n",
            "|       from small pool |      48 MB |      60 MB |     525 GB |     525 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Active memory         |    7180 MB |    8592 MB |   12362 GB |   12355 GB |\n",
            "|       from large pool |    7131 MB |    8544 MB |   11837 GB |   11830 GB |\n",
            "|       from small pool |      48 MB |      60 MB |     525 GB |     525 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| GPU reserved memory   |    9552 MB |   10718 MB |  151886 MB |  142334 MB |\n",
            "|       from large pool |    9494 MB |   10580 MB |  147362 MB |  137868 MB |\n",
            "|       from small pool |      58 MB |     138 MB |    4524 MB |    4466 MB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Non-releasable memory |     957 MB |    1047 MB |   13404 GB |   13403 GB |\n",
            "|       from large pool |     948 MB |    1037 MB |   12829 GB |   12828 GB |\n",
            "|       from small pool |       9 MB |      15 MB |     574 GB |     574 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Allocations           |     592    |     595    |    5846 K  |    5845 K  |\n",
            "|       from large pool |     160    |     164    |    1975 K  |    1975 K  |\n",
            "|       from small pool |     432    |     516    |    3870 K  |    3870 K  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Active allocs         |     592    |     595    |    5846 K  |    5845 K  |\n",
            "|       from large pool |     160    |     164    |    1975 K  |    1975 K  |\n",
            "|       from small pool |     432    |     516    |    3870 K  |    3870 K  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| GPU reserved segments |      77    |     118    |    2458    |    2381    |\n",
            "|       from large pool |      48    |      49    |     196    |     148    |\n",
            "|       from small pool |      29    |      69    |    2262    |    2233    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Non-releasable allocs |      63    |      64    |    2633 K  |    2633 K  |\n",
            "|       from large pool |      34    |      35    |    1239 K  |    1239 K  |\n",
            "|       from small pool |      29    |      38    |    1394 K  |    1394 K  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Oversize allocations  |       0    |       0    |       0    |       0    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Oversize GPU segments |       0    |       0    |       0    |       0    |\n",
            "|===========================================================================|\n",
            "\n",
            "2022-03-10 06:21:51 | WARNING | fairseq.trainer | attempting to recover from OOM in forward/backward pass\n",
            "epoch 008:  76% 529/694 [01:38<00:24,  6.85it/s, loss=2.54, ppl=5.82, wps=14273.7, ups=5.07, wpb=2813.7, bsz=128, num_updates=5300, lr=0.0005, gnorm=1.044, train_wall=19, gb_free=10.4, wall=1018]2022-03-10 06:22:25 | WARNING | fairseq.trainer | OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 1.93 GiB (GPU 0; 11.17 GiB total capacity; 8.11 GiB already allocated; 1.93 GiB free; 8.72 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
            "2022-03-10 06:22:25 | WARNING | fairseq.trainer | |===========================================================================|\n",
            "|                  PyTorch CUDA memory summary, device ID 0                 |\n",
            "|---------------------------------------------------------------------------|\n",
            "|            CUDA OOMs: 40           |        cudaMalloc retries: 84        |\n",
            "|===========================================================================|\n",
            "|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Allocated memory      |    8265 MB |    8303 MB |   12797 GB |   12788 GB |\n",
            "|       from large pool |    8215 MB |    8253 MB |   12254 GB |   12246 GB |\n",
            "|       from small pool |      50 MB |      60 MB |     542 GB |     542 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Active memory         |    8265 MB |    8303 MB |   12797 GB |   12788 GB |\n",
            "|       from large pool |    8215 MB |    8253 MB |   12254 GB |   12246 GB |\n",
            "|       from small pool |      50 MB |      60 MB |     542 GB |     542 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| GPU reserved memory   |    8934 MB |   10834 MB |  154582 MB |  145648 MB |\n",
            "|       from large pool |    8876 MB |   10696 MB |  149978 MB |  141102 MB |\n",
            "|       from small pool |      58 MB |     138 MB |    4604 MB |    4546 MB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Non-releasable memory |  684505 KB |    1695 MB |   13888 GB |   13888 GB |\n",
            "|       from large pool |  676395 KB |    1687 MB |   13295 GB |   13295 GB |\n",
            "|       from small pool |    8110 KB |      36 MB |     593 GB |     593 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Allocations           |     593    |     595    |    6044 K  |    6043 K  |\n",
            "|       from large pool |     162    |     163    |    2043 K  |    2043 K  |\n",
            "|       from small pool |     431    |     516    |    4000 K  |    4000 K  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Active allocs         |     593    |     595    |    6044 K  |    6043 K  |\n",
            "|       from large pool |     162    |     163    |    2043 K  |    2043 K  |\n",
            "|       from small pool |     431    |     516    |    4000 K  |    4000 K  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| GPU reserved segments |      77    |     118    |    2500    |    2423    |\n",
            "|       from large pool |      48    |      49    |     198    |     150    |\n",
            "|       from small pool |      29    |      69    |    2302    |    2273    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Non-releasable allocs |      65    |      66    |    2723 K  |    2723 K  |\n",
            "|       from large pool |      36    |      37    |    1283 K  |    1283 K  |\n",
            "|       from small pool |      29    |      61    |    1440 K  |    1440 K  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Oversize allocations  |       0    |       0    |       0    |       0    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Oversize GPU segments |       0    |       0    |       0    |       0    |\n",
            "|===========================================================================|\n",
            "\n",
            "2022-03-10 06:22:25 | WARNING | fairseq.trainer | attempting to recover from OOM in forward/backward pass\n",
            "epoch 008:  82% 568/694 [01:46<00:17,  7.09it/s, loss=2.54, ppl=5.82, wps=14273.7, ups=5.07, wpb=2813.7, bsz=128, num_updates=5300, lr=0.0005, gnorm=1.044, train_wall=19, gb_free=10.4, wall=1018]2022-03-10 06:22:32 | WARNING | fairseq.trainer | OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 1.56 GiB (GPU 0; 11.17 GiB total capacity; 9.43 GiB already allocated; 275.81 MiB free; 10.38 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
            "2022-03-10 06:22:32 | WARNING | fairseq.trainer | |===========================================================================|\n",
            "|                  PyTorch CUDA memory summary, device ID 0                 |\n",
            "|---------------------------------------------------------------------------|\n",
            "|            CUDA OOMs: 41           |        cudaMalloc retries: 86        |\n",
            "|===========================================================================|\n",
            "|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Allocated memory      |    8062 MB |    9658 MB |   12899 GB |   12891 GB |\n",
            "|       from large pool |    8013 MB |    9609 MB |   12352 GB |   12344 GB |\n",
            "|       from small pool |      48 MB |      60 MB |     547 GB |     546 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Active memory         |    8062 MB |    9658 MB |   12899 GB |   12891 GB |\n",
            "|       from large pool |    8013 MB |    9609 MB |   12352 GB |   12344 GB |\n",
            "|       from small pool |      48 MB |      60 MB |     547 GB |     546 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| GPU reserved memory   |   10632 MB |   10632 MB |  158976 MB |  148344 MB |\n",
            "|       from large pool |   10574 MB |   10574 MB |  154292 MB |  143718 MB |\n",
            "|       from small pool |      58 MB |     138 MB |    4684 MB |    4626 MB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Non-releasable memory |     971 MB |    1481 MB |   14001 GB |   14000 GB |\n",
            "|       from large pool |     962 MB |    1471 MB |   13403 GB |   13402 GB |\n",
            "|       from small pool |       9 MB |      30 MB |     597 GB |     597 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Allocations           |     592    |     595    |    6086 K  |    6085 K  |\n",
            "|       from large pool |     160    |     164    |    2057 K  |    2057 K  |\n",
            "|       from small pool |     432    |     516    |    4028 K  |    4028 K  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Active allocs         |     592    |     595    |    6086 K  |    6085 K  |\n",
            "|       from large pool |     160    |     164    |    2057 K  |    2057 K  |\n",
            "|       from small pool |     432    |     516    |    4028 K  |    4028 K  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| GPU reserved segments |      78    |     118    |    2543    |    2465    |\n",
            "|       from large pool |      49    |      49    |     201    |     152    |\n",
            "|       from small pool |      29    |      69    |    2342    |    2313    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Non-releasable allocs |      56    |      57    |    2742 K  |    2742 K  |\n",
            "|       from large pool |      27    |      28    |    1291 K  |    1291 K  |\n",
            "|       from small pool |      29    |      52    |    1451 K  |    1451 K  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Oversize allocations  |       0    |       0    |       0    |       0    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Oversize GPU segments |       0    |       0    |       0    |       0    |\n",
            "|===========================================================================|\n",
            "\n",
            "2022-03-10 06:22:32 | WARNING | fairseq.trainer | attempting to recover from OOM in forward/backward pass\n",
            "epoch 008: 100% 693/694 [02:09<00:00,  5.65it/s, loss=2.412, ppl=5.32, wps=14118.8, ups=5.62, wpb=2511.6, bsz=128, num_updates=5500, lr=0.0005, gnorm=1.045, train_wall=17, gb_free=10.4, wall=1057]2022-03-10 06:22:55 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "\n",
            "epoch 008 | valid on 'valid' subset:   0% 0/8 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 008 | valid on 'valid' subset:  12% 1/8 [00:00<00:00,  7.40it/s]\u001b[A\n",
            "epoch 008 | valid on 'valid' subset:  38% 3/8 [00:00<00:00, 12.61it/s]\u001b[A\n",
            "epoch 008 | valid on 'valid' subset:  62% 5/8 [00:00<00:00, 12.11it/s]\u001b[A\n",
            "epoch 008 | valid on 'valid' subset:  88% 7/8 [00:00<00:00, 10.10it/s]\u001b[A\n",
            "                                                                      \u001b[A2022-03-10 06:22:56 | INFO | valid | epoch 008 | valid on 'valid' subset | loss 3.829 | ppl 14.21 | wps 26269.3 | wpb 2673.1 | bsz 125 | num_updates 5511 | best_loss 3.748\n",
            "2022-03-10 06:22:56 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 8 @ 5511 updates\n",
            "2022-03-10 06:22:56 | INFO | fairseq.trainer | Saving checkpoint to /content/checkpoints/checkpoint8.pt\n",
            "2022-03-10 06:22:57 | INFO | fairseq.trainer | Finished saving checkpoint to /content/checkpoints/checkpoint8.pt\n",
            "2022-03-10 06:22:57 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/checkpoint8.pt (epoch 8 @ 5511 updates, score 3.829) (writing took 1.1163326440000674 seconds)\n",
            "2022-03-10 06:22:57 | INFO | fairseq_cli.train | end of epoch 8 (average epoch stats below)\n",
            "2022-03-10 06:22:57 | INFO | train | epoch 008 | loss 2.42 | ppl 5.35 | wps 13365.9 | ups 5.21 | wpb 2564 | bsz 127.9 | num_updates 5511 | lr 0.0005 | gnorm 1.051 | train_wall 124 | gb_free 10.1 | wall 1060\n",
            "epoch 009:   0% 0/694 [00:00<?, ?it/s]2022-03-10 06:22:57 | INFO | fairseq.data.iterators | grouped total_num_itrs = 694\n",
            "2022-03-10 06:22:57 | INFO | fairseq.trainer | begin training epoch 9\n",
            "2022-03-10 06:22:57 | INFO | fairseq_cli.train | Start iterating over samples\n",
            "epoch 009:   1% 7/694 [00:01<02:35,  4.43it/s]2022-03-10 06:23:00 | WARNING | fairseq.trainer | OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 1.78 GiB (GPU 0; 11.17 GiB total capacity; 7.63 GiB already allocated; 1.61 GiB free; 9.04 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
            "2022-03-10 06:23:00 | WARNING | fairseq.trainer | |===========================================================================|\n",
            "|                  PyTorch CUDA memory summary, device ID 0                 |\n",
            "|---------------------------------------------------------------------------|\n",
            "|            CUDA OOMs: 42           |        cudaMalloc retries: 88        |\n",
            "|===========================================================================|\n",
            "|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Allocated memory      |    7808 MB |    7820 MB |   13226 GB |   13218 GB |\n",
            "|       from large pool |    7758 MB |    7771 MB |   12667 GB |   12660 GB |\n",
            "|       from small pool |      49 MB |      60 MB |     558 GB |     558 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Active memory         |    7808 MB |    7820 MB |   13226 GB |   13218 GB |\n",
            "|       from large pool |    7758 MB |    7771 MB |   12667 GB |   12660 GB |\n",
            "|       from small pool |      49 MB |      60 MB |     558 GB |     558 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| GPU reserved memory   |    9256 MB |    9256 MB |  160852 MB |  151596 MB |\n",
            "|       from large pool |    9198 MB |    9198 MB |  156112 MB |  146914 MB |\n",
            "|       from small pool |      58 MB |     114 MB |    4740 MB |    4682 MB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Non-releasable memory |    1447 MB |    2047 MB |   14370 GB |   14369 GB |\n",
            "|       from large pool |    1439 MB |    2038 MB |   13760 GB |   13758 GB |\n",
            "|       from small pool |       8 MB |      13 MB |     610 GB |     610 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Allocations           |     592    |     595    |    6237 K  |    6236 K  |\n",
            "|       from large pool |     162    |     164    |    2111 K  |    2111 K  |\n",
            "|       from small pool |     430    |     516    |    4125 K  |    4125 K  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Active allocs         |     592    |     595    |    6237 K  |    6236 K  |\n",
            "|       from large pool |     162    |     164    |    2111 K  |    2111 K  |\n",
            "|       from small pool |     430    |     516    |    4125 K  |    4125 K  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| GPU reserved segments |      77    |     105    |    2572    |    2495    |\n",
            "|       from large pool |      48    |      48    |     202    |     154    |\n",
            "|       from small pool |      29    |      57    |    2370    |    2341    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Non-releasable allocs |      56    |      56    |    2811 K  |    2811 K  |\n",
            "|       from large pool |      29    |      29    |    1326 K  |    1326 K  |\n",
            "|       from small pool |      27    |      34    |    1485 K  |    1485 K  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Oversize allocations  |       0    |       0    |       0    |       0    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Oversize GPU segments |       0    |       0    |       0    |       0    |\n",
            "|===========================================================================|\n",
            "\n",
            "2022-03-10 06:23:00 | WARNING | fairseq.trainer | attempting to recover from OOM in forward/backward pass\n",
            "epoch 009:  21% 148/694 [00:27<02:33,  3.55it/s, loss=2.125, ppl=4.36, wps=12317.7, ups=4.9, wpb=2515.1, bsz=128, num_updates=5600, lr=0.0005, gnorm=1.002, train_wall=17, gb_free=9.7, wall=1077]2022-03-10 06:23:25 | WARNING | fairseq.trainer | OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 1.93 GiB (GPU 0; 11.17 GiB total capacity; 8.11 GiB already allocated; 573.81 MiB free; 10.09 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
            "2022-03-10 06:23:25 | WARNING | fairseq.trainer | |===========================================================================|\n",
            "|                  PyTorch CUDA memory summary, device ID 0                 |\n",
            "|---------------------------------------------------------------------------|\n",
            "|            CUDA OOMs: 43           |        cudaMalloc retries: 89        |\n",
            "|===========================================================================|\n",
            "|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Allocated memory      |    8264 MB |    8301 MB |   13544 GB |   13536 GB |\n",
            "|       from large pool |    8214 MB |    8251 MB |   12972 GB |   12964 GB |\n",
            "|       from small pool |      50 MB |      60 MB |     571 GB |     571 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Active memory         |    8264 MB |    8301 MB |   13544 GB |   13536 GB |\n",
            "|       from large pool |    8214 MB |    8251 MB |   12972 GB |   12964 GB |\n",
            "|       from small pool |      50 MB |      60 MB |     571 GB |     571 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| GPU reserved memory   |   10334 MB |   10410 MB |  162006 MB |  151672 MB |\n",
            "|       from large pool |   10276 MB |   10276 MB |  157190 MB |  146914 MB |\n",
            "|       from small pool |      58 MB |     134 MB |    4816 MB |    4758 MB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Non-releasable memory |    2069 MB |    2086 MB |   14709 GB |   14707 GB |\n",
            "|       from large pool |    2061 MB |    2078 MB |   14084 GB |   14082 GB |\n",
            "|       from small pool |       7 MB |      47 MB |     625 GB |     625 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Allocations           |     593    |     595    |    6396 K  |    6395 K  |\n",
            "|       from large pool |     162    |     163    |    2165 K  |    2165 K  |\n",
            "|       from small pool |     431    |     516    |    4230 K  |    4229 K  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Active allocs         |     593    |     595    |    6396 K  |    6395 K  |\n",
            "|       from large pool |     162    |     163    |    2165 K  |    2165 K  |\n",
            "|       from small pool |     431    |     516    |    4230 K  |    4229 K  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| GPU reserved segments |      78    |     116    |    2611    |    2533    |\n",
            "|       from large pool |      49    |      49    |     203    |     154    |\n",
            "|       from small pool |      29    |      67    |    2408    |    2379    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Non-releasable allocs |      67    |      68    |    2883 K  |    2883 K  |\n",
            "|       from large pool |      38    |      38    |    1361 K  |    1360 K  |\n",
            "|       from small pool |      29    |      60    |    1522 K  |    1522 K  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Oversize allocations  |       0    |       0    |       0    |       0    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Oversize GPU segments |       0    |       0    |       0    |       0    |\n",
            "|===========================================================================|\n",
            "\n",
            "2022-03-10 06:23:25 | WARNING | fairseq.trainer | attempting to recover from OOM in forward/backward pass\n",
            "epoch 009:  76% 527/694 [01:36<00:41,  4.03it/s, loss=2.356, ppl=5.12, wps=14068.7, ups=5.44, wpb=2587.7, bsz=128, num_updates=6000, lr=0.0005, gnorm=1.098, train_wall=18, gb_free=10.6, wall=1150]2022-03-10 06:24:35 | WARNING | fairseq.trainer | OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 1.43 GiB (GPU 0; 11.17 GiB total capacity; 8.46 GiB already allocated; 1.27 GiB free; 9.38 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
            "2022-03-10 06:24:35 | WARNING | fairseq.trainer | |===========================================================================|\n",
            "|                  PyTorch CUDA memory summary, device ID 0                 |\n",
            "|---------------------------------------------------------------------------|\n",
            "|            CUDA OOMs: 44           |        cudaMalloc retries: 92        |\n",
            "|===========================================================================|\n",
            "|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Allocated memory      |    7196 MB |    8662 MB |   14417 GB |   14410 GB |\n",
            "|       from large pool |    7148 MB |    8614 MB |   13806 GB |   13799 GB |\n",
            "|       from small pool |      48 MB |      60 MB |     611 GB |     611 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Active memory         |    7196 MB |    8662 MB |   14417 GB |   14410 GB |\n",
            "|       from large pool |    7148 MB |    8614 MB |   13806 GB |   13799 GB |\n",
            "|       from small pool |      48 MB |      60 MB |     611 GB |     611 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| GPU reserved memory   |    9604 MB |   10718 MB |  166132 MB |  156528 MB |\n",
            "|       from large pool |    9546 MB |   10580 MB |  161156 MB |  151610 MB |\n",
            "|       from small pool |      58 MB |     138 MB |    4976 MB |    4918 MB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Non-releasable memory |     941 MB |    1123 MB |   15692 GB |   15691 GB |\n",
            "|       from large pool |     931 MB |    1113 MB |   15023 GB |   15023 GB |\n",
            "|       from small pool |       9 MB |      12 MB |     668 GB |     668 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Allocations           |     592    |     595    |    6820 K  |    6820 K  |\n",
            "|       from large pool |     160    |     164    |    2307 K  |    2307 K  |\n",
            "|       from small pool |     432    |     516    |    4513 K  |    4512 K  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Active allocs         |     592    |     595    |    6820 K  |    6820 K  |\n",
            "|       from large pool |     160    |     164    |    2307 K  |    2307 K  |\n",
            "|       from small pool |     432    |     516    |    4513 K  |    4512 K  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| GPU reserved segments |      77    |     118    |    2694    |    2617    |\n",
            "|       from large pool |      48    |      49    |     206    |     158    |\n",
            "|       from small pool |      29    |      69    |    2488    |    2459    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Non-releasable allocs |      63    |      63    |    3077 K  |    3077 K  |\n",
            "|       from large pool |      32    |      32    |    1450 K  |    1450 K  |\n",
            "|       from small pool |      31    |      33    |    1626 K  |    1626 K  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Oversize allocations  |       0    |       0    |       0    |       0    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Oversize GPU segments |       0    |       0    |       0    |       0    |\n",
            "|===========================================================================|\n",
            "\n",
            "2022-03-10 06:24:35 | WARNING | fairseq.trainer | attempting to recover from OOM in forward/backward pass\n",
            "epoch 009:  82% 569/694 [01:45<00:23,  5.36it/s, loss=2.356, ppl=5.12, wps=14068.7, ups=5.44, wpb=2587.7, bsz=128, num_updates=6000, lr=0.0005, gnorm=1.098, train_wall=18, gb_free=10.6, wall=1150]2022-03-10 06:24:44 | WARNING | fairseq.trainer | OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 1.38 GiB (GPU 0; 11.17 GiB total capacity; 8.39 GiB already allocated; 1.32 GiB free; 9.33 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
            "2022-03-10 06:24:44 | WARNING | fairseq.trainer | |===========================================================================|\n",
            "|                  PyTorch CUDA memory summary, device ID 0                 |\n",
            "|---------------------------------------------------------------------------|\n",
            "|            CUDA OOMs: 45           |        cudaMalloc retries: 94        |\n",
            "|===========================================================================|\n",
            "|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Allocated memory      |    7180 MB |    8592 MB |   14534 GB |   14527 GB |\n",
            "|       from large pool |    7131 MB |    8544 MB |   13919 GB |   13912 GB |\n",
            "|       from small pool |      48 MB |      60 MB |     614 GB |     614 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Active memory         |    7180 MB |    8592 MB |   14534 GB |   14527 GB |\n",
            "|       from large pool |    7131 MB |    8544 MB |   13919 GB |   13912 GB |\n",
            "|       from small pool |      48 MB |      60 MB |     614 GB |     614 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| GPU reserved memory   |    9552 MB |   10424 MB |  169832 MB |  160280 MB |\n",
            "|       from large pool |    9494 MB |   10316 MB |  164806 MB |  155312 MB |\n",
            "|       from small pool |      58 MB |     108 MB |    5026 MB |    4968 MB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Non-releasable memory |     957 MB |    1047 MB |   15817 GB |   15816 GB |\n",
            "|       from large pool |     948 MB |    1037 MB |   15145 GB |   15144 GB |\n",
            "|       from small pool |       9 MB |      13 MB |     672 GB |     672 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Allocations           |     592    |     595    |    6867 K  |    6867 K  |\n",
            "|       from large pool |     160    |     164    |    2324 K  |    2324 K  |\n",
            "|       from small pool |     432    |     516    |    4543 K  |    4542 K  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Active allocs         |     592    |     595    |    6867 K  |    6867 K  |\n",
            "|       from large pool |     160    |     164    |    2324 K  |    2324 K  |\n",
            "|       from small pool |     432    |     516    |    4543 K  |    4542 K  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| GPU reserved segments |      77    |     103    |    2722    |    2645    |\n",
            "|       from large pool |      48    |      49    |     209    |     161    |\n",
            "|       from small pool |      29    |      54    |    2513    |    2484    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Non-releasable allocs |      60    |      61    |    3098 K  |    3098 K  |\n",
            "|       from large pool |      34    |      35    |    1461 K  |    1461 K  |\n",
            "|       from small pool |      26    |      32    |    1636 K  |    1636 K  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Oversize allocations  |       0    |       0    |       0    |       0    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Oversize GPU segments |       0    |       0    |       0    |       0    |\n",
            "|===========================================================================|\n",
            "\n",
            "2022-03-10 06:24:44 | WARNING | fairseq.trainer | attempting to recover from OOM in forward/backward pass\n",
            "epoch 009:  83% 575/694 [01:47<00:20,  5.86it/s, loss=2.356, ppl=5.12, wps=14068.7, ups=5.44, wpb=2587.7, bsz=128, num_updates=6000, lr=0.0005, gnorm=1.098, train_wall=18, gb_free=10.6, wall=1150]2022-03-10 06:24:45 | WARNING | fairseq.trainer | OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 1.56 GiB (GPU 0; 11.17 GiB total capacity; 7.87 GiB already allocated; 1.14 GiB free; 9.52 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
            "2022-03-10 06:24:45 | WARNING | fairseq.trainer | |===========================================================================|\n",
            "|                  PyTorch CUDA memory summary, device ID 0                 |\n",
            "|---------------------------------------------------------------------------|\n",
            "|            CUDA OOMs: 46           |        cudaMalloc retries: 95        |\n",
            "|===========================================================================|\n",
            "|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Allocated memory      |    8062 MB |    8128 MB |   14552 GB |   14544 GB |\n",
            "|       from large pool |    8013 MB |    8079 MB |   13936 GB |   13929 GB |\n",
            "|       from small pool |      48 MB |      60 MB |     615 GB |     615 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Active memory         |    8062 MB |    8128 MB |   14552 GB |   14544 GB |\n",
            "|       from large pool |    8013 MB |    8079 MB |   13936 GB |   13929 GB |\n",
            "|       from small pool |      48 MB |      60 MB |     615 GB |     615 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| GPU reserved memory   |    9744 MB |    9770 MB |  171464 MB |  161720 MB |\n",
            "|       from large pool |    9678 MB |    9678 MB |  166404 MB |  156726 MB |\n",
            "|       from small pool |      66 MB |      92 MB |    5060 MB |    4994 MB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Non-releasable memory |    1681 MB |    1904 MB |   15836 GB |   15835 GB |\n",
            "|       from large pool |    1664 MB |    1887 MB |   15163 GB |   15162 GB |\n",
            "|       from small pool |      17 MB |      32 MB |     672 GB |     672 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Allocations           |     593    |     595    |    6873 K  |    6872 K  |\n",
            "|       from large pool |     160    |     164    |    2326 K  |    2326 K  |\n",
            "|       from small pool |     433    |     516    |    4546 K  |    4546 K  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Active allocs         |     593    |     595    |    6873 K  |    6872 K  |\n",
            "|       from large pool |     160    |     164    |    2326 K  |    2326 K  |\n",
            "|       from small pool |     433    |     516    |    4546 K  |    4546 K  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| GPU reserved segments |      81    |      94    |    2740    |    2659    |\n",
            "|       from large pool |      48    |      48    |     210    |     162    |\n",
            "|       from small pool |      33    |      46    |    2530    |    2497    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Non-releasable allocs |      59    |      65    |    3100 K  |    3100 K  |\n",
            "|       from large pool |      27    |      27    |    1462 K  |    1462 K  |\n",
            "|       from small pool |      32    |      60    |    1638 K  |    1638 K  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Oversize allocations  |       0    |       0    |       0    |       0    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Oversize GPU segments |       0    |       0    |       0    |       0    |\n",
            "|===========================================================================|\n",
            "\n",
            "2022-03-10 06:24:45 | WARNING | fairseq.trainer | attempting to recover from OOM in forward/backward pass\n",
            "epoch 009: 100% 692/694 [02:09<00:00,  7.46it/s, loss=2.431, ppl=5.39, wps=13313.4, ups=4.78, wpb=2785.2, bsz=128, num_updates=6100, lr=0.0005, gnorm=1.072, train_wall=19, gb_free=10, wall=1171]2022-03-10 06:25:07 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "\n",
            "epoch 009 | valid on 'valid' subset:   0% 0/8 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 009 | valid on 'valid' subset:  12% 1/8 [00:00<00:00,  7.13it/s]\u001b[A\n",
            "epoch 009 | valid on 'valid' subset:  38% 3/8 [00:00<00:00, 11.87it/s]\u001b[A\n",
            "epoch 009 | valid on 'valid' subset:  62% 5/8 [00:00<00:00, 11.35it/s]\u001b[A\n",
            "epoch 009 | valid on 'valid' subset:  88% 7/8 [00:00<00:00,  9.82it/s]\u001b[A\n",
            "                                                                      \u001b[A2022-03-10 06:25:08 | INFO | valid | epoch 009 | valid on 'valid' subset | loss 3.946 | ppl 15.41 | wps 25399.6 | wpb 2673.1 | bsz 125 | num_updates 6200 | best_loss 3.748\n",
            "2022-03-10 06:25:08 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 9 @ 6200 updates\n",
            "2022-03-10 06:25:08 | INFO | fairseq.trainer | Saving checkpoint to /content/checkpoints/checkpoint9.pt\n",
            "2022-03-10 06:25:08 | INFO | fairseq.trainer | Finished saving checkpoint to /content/checkpoints/checkpoint9.pt\n",
            "2022-03-10 06:25:09 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/checkpoint9.pt (epoch 9 @ 6200 updates, score 3.946) (writing took 1.1113611050000145 seconds)\n",
            "2022-03-10 06:25:09 | INFO | fairseq_cli.train | end of epoch 9 (average epoch stats below)\n",
            "2022-03-10 06:25:09 | INFO | train | epoch 009 | loss 2.266 | ppl 4.81 | wps 13404.4 | ups 5.23 | wpb 2564 | bsz 127.9 | num_updates 6200 | lr 0.0005 | gnorm 1.065 | train_wall 124 | gb_free 10.6 | wall 1192\n",
            "epoch 010:   0% 0/694 [00:00<?, ?it/s]2022-03-10 06:25:09 | INFO | fairseq.data.iterators | grouped total_num_itrs = 694\n",
            "2022-03-10 06:25:09 | INFO | fairseq.trainer | begin training epoch 10\n",
            "2022-03-10 06:25:09 | INFO | fairseq_cli.train | Start iterating over samples\n",
            "epoch 010:  25% 172/694 [00:30<01:36,  5.41it/s, loss=2.035, ppl=4.1, wps=13034.6, ups=5.02, wpb=2594.3, bsz=127.2, num_updates=6300, lr=0.0005, gnorm=1.014, train_wall=17, gb_free=9.9, wall=1210]2022-03-10 06:25:40 | WARNING | fairseq.trainer | OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 1.38 GiB (GPU 0; 11.17 GiB total capacity; 8.39 GiB already allocated; 873.81 MiB free; 9.80 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
            "2022-03-10 06:25:40 | WARNING | fairseq.trainer | |===========================================================================|\n",
            "|                  PyTorch CUDA memory summary, device ID 0                 |\n",
            "|---------------------------------------------------------------------------|\n",
            "|            CUDA OOMs: 47           |        cudaMalloc retries: 98        |\n",
            "|===========================================================================|\n",
            "|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Allocated memory      |    7180 MB |    8592 MB |   15234 GB |   15227 GB |\n",
            "|       from large pool |    7131 MB |    8544 MB |   14589 GB |   14582 GB |\n",
            "|       from small pool |      48 MB |      60 MB |     644 GB |     644 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Active memory         |    7180 MB |    8592 MB |   15234 GB |   15227 GB |\n",
            "|       from large pool |    7131 MB |    8544 MB |   14589 GB |   14582 GB |\n",
            "|       from small pool |      48 MB |      60 MB |     644 GB |     644 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| GPU reserved memory   |   10034 MB |   10112 MB |  174150 MB |  164116 MB |\n",
            "|       from large pool |    9974 MB |    9974 MB |  168882 MB |  158908 MB |\n",
            "|       from small pool |      60 MB |     138 MB |    5268 MB |    5208 MB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Non-releasable memory |    1033 MB |    1441 MB |   16619 GB |   16618 GB |\n",
            "|       from large pool |    1022 MB |    1429 MB |   15914 GB |   15913 GB |\n",
            "|       from small pool |      11 MB |      15 MB |     705 GB |     705 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Allocations           |     592    |     595    |    7202 K  |    7201 K  |\n",
            "|       from large pool |     160    |     164    |    2438 K  |    2438 K  |\n",
            "|       from small pool |     432    |     516    |    4763 K  |    4763 K  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Active allocs         |     592    |     595    |    7202 K  |    7201 K  |\n",
            "|       from large pool |     160    |     164    |    2438 K  |    2438 K  |\n",
            "|       from small pool |     432    |     516    |    4763 K  |    4763 K  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| GPU reserved segments |      78    |     117    |    2846    |    2768    |\n",
            "|       from large pool |      48    |      48    |     212    |     164    |\n",
            "|       from small pool |      30    |      69    |    2634    |    2604    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Non-releasable allocs |      63    |      64    |    3250 K  |    3250 K  |\n",
            "|       from large pool |      34    |      35    |    1533 K  |    1533 K  |\n",
            "|       from small pool |      29    |      34    |    1716 K  |    1716 K  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Oversize allocations  |       0    |       0    |       0    |       0    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Oversize GPU segments |       0    |       0    |       0    |       0    |\n",
            "|===========================================================================|\n",
            "\n",
            "2022-03-10 06:25:40 | WARNING | fairseq.trainer | attempting to recover from OOM in forward/backward pass\n",
            "epoch 010:  50% 345/694 [01:05<00:58,  6.00it/s, loss=2.13, ppl=4.38, wps=12619.8, ups=4.88, wpb=2587.4, bsz=128, num_updates=6500, lr=0.0005, gnorm=1.068, train_wall=20, gb_free=10.3, wall=1250]2022-03-10 06:26:16 | WARNING | fairseq.trainer | OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 1.78 GiB (GPU 0; 11.17 GiB total capacity; 9.33 GiB already allocated; 1.09 GiB free; 9.57 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
            "2022-03-10 06:26:16 | WARNING | fairseq.trainer | |===========================================================================|\n",
            "|                  PyTorch CUDA memory summary, device ID 0                 |\n",
            "|---------------------------------------------------------------------------|\n",
            "|            CUDA OOMs: 48           |        cudaMalloc retries: 103       |\n",
            "|===========================================================================|\n",
            "|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Allocated memory      |    9550 MB |    9626 MB |   15671 GB |   15662 GB |\n",
            "|       from large pool |    9500 MB |    9576 MB |   15010 GB |   15001 GB |\n",
            "|       from small pool |      49 MB |      60 MB |     660 GB |     660 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Active memory         |    9550 MB |    9626 MB |   15671 GB |   15662 GB |\n",
            "|       from large pool |    9500 MB |    9576 MB |   15010 GB |   15001 GB |\n",
            "|       from small pool |      49 MB |      60 MB |     660 GB |     660 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| GPU reserved memory   |    9796 MB |   10522 MB |  194442 MB |  184646 MB |\n",
            "|       from large pool |    9736 MB |   10416 MB |  188974 MB |  179238 MB |\n",
            "|       from small pool |      60 MB |     106 MB |    5468 MB |    5408 MB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Non-releasable memory |  251773 KB |    1381 MB |   17070 GB |   17070 GB |\n",
            "|       from large pool |  240767 KB |    1369 MB |   16348 GB |   16347 GB |\n",
            "|       from small pool |   11006 KB |      17 MB |     722 GB |     722 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Allocations           |     593    |     595    |    7396 K  |    7395 K  |\n",
            "|       from large pool |     160    |     164    |    2506 K  |    2505 K  |\n",
            "|       from small pool |     433    |     516    |    4890 K  |    4889 K  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Active allocs         |     593    |     595    |    7396 K  |    7395 K  |\n",
            "|       from large pool |     160    |     164    |    2506 K  |    2505 K  |\n",
            "|       from small pool |     433    |     516    |    4890 K  |    4889 K  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| GPU reserved segments |      63    |      87    |    2980    |    2917    |\n",
            "|       from large pool |      33    |      34    |     246    |     213    |\n",
            "|       from small pool |      30    |      53    |    2734    |    2704    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Non-releasable allocs |      45    |      46    |    3337 K  |    3337 K  |\n",
            "|       from large pool |      20    |      20    |    1575 K  |    1575 K  |\n",
            "|       from small pool |      25    |      37    |    1761 K  |    1761 K  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Oversize allocations  |       0    |       0    |       0    |       0    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Oversize GPU segments |       0    |       0    |       0    |       0    |\n",
            "|===========================================================================|\n",
            "\n",
            "2022-03-10 06:26:16 | WARNING | fairseq.trainer | attempting to recover from OOM in forward/backward pass\n",
            "epoch 010:  77% 532/694 [01:40<00:22,  7.08it/s, loss=2.274, ppl=4.84, wps=13503.7, ups=5.09, wpb=2651.4, bsz=128, num_updates=6700, lr=0.0005, gnorm=1.118, train_wall=19, gb_free=10.4, wall=1288]2022-03-10 06:26:50 | WARNING | fairseq.trainer | OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 1.56 GiB (GPU 0; 11.17 GiB total capacity; 7.87 GiB already allocated; 1.15 GiB free; 9.51 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
            "2022-03-10 06:26:50 | WARNING | fairseq.trainer | |===========================================================================|\n",
            "|                  PyTorch CUDA memory summary, device ID 0                 |\n",
            "|---------------------------------------------------------------------------|\n",
            "|            CUDA OOMs: 49           |        cudaMalloc retries: 104       |\n",
            "|===========================================================================|\n",
            "|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Allocated memory      |    8057 MB |    8123 MB |   16101 GB |   16093 GB |\n",
            "|       from large pool |    8008 MB |    8074 MB |   15422 GB |   15414 GB |\n",
            "|       from small pool |      48 MB |      60 MB |     679 GB |     679 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Active memory         |    8057 MB |    8123 MB |   16101 GB |   16093 GB |\n",
            "|       from large pool |    8008 MB |    8074 MB |   15422 GB |   15414 GB |\n",
            "|       from small pool |      48 MB |      60 MB |     679 GB |     679 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| GPU reserved memory   |    9734 MB |    9874 MB |  194520 MB |  184786 MB |\n",
            "|       from large pool |    9676 MB |    9736 MB |  188974 MB |  179298 MB |\n",
            "|       from small pool |      58 MB |     138 MB |    5546 MB |    5488 MB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Non-releasable memory |    1676 MB |    1706 MB |   17528 GB |   17526 GB |\n",
            "|       from large pool |    1667 MB |    1697 MB |   16785 GB |   16783 GB |\n",
            "|       from small pool |       9 MB |      19 MB |     743 GB |     743 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Allocations           |     593    |     595    |    7605 K  |    7604 K  |\n",
            "|       from large pool |     160    |     164    |    2576 K  |    2576 K  |\n",
            "|       from small pool |     433    |     516    |    5029 K  |    5028 K  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Active allocs         |     593    |     595    |    7605 K  |    7604 K  |\n",
            "|       from large pool |     160    |     164    |    2576 K  |    2576 K  |\n",
            "|       from small pool |     433    |     516    |    5029 K  |    5028 K  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| GPU reserved segments |      61    |     102    |    3019    |    2958    |\n",
            "|       from large pool |      32    |      33    |     246    |     214    |\n",
            "|       from small pool |      29    |      69    |    2773    |    2744    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Non-releasable allocs |      52    |      55    |    3428 K  |    3428 K  |\n",
            "|       from large pool |      23    |      24    |    1617 K  |    1617 K  |\n",
            "|       from small pool |      29    |      38    |    1811 K  |    1811 K  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Oversize allocations  |       0    |       0    |       0    |       0    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Oversize GPU segments |       0    |       0    |       0    |       0    |\n",
            "|===========================================================================|\n",
            "\n",
            "2022-03-10 06:26:50 | WARNING | fairseq.trainer | attempting to recover from OOM in forward/backward pass\n",
            "epoch 010:  97% 674/694 [02:06<00:04,  4.74it/s, loss=2.297, ppl=4.91, wps=13410.9, ups=5.06, wpb=2650.6, bsz=128, num_updates=6800, lr=0.0005, gnorm=1.133, train_wall=19, gb_free=5.7, wall=1308]2022-03-10 06:27:16 | WARNING | fairseq.trainer | OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 1.93 GiB (GPU 0; 11.17 GiB total capacity; 8.11 GiB already allocated; 1.15 GiB free; 9.51 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
            "2022-03-10 06:27:16 | WARNING | fairseq.trainer | |===========================================================================|\n",
            "|                  PyTorch CUDA memory summary, device ID 0                 |\n",
            "|---------------------------------------------------------------------------|\n",
            "|            CUDA OOMs: 50           |        cudaMalloc retries: 105       |\n",
            "|===========================================================================|\n",
            "|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Allocated memory      |    8263 MB |    8301 MB |   16439 GB |   16431 GB |\n",
            "|       from large pool |    8213 MB |    8251 MB |   15744 GB |   15736 GB |\n",
            "|       from small pool |      50 MB |      60 MB |     695 GB |     695 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Active memory         |    8263 MB |    8301 MB |   16439 GB |   16431 GB |\n",
            "|       from large pool |    8213 MB |    8251 MB |   15744 GB |   15736 GB |\n",
            "|       from small pool |      50 MB |      60 MB |     695 GB |     695 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| GPU reserved memory   |    9734 MB |    9814 MB |  194600 MB |  184866 MB |\n",
            "|       from large pool |    9676 MB |    9676 MB |  188974 MB |  179298 MB |\n",
            "|       from small pool |      58 MB |     138 MB |    5626 MB |    5568 MB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Non-releasable memory |    1470 MB |    1859 MB |   17900 GB |   17898 GB |\n",
            "|       from large pool |    1462 MB |    1851 MB |   17139 GB |   17138 GB |\n",
            "|       from small pool |       7 MB |      19 MB |     760 GB |     760 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Allocations           |     593    |     595    |    7764 K  |    7763 K  |\n",
            "|       from large pool |     162    |     163    |    2627 K  |    2627 K  |\n",
            "|       from small pool |     431    |     516    |    5136 K  |    5136 K  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Active allocs         |     593    |     595    |    7764 K  |    7763 K  |\n",
            "|       from large pool |     162    |     163    |    2627 K  |    2627 K  |\n",
            "|       from small pool |     431    |     516    |    5136 K  |    5136 K  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| GPU reserved segments |      61    |     101    |    3059    |    2998    |\n",
            "|       from large pool |      32    |      32    |     246    |     214    |\n",
            "|       from small pool |      29    |      69    |    2813    |    2784    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Non-releasable allocs |      49    |      51    |    3498 K  |    3498 K  |\n",
            "|       from large pool |      22    |      22    |    1647 K  |    1647 K  |\n",
            "|       from small pool |      27    |      45    |    1851 K  |    1850 K  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Oversize allocations  |       0    |       0    |       0    |       0    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Oversize GPU segments |       0    |       0    |       0    |       0    |\n",
            "|===========================================================================|\n",
            "\n",
            "2022-03-10 06:27:16 | WARNING | fairseq.trainer | attempting to recover from OOM in forward/backward pass\n",
            "epoch 010: 100% 692/694 [02:10<00:00,  4.84it/s, loss=2.297, ppl=4.91, wps=13410.9, ups=5.06, wpb=2650.6, bsz=128, num_updates=6800, lr=0.0005, gnorm=1.133, train_wall=19, gb_free=5.7, wall=1308]2022-03-10 06:27:19 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "\n",
            "epoch 010 | valid on 'valid' subset:   0% 0/8 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 010 | valid on 'valid' subset:  12% 1/8 [00:00<00:01,  6.87it/s]\u001b[A\n",
            "epoch 010 | valid on 'valid' subset:  38% 3/8 [00:00<00:00, 12.92it/s]\u001b[A\n",
            "epoch 010 | valid on 'valid' subset:  62% 5/8 [00:00<00:00, 11.96it/s]\u001b[A\n",
            "epoch 010 | valid on 'valid' subset:  88% 7/8 [00:00<00:00, 10.14it/s]\u001b[A\n",
            "                                                                      \u001b[A2022-03-10 06:27:20 | INFO | valid | epoch 010 | valid on 'valid' subset | loss 4.025 | ppl 16.28 | wps 26132.1 | wpb 2673.1 | bsz 125 | num_updates 6890 | best_loss 3.748\n",
            "2022-03-10 06:27:20 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 10 @ 6890 updates\n",
            "2022-03-10 06:27:20 | INFO | fairseq.trainer | Saving checkpoint to /content/checkpoints/checkpoint10.pt\n",
            "2022-03-10 06:27:21 | INFO | fairseq.trainer | Finished saving checkpoint to /content/checkpoints/checkpoint10.pt\n",
            "2022-03-10 06:27:21 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/checkpoint10.pt (epoch 10 @ 6890 updates, score 4.025) (writing took 0.9942174349998822 seconds)\n",
            "2022-03-10 06:27:21 | INFO | fairseq_cli.train | end of epoch 10 (average epoch stats below)\n",
            "2022-03-10 06:27:21 | INFO | train | epoch 010 | loss 2.14 | ppl 4.41 | wps 13401.5 | ups 5.22 | wpb 2569.2 | bsz 127.9 | num_updates 6890 | lr 0.0005 | gnorm 1.084 | train_wall 125 | gb_free 10.6 | wall 1325\n",
            "2022-03-10 06:27:21 | INFO | fairseq_cli.train | done training in 1324.5 seconds\n",
            "\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Waiting for W&B process to finish... \u001b[32m(success).\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                                                                                \n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run history:\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:              train/bsz â–â–â–â–â–â–â–â–â–â–\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:          train/gb_free â–ˆâ–†â–ˆâ–â–‡â–ˆâ–„â–…â–ˆâ–ˆ\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:            train/gnorm â–†â–â–â–‚â–ƒâ–„â–†â–‡â–‡â–ˆ\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:             train/loss â–ˆâ–…â–„â–ƒâ–ƒâ–‚â–‚â–‚â–â–\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:               train/lr â–â–â–â–â–â–â–â–â–â–\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:              train/ppl â–ˆâ–ƒâ–‚â–‚â–‚â–â–â–â–â–\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:       train/train_wall â–…â–â–â–â–â–â–…â–…â–…â–ˆ\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:              train/ups â–‚â–‚â–„â–„â–‡â–ˆâ–â–‚â–…â–„\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:             train/wall â–â–‚â–ƒâ–ƒâ–„â–…â–†â–†â–‡â–ˆ\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:              train/wpb â–†â–†â–â–†â–†â–†â–†â–†â–†â–ˆ\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:              train/wps â–ƒâ–ƒâ–â–„â–‡â–ˆâ–â–ƒâ–…â–…\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:        train_inner/bsz â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:    train_inner/gb_free â–ˆâ–‚â–‡â–ˆâ–ˆâ–ˆâ–‡â–‡â–…â–ˆâ–ˆâ–ˆâ–‡â–ˆâ–‡â–ˆâ–ˆâ–ˆâ–‡â–†â–ˆâ–‡â–†â–ˆâ–ˆâ–ˆâ–‡â–…â–‡â–ˆâ–ˆâ–ˆâ–ˆâ–‡â–ˆâ–‡â–ˆâ–‡â–‡â–\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:      train_inner/gnorm â–ˆâ–‚â–‚â–â–â–â–â–â–â–â–â–â–â–â–‚â–‚â–‚â–‚â–‚â–‚â–â–‚â–‚â–‚â–‚â–‚â–ƒâ–‚â–‚â–‚â–‚â–ƒâ–‚â–‚â–ƒâ–ƒâ–ƒâ–‚â–ƒâ–ƒ\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:       train_inner/loss â–ˆâ–‡â–†â–…â–…â–„â–„â–„â–„â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–â–‚â–‚â–â–â–â–‚â–â–â–â–‚â–â–â–â–\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:         train_inner/lr â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:        train_inner/ppl â–ˆâ–„â–ƒâ–ƒâ–‚â–‚â–‚â–‚â–‚â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: train_inner/train_wall â–ƒâ–‚â–‚â–ˆâ–†â–‚â–…â–…â–ˆâ–‚â–‚â–…â–†â–…â–…â–…â–â–…â–ƒâ–…â–…â–†â–‚â–‡â–‚â–ƒâ–‡â–ƒâ–‚â–ƒâ–ƒâ–†â–ƒâ–ƒâ–‡â–†â–†â–…â–ƒâ–†\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:        train_inner/ups â–‡â–ˆâ–ˆâ–‚â–‚â–ˆâ–…â–…â–â–‡â–‡â–…â–‚â–…â–…â–†â–…â–…â–†â–…â–ƒâ–ƒâ–ˆâ–ƒâ–…â–†â–ƒâ–†â–…â–†â–†â–ƒâ–†â–†â–„â–ƒâ–…â–…â–…â–„\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:       train_inner/wall â–â–â–â–‚â–‚â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–„â–„â–…â–…â–…â–…â–…â–…â–†â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆ\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:        train_inner/wpb â–„â–‚â–‚â–‡â–…â–‚â–ƒâ–ƒâ–ˆâ–‚â–ƒâ–…â–„â–…â–„â–‚â–‚â–„â–ƒâ–„â–„â–†â–â–†â–‚â–ƒâ–„â–ƒâ–â–ƒâ–‚â–…â–ƒâ–‚â–…â–†â–„â–„â–„â–…\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:        train_inner/wps â–ˆâ–†â–‡â–„â–‚â–‡â–…â–…â–ƒâ–†â–†â–†â–â–†â–…â–…â–ƒâ–…â–†â–…â–ƒâ–…â–†â–…â–ƒâ–†â–ƒâ–†â–ƒâ–†â–…â–„â–†â–…â–„â–…â–…â–…â–†â–…\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:        valid/best_loss â–ˆâ–„â–â–â–â–â–â–â–\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:              valid/bsz â–â–â–â–â–â–â–â–â–â–\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:             valid/loss â–ˆâ–„â–‚â–â–â–â–‚â–‚â–‚â–ƒ\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:              valid/ppl â–ˆâ–ƒâ–‚â–â–â–â–â–â–‚â–‚\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:              valid/wpb â–â–â–â–â–â–â–â–â–â–\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:              valid/wps â–†â–â–†â–ˆâ–ƒâ–‚â–†â–†â–ƒâ–…\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run summary:\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:              train/bsz 127.9\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:          train/gb_free 10.6\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:            train/gnorm 1.084\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:             train/loss 2.14\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:               train/lr 0.0005\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:              train/ppl 4.41\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:       train/train_wall 125.0\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:              train/ups 5.22\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:             train/wall 1325.0\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:              train/wpb 2569.2\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:              train/wps 13401.5\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:        train_inner/bsz 128.0\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:    train_inner/gb_free 5.7\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:      train_inner/gnorm 1.133\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:       train_inner/loss 2.297\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:         train_inner/lr 0.0005\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:        train_inner/ppl 4.91\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: train_inner/train_wall 19.0\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:        train_inner/ups 5.06\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:       train_inner/wall 1308.0\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:        train_inner/wpb 2650.6\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:        train_inner/wps 13410.9\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:        valid/best_loss 3.748\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:              valid/bsz 125.0\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:             valid/loss 4.025\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:              valid/ppl 16.28\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:              valid/wpb 2673.1\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:              valid/wps 26132.1\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Synced \u001b[33mcheckpoints\u001b[0m: \u001b[34m\u001b[4mhttps://wandb.ai/rahul2023_usa/Transformer%20from%20scratch%20-%2010%20March%20-%20kn%20to%20ta/runs/3q2aylep\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Find logs at: \u001b[35m\u001b[1m./wandb/run-20220310_060517-3q2aylep/logs\u001b[0m\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!fairseq-generate /content/content/nmt-kn-ta/tokenized.kn-ta \\\n",
        "  --path /content/checkpoints/checkpoint_best.pt \\\n",
        "  --beam 5 \\\n",
        "  --remove-bpe \\\n",
        " | grep ^H | LC_ALL=C sort -V | cut -f3- > test_ta_generated_scratch_transformer.txt "
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "22qq0tPibeTI",
        "outputId": "90319d1b-ac06-4015-8fe1-26688b27d701"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2022-03-10 06:28:06 | INFO | fairseq.tasks.text_to_speech | Please install tensorboardX: pip install tensorboardX\n",
            "2022-03-10 06:28:09 | INFO | fairseq_cli.generate | {'_name': None, 'common': {'_name': None, 'no_progress_bar': False, 'log_interval': 100, 'log_format': None, 'log_file': None, 'tensorboard_logdir': None, 'wandb_project': None, 'azureml_logging': False, 'seed': 1, 'cpu': False, 'tpu': False, 'bf16': False, 'memory_efficient_bf16': False, 'fp16': False, 'memory_efficient_fp16': False, 'fp16_no_flatten_grads': False, 'fp16_init_scale': 128, 'fp16_scale_window': None, 'fp16_scale_tolerance': 0.0, 'on_cpu_convert_precision': False, 'min_loss_scale': 0.0001, 'threshold_loss_scale': None, 'amp': False, 'amp_batch_retries': 2, 'amp_init_scale': 128, 'amp_scale_window': None, 'user_dir': None, 'empty_cache_freq': 0, 'all_gather_list_size': 16384, 'model_parallel_size': 1, 'quantization_config_path': None, 'profile': False, 'reset_logging': False, 'suppress_crashes': False, 'use_plasma_view': False, 'plasma_path': '/tmp/plasma'}, 'common_eval': {'_name': None, 'path': '/content/checkpoints/checkpoint_best.pt', 'post_process': 'subword_nmt', 'quiet': False, 'model_overrides': '{}', 'results_path': None}, 'distributed_training': {'_name': None, 'distributed_world_size': 1, 'distributed_num_procs': 1, 'distributed_rank': 0, 'distributed_backend': 'nccl', 'distributed_init_method': None, 'distributed_port': -1, 'device_id': 0, 'distributed_no_spawn': False, 'ddp_backend': 'pytorch_ddp', 'ddp_comm_hook': 'none', 'bucket_cap_mb': 25, 'fix_batches_to_gpus': False, 'find_unused_parameters': False, 'gradient_as_bucket_view': False, 'fast_stat_sync': False, 'heartbeat_timeout': -1, 'broadcast_buffers': False, 'slowmo_momentum': None, 'slowmo_base_algorithm': 'localsgd', 'localsgd_frequency': 3, 'nprocs_per_node': 1, 'pipeline_model_parallel': False, 'pipeline_balance': None, 'pipeline_devices': None, 'pipeline_chunks': 0, 'pipeline_encoder_balance': None, 'pipeline_encoder_devices': None, 'pipeline_decoder_balance': None, 'pipeline_decoder_devices': None, 'pipeline_checkpoint': 'never', 'zero_sharding': 'none', 'fp16': False, 'memory_efficient_fp16': False, 'tpu': False, 'no_reshard_after_forward': False, 'fp32_reduce_scatter': False, 'cpu_offload': False, 'use_sharded_state': False, 'not_fsdp_flatten_parameters': False}, 'dataset': {'_name': None, 'num_workers': 1, 'skip_invalid_size_inputs_valid_test': False, 'max_tokens': 12000, 'batch_size': None, 'required_batch_size_multiple': 8, 'required_seq_len_multiple': 1, 'dataset_impl': None, 'data_buffer_size': 10, 'train_subset': 'train', 'valid_subset': 'valid', 'combine_valid_subsets': None, 'ignore_unused_valid_subsets': False, 'validate_interval': 1, 'validate_interval_updates': 0, 'validate_after_updates': 0, 'fixed_validation_seed': None, 'disable_validation': False, 'max_tokens_valid': None, 'batch_size_valid': None, 'max_valid_steps': None, 'curriculum': 0, 'gen_subset': 'test', 'num_shards': 1, 'shard_id': 0, 'grouped_shuffling': False, 'update_epoch_batch_itr': False, 'update_ordered_indices_seed': False}, 'optimization': {'_name': None, 'max_epoch': 0, 'max_update': 0, 'stop_time_hours': 0.0, 'clip_norm': 0.0, 'sentence_avg': False, 'update_freq': [1], 'lr': [0.25], 'stop_min_lr': -1.0, 'use_bmuf': False, 'skip_remainder_batch': False}, 'checkpoint': {'_name': None, 'save_dir': 'checkpoints', 'restore_file': 'checkpoint_last.pt', 'continue_once': None, 'finetune_from_model': None, 'reset_dataloader': False, 'reset_lr_scheduler': False, 'reset_meters': False, 'reset_optimizer': False, 'optimizer_overrides': '{}', 'save_interval': 1, 'save_interval_updates': 0, 'keep_interval_updates': -1, 'keep_interval_updates_pattern': -1, 'keep_last_epochs': -1, 'keep_best_checkpoints': -1, 'no_save': False, 'no_epoch_checkpoints': False, 'no_last_checkpoints': False, 'no_save_optimizer_state': False, 'best_checkpoint_metric': 'loss', 'maximize_best_checkpoint_metric': False, 'patience': -1, 'checkpoint_suffix': '', 'checkpoint_shard_count': 1, 'load_checkpoint_on_all_dp_ranks': False, 'write_checkpoints_asynchronously': False, 'model_parallel_size': 1}, 'bmuf': {'_name': None, 'block_lr': 1.0, 'block_momentum': 0.875, 'global_sync_iter': 50, 'warmup_iterations': 500, 'use_nbm': False, 'average_sync': False, 'distributed_world_size': 1}, 'generation': {'_name': None, 'beam': 5, 'nbest': 1, 'max_len_a': 0.0, 'max_len_b': 200, 'min_len': 1, 'match_source_len': False, 'unnormalized': False, 'no_early_stop': False, 'no_beamable_mm': False, 'lenpen': 1.0, 'unkpen': 0.0, 'replace_unk': None, 'sacrebleu': False, 'score_reference': False, 'prefix_size': 0, 'no_repeat_ngram_size': 0, 'sampling': False, 'sampling_topk': -1, 'sampling_topp': -1.0, 'constraints': None, 'temperature': 1.0, 'diverse_beam_groups': -1, 'diverse_beam_strength': 0.5, 'diversity_rate': -1.0, 'print_alignment': None, 'print_step': False, 'lm_path': None, 'lm_weight': 0.0, 'iter_decode_eos_penalty': 0.0, 'iter_decode_max_iter': 10, 'iter_decode_force_max_iter': False, 'iter_decode_with_beam': 1, 'iter_decode_with_external_reranker': False, 'retain_iter_history': False, 'retain_dropout': False, 'retain_dropout_modules': None, 'decoding_format': None, 'no_seed_provided': False}, 'eval_lm': {'_name': None, 'output_word_probs': False, 'output_word_stats': False, 'context_window': 0, 'softmax_batch': 9223372036854775807}, 'interactive': {'_name': None, 'buffer_size': 0, 'input': '-'}, 'model': {'_name': 'wav2vec2', 'extractor_mode': 'default', 'encoder_layers': 12, 'encoder_embed_dim': 768, 'encoder_ffn_embed_dim': 3072, 'encoder_attention_heads': 12, 'activation_fn': 'gelu', 'layer_type': 'transformer', 'dropout': 0.1, 'attention_dropout': 0.1, 'activation_dropout': 0.0, 'encoder_layerdrop': 0.0, 'dropout_input': 0.0, 'dropout_features': 0.0, 'final_dim': 0, 'layer_norm_first': False, 'conv_feature_layers': '[(512, 10, 5)] + [(512, 3, 2)] * 4 + [(512,2,2)] + [(512,2,2)]', 'conv_bias': False, 'logit_temp': 0.1, 'quantize_targets': False, 'quantize_input': False, 'same_quantizer': False, 'target_glu': False, 'feature_grad_mult': 1.0, 'quantizer_depth': 1, 'quantizer_factor': 3, 'latent_vars': 320, 'latent_groups': 2, 'latent_dim': 0, 'mask_length': 10, 'mask_prob': 0.65, 'mask_selection': 'static', 'mask_other': 0.0, 'no_mask_overlap': False, 'mask_min_space': 1, 'require_same_masks': True, 'mask_dropout': 0.0, 'mask_channel_length': 10, 'mask_channel_prob': 0.0, 'mask_channel_before': False, 'mask_channel_selection': 'static', 'mask_channel_other': 0.0, 'no_mask_channel_overlap': False, 'mask_channel_min_space': 1, 'num_negatives': 100, 'negatives_from_everywhere': False, 'cross_sample_negatives': 0, 'codebook_negatives': 0, 'conv_pos': 128, 'conv_pos_groups': 16, 'pos_conv_depth': 1, 'latent_temp': [2.0, 0.5, 0.999995], 'max_positions': 100000, 'checkpoint_activations': False, 'required_seq_len_multiple': 1, 'crop_seq_to_multiple': 1, 'depthwise_conv_kernel_size': 31, 'attn_type': '', 'pos_enc_type': 'abs', 'fp16': False}, 'task': {'_name': 'translation', 'data': '/content/content/nmt-kn-ta/tokenized.kn-ta', 'source_lang': None, 'target_lang': None, 'load_alignments': False, 'left_pad_source': True, 'left_pad_target': False, 'max_source_positions': 1024, 'max_target_positions': 1024, 'upsample_primary': -1, 'truncate_source': False, 'num_batch_buckets': 0, 'train_subset': 'train', 'dataset_impl': None, 'required_seq_len_multiple': 1, 'eval_bleu': False, 'eval_bleu_args': '{}', 'eval_bleu_detok': 'space', 'eval_bleu_detok_args': '{}', 'eval_tokenized_bleu': False, 'eval_bleu_remove_bpe': None, 'eval_bleu_print_samples': False}, 'criterion': {'_name': 'cross_entropy', 'sentence_avg': True}, 'optimizer': None, 'lr_scheduler': {'_name': 'fixed', 'force_anneal': None, 'lr_shrink': 0.1, 'warmup_updates': 0, 'lr': [0.25]}, 'scoring': {'_name': 'bleu', 'pad': 1, 'eos': 2, 'unk': 3}, 'bpe': None, 'tokenizer': None, 'ema': {'_name': None, 'store_ema': False, 'ema_decay': 0.9999, 'ema_start_update': 0, 'ema_seed_model': None, 'ema_update_freq': 1, 'ema_fp32': False}, 'simul_type': None}\n",
            "2022-03-10 06:28:09 | INFO | fairseq.tasks.translation | [kn] dictionary: 19144 types\n",
            "2022-03-10 06:28:09 | INFO | fairseq.tasks.translation | [ta] dictionary: 13456 types\n",
            "2022-03-10 06:28:09 | INFO | fairseq_cli.generate | loading model(s) from /content/checkpoints/checkpoint_best.pt\n",
            "2022-03-10 06:28:10 | INFO | fairseq.data.data_utils | loaded 1,000 examples from: /content/content/nmt-kn-ta/tokenized.kn-ta/test.kn-ta.kn\n",
            "2022-03-10 06:28:10 | INFO | fairseq.data.data_utils | loaded 1,000 examples from: /content/content/nmt-kn-ta/tokenized.kn-ta/test.kn-ta.ta\n",
            "2022-03-10 06:28:10 | INFO | fairseq.tasks.translation | /content/content/nmt-kn-ta/tokenized.kn-ta test kn-ta 1000 examples\n",
            "2022-03-10 06:28:27 | INFO | fairseq_cli.generate | NOTE: hypothesis and token scores are output in base 2\n",
            "2022-03-10 06:28:27 | INFO | fairseq_cli.generate | Translated 1,000 sentences (16,584 tokens) in 8.1s (124.10 sentences/s, 2058.10 tokens/s)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!fairseq-generate /content/content/nmt-kn-ta/tokenized.kn-ta \\\n",
        "  --path /content/checkpoints/checkpoint_best.pt \\\n",
        "  --beam 5 \\\n",
        "   --remove-bpe \\\n",
        "  | grep ^T | LC_ALL=C sort -V | cut -f2- > test_ta_actual_scratch_transformer.txt "
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "J-t358GnbnFE",
        "outputId": "3d999c89-88b0-4254-8b49-db1fbf623329"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2022-03-10 06:28:34 | INFO | fairseq.tasks.text_to_speech | Please install tensorboardX: pip install tensorboardX\n",
            "2022-03-10 06:28:37 | INFO | fairseq_cli.generate | {'_name': None, 'common': {'_name': None, 'no_progress_bar': False, 'log_interval': 100, 'log_format': None, 'log_file': None, 'tensorboard_logdir': None, 'wandb_project': None, 'azureml_logging': False, 'seed': 1, 'cpu': False, 'tpu': False, 'bf16': False, 'memory_efficient_bf16': False, 'fp16': False, 'memory_efficient_fp16': False, 'fp16_no_flatten_grads': False, 'fp16_init_scale': 128, 'fp16_scale_window': None, 'fp16_scale_tolerance': 0.0, 'on_cpu_convert_precision': False, 'min_loss_scale': 0.0001, 'threshold_loss_scale': None, 'amp': False, 'amp_batch_retries': 2, 'amp_init_scale': 128, 'amp_scale_window': None, 'user_dir': None, 'empty_cache_freq': 0, 'all_gather_list_size': 16384, 'model_parallel_size': 1, 'quantization_config_path': None, 'profile': False, 'reset_logging': False, 'suppress_crashes': False, 'use_plasma_view': False, 'plasma_path': '/tmp/plasma'}, 'common_eval': {'_name': None, 'path': '/content/checkpoints/checkpoint_best.pt', 'post_process': 'subword_nmt', 'quiet': False, 'model_overrides': '{}', 'results_path': None}, 'distributed_training': {'_name': None, 'distributed_world_size': 1, 'distributed_num_procs': 1, 'distributed_rank': 0, 'distributed_backend': 'nccl', 'distributed_init_method': None, 'distributed_port': -1, 'device_id': 0, 'distributed_no_spawn': False, 'ddp_backend': 'pytorch_ddp', 'ddp_comm_hook': 'none', 'bucket_cap_mb': 25, 'fix_batches_to_gpus': False, 'find_unused_parameters': False, 'gradient_as_bucket_view': False, 'fast_stat_sync': False, 'heartbeat_timeout': -1, 'broadcast_buffers': False, 'slowmo_momentum': None, 'slowmo_base_algorithm': 'localsgd', 'localsgd_frequency': 3, 'nprocs_per_node': 1, 'pipeline_model_parallel': False, 'pipeline_balance': None, 'pipeline_devices': None, 'pipeline_chunks': 0, 'pipeline_encoder_balance': None, 'pipeline_encoder_devices': None, 'pipeline_decoder_balance': None, 'pipeline_decoder_devices': None, 'pipeline_checkpoint': 'never', 'zero_sharding': 'none', 'fp16': False, 'memory_efficient_fp16': False, 'tpu': False, 'no_reshard_after_forward': False, 'fp32_reduce_scatter': False, 'cpu_offload': False, 'use_sharded_state': False, 'not_fsdp_flatten_parameters': False}, 'dataset': {'_name': None, 'num_workers': 1, 'skip_invalid_size_inputs_valid_test': False, 'max_tokens': 12000, 'batch_size': None, 'required_batch_size_multiple': 8, 'required_seq_len_multiple': 1, 'dataset_impl': None, 'data_buffer_size': 10, 'train_subset': 'train', 'valid_subset': 'valid', 'combine_valid_subsets': None, 'ignore_unused_valid_subsets': False, 'validate_interval': 1, 'validate_interval_updates': 0, 'validate_after_updates': 0, 'fixed_validation_seed': None, 'disable_validation': False, 'max_tokens_valid': None, 'batch_size_valid': None, 'max_valid_steps': None, 'curriculum': 0, 'gen_subset': 'test', 'num_shards': 1, 'shard_id': 0, 'grouped_shuffling': False, 'update_epoch_batch_itr': False, 'update_ordered_indices_seed': False}, 'optimization': {'_name': None, 'max_epoch': 0, 'max_update': 0, 'stop_time_hours': 0.0, 'clip_norm': 0.0, 'sentence_avg': False, 'update_freq': [1], 'lr': [0.25], 'stop_min_lr': -1.0, 'use_bmuf': False, 'skip_remainder_batch': False}, 'checkpoint': {'_name': None, 'save_dir': 'checkpoints', 'restore_file': 'checkpoint_last.pt', 'continue_once': None, 'finetune_from_model': None, 'reset_dataloader': False, 'reset_lr_scheduler': False, 'reset_meters': False, 'reset_optimizer': False, 'optimizer_overrides': '{}', 'save_interval': 1, 'save_interval_updates': 0, 'keep_interval_updates': -1, 'keep_interval_updates_pattern': -1, 'keep_last_epochs': -1, 'keep_best_checkpoints': -1, 'no_save': False, 'no_epoch_checkpoints': False, 'no_last_checkpoints': False, 'no_save_optimizer_state': False, 'best_checkpoint_metric': 'loss', 'maximize_best_checkpoint_metric': False, 'patience': -1, 'checkpoint_suffix': '', 'checkpoint_shard_count': 1, 'load_checkpoint_on_all_dp_ranks': False, 'write_checkpoints_asynchronously': False, 'model_parallel_size': 1}, 'bmuf': {'_name': None, 'block_lr': 1.0, 'block_momentum': 0.875, 'global_sync_iter': 50, 'warmup_iterations': 500, 'use_nbm': False, 'average_sync': False, 'distributed_world_size': 1}, 'generation': {'_name': None, 'beam': 5, 'nbest': 1, 'max_len_a': 0.0, 'max_len_b': 200, 'min_len': 1, 'match_source_len': False, 'unnormalized': False, 'no_early_stop': False, 'no_beamable_mm': False, 'lenpen': 1.0, 'unkpen': 0.0, 'replace_unk': None, 'sacrebleu': False, 'score_reference': False, 'prefix_size': 0, 'no_repeat_ngram_size': 0, 'sampling': False, 'sampling_topk': -1, 'sampling_topp': -1.0, 'constraints': None, 'temperature': 1.0, 'diverse_beam_groups': -1, 'diverse_beam_strength': 0.5, 'diversity_rate': -1.0, 'print_alignment': None, 'print_step': False, 'lm_path': None, 'lm_weight': 0.0, 'iter_decode_eos_penalty': 0.0, 'iter_decode_max_iter': 10, 'iter_decode_force_max_iter': False, 'iter_decode_with_beam': 1, 'iter_decode_with_external_reranker': False, 'retain_iter_history': False, 'retain_dropout': False, 'retain_dropout_modules': None, 'decoding_format': None, 'no_seed_provided': False}, 'eval_lm': {'_name': None, 'output_word_probs': False, 'output_word_stats': False, 'context_window': 0, 'softmax_batch': 9223372036854775807}, 'interactive': {'_name': None, 'buffer_size': 0, 'input': '-'}, 'model': {'_name': 'wav2vec2', 'extractor_mode': 'default', 'encoder_layers': 12, 'encoder_embed_dim': 768, 'encoder_ffn_embed_dim': 3072, 'encoder_attention_heads': 12, 'activation_fn': 'gelu', 'layer_type': 'transformer', 'dropout': 0.1, 'attention_dropout': 0.1, 'activation_dropout': 0.0, 'encoder_layerdrop': 0.0, 'dropout_input': 0.0, 'dropout_features': 0.0, 'final_dim': 0, 'layer_norm_first': False, 'conv_feature_layers': '[(512, 10, 5)] + [(512, 3, 2)] * 4 + [(512,2,2)] + [(512,2,2)]', 'conv_bias': False, 'logit_temp': 0.1, 'quantize_targets': False, 'quantize_input': False, 'same_quantizer': False, 'target_glu': False, 'feature_grad_mult': 1.0, 'quantizer_depth': 1, 'quantizer_factor': 3, 'latent_vars': 320, 'latent_groups': 2, 'latent_dim': 0, 'mask_length': 10, 'mask_prob': 0.65, 'mask_selection': 'static', 'mask_other': 0.0, 'no_mask_overlap': False, 'mask_min_space': 1, 'require_same_masks': True, 'mask_dropout': 0.0, 'mask_channel_length': 10, 'mask_channel_prob': 0.0, 'mask_channel_before': False, 'mask_channel_selection': 'static', 'mask_channel_other': 0.0, 'no_mask_channel_overlap': False, 'mask_channel_min_space': 1, 'num_negatives': 100, 'negatives_from_everywhere': False, 'cross_sample_negatives': 0, 'codebook_negatives': 0, 'conv_pos': 128, 'conv_pos_groups': 16, 'pos_conv_depth': 1, 'latent_temp': [2.0, 0.5, 0.999995], 'max_positions': 100000, 'checkpoint_activations': False, 'required_seq_len_multiple': 1, 'crop_seq_to_multiple': 1, 'depthwise_conv_kernel_size': 31, 'attn_type': '', 'pos_enc_type': 'abs', 'fp16': False}, 'task': {'_name': 'translation', 'data': '/content/content/nmt-kn-ta/tokenized.kn-ta', 'source_lang': None, 'target_lang': None, 'load_alignments': False, 'left_pad_source': True, 'left_pad_target': False, 'max_source_positions': 1024, 'max_target_positions': 1024, 'upsample_primary': -1, 'truncate_source': False, 'num_batch_buckets': 0, 'train_subset': 'train', 'dataset_impl': None, 'required_seq_len_multiple': 1, 'eval_bleu': False, 'eval_bleu_args': '{}', 'eval_bleu_detok': 'space', 'eval_bleu_detok_args': '{}', 'eval_tokenized_bleu': False, 'eval_bleu_remove_bpe': None, 'eval_bleu_print_samples': False}, 'criterion': {'_name': 'cross_entropy', 'sentence_avg': True}, 'optimizer': None, 'lr_scheduler': {'_name': 'fixed', 'force_anneal': None, 'lr_shrink': 0.1, 'warmup_updates': 0, 'lr': [0.25]}, 'scoring': {'_name': 'bleu', 'pad': 1, 'eos': 2, 'unk': 3}, 'bpe': None, 'tokenizer': None, 'ema': {'_name': None, 'store_ema': False, 'ema_decay': 0.9999, 'ema_start_update': 0, 'ema_seed_model': None, 'ema_update_freq': 1, 'ema_fp32': False}, 'simul_type': None}\n",
            "2022-03-10 06:28:37 | INFO | fairseq.tasks.translation | [kn] dictionary: 19144 types\n",
            "2022-03-10 06:28:37 | INFO | fairseq.tasks.translation | [ta] dictionary: 13456 types\n",
            "2022-03-10 06:28:37 | INFO | fairseq_cli.generate | loading model(s) from /content/checkpoints/checkpoint_best.pt\n",
            "2022-03-10 06:28:37 | INFO | fairseq.data.data_utils | loaded 1,000 examples from: /content/content/nmt-kn-ta/tokenized.kn-ta/test.kn-ta.kn\n",
            "2022-03-10 06:28:37 | INFO | fairseq.data.data_utils | loaded 1,000 examples from: /content/content/nmt-kn-ta/tokenized.kn-ta/test.kn-ta.ta\n",
            "2022-03-10 06:28:37 | INFO | fairseq.tasks.translation | /content/content/nmt-kn-ta/tokenized.kn-ta test kn-ta 1000 examples\n",
            "2022-03-10 06:28:54 | INFO | fairseq_cli.generate | NOTE: hypothesis and token scores are output in base 2\n",
            "2022-03-10 06:28:54 | INFO | fairseq_cli.generate | Translated 1,000 sentences (16,584 tokens) in 7.8s (128.22 sentences/s, 2126.46 tokens/s)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!fairseq-score --sys /content/test_ta_generated_scratch_transformer.txt --ref /content/test_ta_actual_scratch_transformer.txt"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Sav230XObv0Y",
        "outputId": "f24214e9-4837-4cdc-c7a2-7573b0d61585"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2022-03-10 06:29:00 | INFO | fairseq.tasks.text_to_speech | Please install tensorboardX: pip install tensorboardX\n",
            "Namespace(ignore_case=False, order=4, ref='/content/test_ta_actual_scratch_transformer.txt', sacrebleu=False, sentence_bleu=False, sys='/content/test_ta_generated_scratch_transformer.txt')\n",
            "BLEU4 = 12.85, 50.5/18.8/10.5/6.0 (BP=0.820, ratio=0.834, syslen=15550, reflen=18634)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from nltk.translate.bleu_score import sentence_bleu\n",
        "f = open(\"/content/test_ta_generated_scratch_transformer.txt\")\n",
        "ml_candidates = [i.strip() for i in f.readlines()]\n",
        "f = open(\"/content/test_ta_actual_scratch_transformer.txt\")\n",
        "ml_gold = [i.strip() for i in f.readlines()]\n",
        "total_four = 0\n",
        "for i in range(len(ml_candidates)):\n",
        "    candidate = ml_candidates[i].split(\" \")\n",
        "    references = ml_gold[i].split(\" \")\n",
        "    reference = [references]\n",
        "    score_cumulative4 = sentence_bleu(reference, candidate, weights=(0.25, 0.25, 0.25, 0.25))\n",
        "    total_four = total_four + score_cumulative4\n",
        "print(total_four/len(ml_candidates))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9i3OsFbibzmf",
        "outputId": "7428e12c-8cd3-40bc-e04d-04609868e9dc"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/nltk/translate/bleu_score.py:490: UserWarning: \n",
            "Corpus/Sentence contains 0 counts of 2-gram overlaps.\n",
            "BLEU scores might be undesirable; use SmoothingFunction().\n",
            "  warnings.warn(_msg)\n",
            "/usr/local/lib/python3.7/dist-packages/nltk/translate/bleu_score.py:490: UserWarning: \n",
            "Corpus/Sentence contains 0 counts of 3-gram overlaps.\n",
            "BLEU scores might be undesirable; use SmoothingFunction().\n",
            "  warnings.warn(_msg)\n",
            "/usr/local/lib/python3.7/dist-packages/nltk/translate/bleu_score.py:490: UserWarning: \n",
            "Corpus/Sentence contains 0 counts of 4-gram overlaps.\n",
            "BLEU scores might be undesirable; use SmoothingFunction().\n",
            "  warnings.warn(_msg)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.3496700574848813\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pOWFJ64yhWPp",
        "outputId": "f922ff8f-98ee-4750-9bd4-1750a6ddbee1"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!cp /content/checkpoints/checkpoint_best.pt /content/drive/MyDrive/Transformer_from_scratch_ta/"
      ],
      "metadata": {
        "id": "xCvT4mcLhaMw"
      },
      "execution_count": 14,
      "outputs": []
    }
  ]
}