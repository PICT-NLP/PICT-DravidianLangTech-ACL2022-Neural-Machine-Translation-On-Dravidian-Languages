{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Transformer from scratch  - kn to ml.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "PhOSXINhI2F-"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        " # w and b (wandb) for logging\n",
        "! pip install wandb\n",
        "\n",
        "# sacremos - for tokenizing\n",
        "! pip install sacremos\n",
        "\n",
        "# fairseq - for training and evaluation of the model\n",
        "! git clone https://github.com/pytorch/fairseq\n",
        "%cd fairseq\n",
        "! pip install --editable ./\n",
        "%cd ..\n",
        "\n",
        "%cd /content/fairseq/\n",
        "!python -m pip install --editable .\n",
        "%cd /content\n",
        "\n",
        "! echo $PYTHONPATH\n",
        "\n",
        "import os\n",
        "os.environ['PYTHONPATH'] += \":/content/fairseq/\"\n",
        "\n",
        "! echo $PYTHONPATH"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import wandb\n",
        "wandb.login()\n",
        "# login authorization."
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 104
        },
        "id": "AprsKOaANwu9",
        "outputId": "14e94916-2a49-4142-fe21-f1ec36e0e417"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/javascript": [
              "\n",
              "        window._wandbApiKey = new Promise((resolve, reject) => {\n",
              "            function loadScript(url) {\n",
              "            return new Promise(function(resolve, reject) {\n",
              "                let newScript = document.createElement(\"script\");\n",
              "                newScript.onerror = reject;\n",
              "                newScript.onload = resolve;\n",
              "                document.body.appendChild(newScript);\n",
              "                newScript.src = url;\n",
              "            });\n",
              "            }\n",
              "            loadScript(\"https://cdn.jsdelivr.net/npm/postmate/build/postmate.min.js\").then(() => {\n",
              "            const iframe = document.createElement('iframe')\n",
              "            iframe.style.cssText = \"width:0;height:0;border:none\"\n",
              "            document.body.appendChild(iframe)\n",
              "            const handshake = new Postmate({\n",
              "                container: iframe,\n",
              "                url: 'https://wandb.ai/authorize'\n",
              "            });\n",
              "            const timeout = setTimeout(() => reject(\"Couldn't auto authenticate\"), 5000)\n",
              "            handshake.then(function(child) {\n",
              "                child.on('authorize', data => {\n",
              "                    clearTimeout(timeout)\n",
              "                    resolve(data)\n",
              "                });\n",
              "            });\n",
              "            })\n",
              "        });\n",
              "    "
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: You can find your API key in your browser here: https://wandb.ai/authorize\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "wandb: Paste an API key from your profile and hit enter, or press ctrl+c to quit: ··········\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/stefan-it/nmt-mk-en.git"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "53dV1LkQN3xJ",
        "outputId": "2daf8588-059e-49c3-a8ff-e9a5469fc173"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'nmt-mk-en'...\n",
            "remote: Enumerating objects: 64, done.\u001b[K\n",
            "remote: Total 64 (delta 0), reused 0 (delta 0), pack-reused 64\u001b[K\n",
            "Unpacking objects: 100% (64/64), done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%cd nmt-kn-ml/"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "C18BjveuOTC_",
        "outputId": "a0f11462-d3c9-4afb-d495-8d35a8ee78ab"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/nmt-kn-ml\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "! chmod a+x /content/nmt-kn-ml/scripts/data_preparation.sh"
      ],
      "metadata": {
        "id": "RjpBic3TOV-5"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "! /content/nmt-kn-ml/scripts/data_preparation.sh data.kn data.ml"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vdwpAnXPOXnF",
        "outputId": "f50739ce-f045-47e6-a061-b7c9dcc6d4d9"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'mosesdecoder'...\n",
            "remote: Enumerating objects: 148090, done.\u001b[K\n",
            "remote: Counting objects: 100% (518/518), done.\u001b[K\n",
            "remote: Compressing objects: 100% (223/223), done.\u001b[K\n",
            "remote: Total 148090 (delta 319), reused 443 (delta 292), pack-reused 147572\u001b[K\n",
            "Receiving objects: 100% (148090/148090), 129.87 MiB | 19.29 MiB/s, done.\n",
            "Resolving deltas: 100% (114345/114345), done.\n",
            "Cloning into 'subword-nmt'...\n",
            "remote: Enumerating objects: 587, done.\u001b[K\n",
            "remote: Counting objects: 100% (11/11), done.\u001b[K\n",
            "remote: Compressing objects: 100% (10/10), done.\u001b[K\n",
            "remote: Total 587 (delta 1), reused 4 (delta 1), pack-reused 576\u001b[K\n",
            "Receiving objects: 100% (587/587), 244.20 KiB | 2.60 MiB/s, done.\n",
            "Resolving deltas: 100% (350/350), done.\n",
            "Cloning into 'seq2seq'...\n",
            "remote: Enumerating objects: 5995, done.\u001b[K\n",
            "remote: Total 5995 (delta 0), reused 0 (delta 0), pack-reused 5995\u001b[K\n",
            "Receiving objects: 100% (5995/5995), 1.63 MiB | 2.54 MiB/s, done.\n",
            "Resolving deltas: 100% (4189/4189), done.\n",
            "Tokenizer Version 1.1\n",
            "Language: sk\n",
            "Number of threads: 8\n",
            "Tokenizer Version 1.1\n",
            "Language: ml\n",
            "Number of threads: 8\n",
            "clean-corpus.perl: processing corpus.kn & .ml to corpus.clean, cutoff 1-500, ratio 9\n",
            ".........\n",
            "Input sentences: 92974  Output sentences:  92897\n",
            "Learning BPE with merge_ops=32000. This may take a while...\n",
            "subword-nmt/learn_bpe.py:338: DeprecationWarning: this script's location has moved to /content/nmt-kn-ml/subword-nmt/subword_nmt. This symbolic link will be removed in a future version. Please point to the new location, or install the package and use the command 'subword-nmt'\n",
            "  DeprecationWarning\n",
            "100% 32000/32000 [01:15<00:00, 422.39it/s] \n",
            "Apply BPE with merge_ops=32000 to tokenized files...\n",
            "subword-nmt/apply_bpe.py:396: DeprecationWarning: this script's location has moved to /content/nmt-kn-ml/subword-nmt/subword_nmt. This symbolic link will be removed in a future version. Please point to the new location, or install the package and use the command 'subword-nmt'\n",
            "  DeprecationWarning\n",
            "subword-nmt/apply_bpe.py:416: ResourceWarning: unclosed file <_io.TextIOWrapper name='bpe.32000' mode='r' encoding='UTF-8'>\n",
            "  args.codes = codecs.open(args.codes.name, encoding='utf-8')\n",
            "ResourceWarning: Enable tracemalloc to get the object allocation traceback\n",
            "corpus.bpe.32000.ml\n",
            "subword-nmt/apply_bpe.py:396: DeprecationWarning: this script's location has moved to /content/nmt-kn-ml/subword-nmt/subword_nmt. This symbolic link will be removed in a future version. Please point to the new location, or install the package and use the command 'subword-nmt'\n",
            "  DeprecationWarning\n",
            "subword-nmt/apply_bpe.py:416: ResourceWarning: unclosed file <_io.TextIOWrapper name='bpe.32000' mode='r' encoding='UTF-8'>\n",
            "  args.codes = codecs.open(args.codes.name, encoding='utf-8')\n",
            "ResourceWarning: Enable tracemalloc to get the object allocation traceback\n",
            "corpus.clean.bpe.32000.ml\n",
            "subword-nmt/apply_bpe.py:396: DeprecationWarning: this script's location has moved to /content/nmt-kn-ml/subword-nmt/subword_nmt. This symbolic link will be removed in a future version. Please point to the new location, or install the package and use the command 'subword-nmt'\n",
            "  DeprecationWarning\n",
            "subword-nmt/apply_bpe.py:416: ResourceWarning: unclosed file <_io.TextIOWrapper name='bpe.32000' mode='r' encoding='UTF-8'>\n",
            "  args.codes = codecs.open(args.codes.name, encoding='utf-8')\n",
            "ResourceWarning: Enable tracemalloc to get the object allocation traceback\n",
            "corpus.bpe.32000.kn\n",
            "subword-nmt/apply_bpe.py:396: DeprecationWarning: this script's location has moved to /content/nmt-kn-ml/subword-nmt/subword_nmt. This symbolic link will be removed in a future version. Please point to the new location, or install the package and use the command 'subword-nmt'\n",
            "  DeprecationWarning\n",
            "subword-nmt/apply_bpe.py:416: ResourceWarning: unclosed file <_io.TextIOWrapper name='bpe.32000' mode='r' encoding='UTF-8'>\n",
            "  args.codes = codecs.open(args.codes.name, encoding='utf-8')\n",
            "ResourceWarning: Enable tracemalloc to get the object allocation traceback\n",
            "corpus.clean.bpe.32000.kn\n",
            "subword-nmt/get_vocab.py:60: DeprecationWarning: this script's location has moved to /content/nmt-kn-ml/subword-nmt/subword_nmt. This symbolic link will be removed in a future version. Please point to the new location, or install the package and use the command 'subword-nmt'\n",
            "  DeprecationWarning\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "! chmod a+x /content/nmt-kn-ml/scripts/split_dataset.sh"
      ],
      "metadata": {
        "id": "4qw2sYocOa5p"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "! /content/nmt-kn-ml/scripts/split_dataset.sh corpus.clean.bpe.32000.kn corpus.clean.bpe.32000.ml"
      ],
      "metadata": {
        "id": "Z4lqq_xBOcfC"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%cd .."
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OxCCth1GOeBD",
        "outputId": "7f9a176f-09e2-4ee4-d4d9-ecee1c384aae"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!fairseq-preprocess --source-lang kn --target-lang ml \\\n",
        "--trainpref nmt-kn-ml/train \\\n",
        "--validpref nmt-kn-ml/dev \\\n",
        "--testpref nmt-kn-ml/test \\\n",
        "--destdir nmt-kn-ml/tokenized.kn-ml \\\n",
        "--thresholdsrc 2 \\\n",
        "--thresholdtgt 2 "
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UlAu5yyEOhmW",
        "outputId": "87e1fbe5-3619-4332-c904-320b036489a8"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2022-03-10 05:22:34 | INFO | fairseq.tasks.text_to_speech | Please install tensorboardX: pip install tensorboardX\n",
            "2022-03-10 05:22:34 | INFO | fairseq_cli.preprocess | Namespace(align_suffix=None, alignfile=None, all_gather_list_size=16384, amp=False, amp_batch_retries=2, amp_init_scale=128, amp_scale_window=None, azureml_logging=False, bf16=False, bpe=None, cpu=False, criterion='cross_entropy', dataset_impl='mmap', destdir='nmt-kn-ml/tokenized.kn-ml', dict_only=False, empty_cache_freq=0, fp16=False, fp16_init_scale=128, fp16_no_flatten_grads=False, fp16_scale_tolerance=0.0, fp16_scale_window=None, joined_dictionary=False, log_file=None, log_format=None, log_interval=100, lr_scheduler='fixed', memory_efficient_bf16=False, memory_efficient_fp16=False, min_loss_scale=0.0001, model_parallel_size=1, no_progress_bar=False, nwordssrc=-1, nwordstgt=-1, on_cpu_convert_precision=False, only_source=False, optimizer=None, padding_factor=8, plasma_path='/tmp/plasma', profile=False, quantization_config_path=None, reset_logging=False, scoring='bleu', seed=1, simul_type=None, source_lang='kn', srcdict=None, suppress_crashes=False, target_lang='ml', task='translation', tensorboard_logdir=None, testpref='nmt-kn-ml/test', tgtdict=None, threshold_loss_scale=None, thresholdsrc=2, thresholdtgt=2, tokenizer=None, tpu=False, trainpref='nmt-kn-ml/train', use_plasma_view=False, user_dir=None, validpref='nmt-kn-ml/dev', wandb_project=None, workers=1)\n",
            "2022-03-10 05:22:53 | INFO | fairseq_cli.preprocess | [kn] Dictionary: 16904 types\n",
            "2022-03-10 05:23:10 | INFO | fairseq_cli.preprocess | [kn] nmt-kn-ml/train.kn: 90897 sents, 1498515 tokens, 0.0141% replaced (by <unk>)\n",
            "2022-03-10 05:23:10 | INFO | fairseq_cli.preprocess | [kn] Dictionary: 16904 types\n",
            "2022-03-10 05:23:10 | INFO | fairseq_cli.preprocess | [kn] nmt-kn-ml/dev.kn: 1000 sents, 16331 tokens, 0.0612% replaced (by <unk>)\n",
            "2022-03-10 05:23:10 | INFO | fairseq_cli.preprocess | [kn] Dictionary: 16904 types\n",
            "2022-03-10 05:23:11 | INFO | fairseq_cli.preprocess | [kn] nmt-kn-ml/test.kn: 1000 sents, 16816 tokens, 0.0416% replaced (by <unk>)\n",
            "2022-03-10 05:23:11 | INFO | fairseq_cli.preprocess | [ml] Dictionary: 15656 types\n",
            "2022-03-10 05:23:31 | INFO | fairseq_cli.preprocess | [ml] nmt-kn-ml/train.ml: 90897 sents, 2034478 tokens, 0.00777% replaced (by <unk>)\n",
            "2022-03-10 05:23:31 | INFO | fairseq_cli.preprocess | [ml] Dictionary: 15656 types\n",
            "2022-03-10 05:23:31 | INFO | fairseq_cli.preprocess | [ml] nmt-kn-ml/dev.ml: 1000 sents, 22401 tokens, 0.0179% replaced (by <unk>)\n",
            "2022-03-10 05:23:31 | INFO | fairseq_cli.preprocess | [ml] Dictionary: 15656 types\n",
            "2022-03-10 05:23:32 | INFO | fairseq_cli.preprocess | [ml] nmt-kn-ml/test.ml: 1000 sents, 22247 tokens, 0.018% replaced (by <unk>)\n",
            "2022-03-10 05:23:32 | INFO | fairseq_cli.preprocess | Wrote preprocessed data to nmt-kn-ml/tokenized.kn-ml\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "! fairseq-train nmt-kn-ml/tokenized.kn-ml \\\n",
        "--arch transformer \\\n",
        "--dropout 0.1 \\\n",
        "--attention-dropout 0.1 \\\n",
        "--activation-dropout 0.1 \\\n",
        "--encoder-embed-dim 256 \\\n",
        "--encoder-ffn-embed-dim 512 \\\n",
        "--encoder-layers 3 \\\n",
        "--encoder-attention-heads 8 \\\n",
        "--encoder-learned-pos \\\n",
        "--decoder-ffn-embed-dim 512 \\\n",
        "--decoder-layers 3 \\\n",
        "--decoder-attention-heads 8 \\\n",
        "--decoder-learned-pos \\\n",
        "--max-epoch 10 \\\n",
        "--optimizer adam \\\n",
        "--lr 5e-4 \\\n",
        "--batch-size 128 \\\n",
        "--seed 1 \\\n",
        "--wandb-project \"Transformer from scratch - 10 March - kn to ml\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eF26otoyOkfC",
        "outputId": "dc8e54c0-af47-4c68-c4af-ef6a7f41bc58"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1;30;43mStreaming output truncated to the last 5000 lines.\u001b[0m\n",
            "|---------------------------------------------------------------------------|\n",
            "| Allocated memory      |    8778 MB |    8846 MB |    2324 GB |    2316 GB |\n",
            "|       from large pool |    8729 MB |    8797 MB |    2242 GB |    2234 GB |\n",
            "|       from small pool |      49 MB |      60 MB |      82 GB |      81 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Active memory         |    8778 MB |    8846 MB |    2324 GB |    2316 GB |\n",
            "|       from large pool |    8729 MB |    8797 MB |    2242 GB |    2234 GB |\n",
            "|       from small pool |      49 MB |      60 MB |      82 GB |      81 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| GPU reserved memory   |    9702 MB |   10864 MB |   58198 MB |   48496 MB |\n",
            "|       from large pool |    9640 MB |   10730 MB |   56702 MB |   47062 MB |\n",
            "|       from small pool |      62 MB |     134 MB |    1496 MB |    1434 MB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Non-releasable memory |     923 MB |    1345 MB |    2407 GB |    2406 GB |\n",
            "|       from large pool |     910 MB |    1331 MB |    2318 GB |    2317 GB |\n",
            "|       from small pool |      12 MB |      23 MB |      89 GB |      89 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Allocations           |     593    |     595    |     974 K  |     974 K  |\n",
            "|       from large pool |     160    |     164    |     339 K  |     339 K  |\n",
            "|       from small pool |     433    |     516    |     635 K  |     634 K  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Active allocs         |     593    |     595    |     974 K  |     974 K  |\n",
            "|       from large pool |     160    |     164    |     339 K  |     339 K  |\n",
            "|       from small pool |     433    |     516    |     635 K  |     634 K  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| GPU reserved segments |      74    |     112    |     860    |     786    |\n",
            "|       from large pool |      43    |      45    |     112    |      69    |\n",
            "|       from small pool |      31    |      67    |     748    |     717    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Non-releasable allocs |      49    |      50    |  447357    |  447308    |\n",
            "|       from large pool |      27    |      27    |  221053    |  221026    |\n",
            "|       from small pool |      22    |      44    |  226304    |  226282    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Oversize allocations  |       0    |       0    |       0    |       0    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Oversize GPU segments |       0    |       0    |       0    |       0    |\n",
            "|===========================================================================|\n",
            "\n",
            "2022-03-10 05:27:03 | WARNING | fairseq.trainer | attempting to recover from OOM in forward/backward pass\n",
            "epoch 002:  37% 262/712 [00:53<01:40,  4.47it/s, loss=4.592, ppl=24.11, wps=13512.4, ups=4.96, wpb=2724.4, bsz=126.9, num_updates=900, lr=0.0005, gnorm=0.94, train_wall=18, gb_free=9.9, wall=190]2022-03-10 05:27:25 | WARNING | fairseq.trainer | OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 1.76 GiB (GPU 0; 11.17 GiB total capacity; 8.04 GiB already allocated; 1.41 GiB free; 9.24 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
            "2022-03-10 05:27:25 | WARNING | fairseq.trainer | |===========================================================================|\n",
            "|                  PyTorch CUDA memory summary, device ID 0                 |\n",
            "|---------------------------------------------------------------------------|\n",
            "|            CUDA OOMs: 19           |        cudaMalloc retries: 35        |\n",
            "|===========================================================================|\n",
            "|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Allocated memory      |    8231 MB |    8295 MB |    2614 GB |    2606 GB |\n",
            "|       from large pool |    8182 MB |    8246 MB |    2523 GB |    2515 GB |\n",
            "|       from small pool |      48 MB |      60 MB |      91 GB |      91 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Active memory         |    8231 MB |    8295 MB |    2614 GB |    2606 GB |\n",
            "|       from large pool |    8182 MB |    8246 MB |    2523 GB |    2515 GB |\n",
            "|       from small pool |      48 MB |      60 MB |      91 GB |      91 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| GPU reserved memory   |    9462 MB |    9498 MB |   64312 MB |   54850 MB |\n",
            "|       from large pool |    9404 MB |    9404 MB |   62646 MB |   53242 MB |\n",
            "|       from small pool |      58 MB |      94 MB |    1666 MB |    1608 MB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Non-releasable memory |    1230 MB |    1545 MB |    2688 GB |    2687 GB |\n",
            "|       from large pool |    1221 MB |    1535 MB |    2588 GB |    2587 GB |\n",
            "|       from small pool |       9 MB |      21 MB |      99 GB |      99 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Allocations           |     593    |     595    |    1085 K  |    1084 K  |\n",
            "|       from large pool |     160    |     164    |     377 K  |     377 K  |\n",
            "|       from small pool |     433    |     516    |     707 K  |     707 K  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Active allocs         |     593    |     595    |    1085 K  |    1084 K  |\n",
            "|       from large pool |     160    |     164    |     377 K  |     377 K  |\n",
            "|       from small pool |     433    |     516    |     707 K  |     707 K  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| GPU reserved segments |      70    |      88    |     950    |     880    |\n",
            "|       from large pool |      41    |      41    |     117    |      76    |\n",
            "|       from small pool |      29    |      47    |     833    |     804    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Non-releasable allocs |      55    |      57    |  497807    |  497752    |\n",
            "|       from large pool |      31    |      31    |  245697    |  245666    |\n",
            "|       from small pool |      24    |      53    |  252110    |  252086    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Oversize allocations  |       0    |       0    |       0    |       0    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Oversize GPU segments |       0    |       0    |       0    |       0    |\n",
            "|===========================================================================|\n",
            "\n",
            "2022-03-10 05:27:25 | WARNING | fairseq.trainer | attempting to recover from OOM in forward/backward pass\n",
            "epoch 002:  42% 300/712 [01:00<01:18,  5.24it/s, loss=4.592, ppl=24.11, wps=13512.4, ups=4.96, wpb=2724.4, bsz=126.9, num_updates=900, lr=0.0005, gnorm=0.94, train_wall=18, gb_free=9.9, wall=190]2022-03-10 05:27:32 | WARNING | fairseq.trainer | OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 2.95 GiB (GPU 0; 11.17 GiB total capacity; 10.51 GiB already allocated; 21.81 MiB free; 10.63 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
            "epoch 002:  42% 301/712 [01:01<02:07,  3.21it/s, loss=4.592, ppl=24.11, wps=13512.4, ups=4.96, wpb=2724.4, bsz=126.9, num_updates=900, lr=0.0005, gnorm=0.94, train_wall=18, gb_free=9.9, wall=190]2022-03-10 05:27:32 | WARNING | fairseq.trainer | |===========================================================================|\n",
            "|                  PyTorch CUDA memory summary, device ID 0                 |\n",
            "|---------------------------------------------------------------------------|\n",
            "|            CUDA OOMs: 20           |        cudaMalloc retries: 37        |\n",
            "|===========================================================================|\n",
            "|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Allocated memory      |   10716 MB |   10765 MB |    2717 GB |    2706 GB |\n",
            "|       from large pool |   10664 MB |   10714 MB |    2622 GB |    2612 GB |\n",
            "|       from small pool |      51 MB |      60 MB |      94 GB |      94 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Active memory         |   10716 MB |   10765 MB |    2717 GB |    2706 GB |\n",
            "|       from large pool |   10664 MB |   10714 MB |    2622 GB |    2612 GB |\n",
            "|       from small pool |      51 MB |      60 MB |      94 GB |      94 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| GPU reserved memory   |   10886 MB |   10902 MB |   65802 MB |   54916 MB |\n",
            "|       from large pool |   10824 MB |   10824 MB |   64066 MB |   53242 MB |\n",
            "|       from small pool |      62 MB |     128 MB |    1736 MB |    1674 MB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Non-releasable memory |  174068 KB |    2002 MB |    2791 GB |    2790 GB |\n",
            "|       from large pool |  162912 KB |    1990 MB |    2687 GB |    2687 GB |\n",
            "|       from small pool |   11156 KB |      24 MB |     103 GB |     103 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Allocations           |     593    |     595    |    1127 K  |    1126 K  |\n",
            "|       from large pool |     162    |     163    |     393 K  |     392 K  |\n",
            "|       from small pool |     431    |     516    |     734 K  |     733 K  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Active allocs         |     593    |     595    |    1127 K  |    1126 K  |\n",
            "|       from large pool |     162    |     163    |     393 K  |     392 K  |\n",
            "|       from small pool |     431    |     516    |     734 K  |     733 K  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| GPU reserved segments |      77    |     109    |     990    |     913    |\n",
            "|       from large pool |      46    |      46    |     122    |      76    |\n",
            "|       from small pool |      31    |      64    |     868    |     837    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Non-releasable allocs |      59    |      59    |  516515    |  516456    |\n",
            "|       from large pool |      34    |      34    |  255570    |  255536    |\n",
            "|       from small pool |      25    |      44    |  260945    |  260920    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Oversize allocations  |       0    |       0    |       0    |       0    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Oversize GPU segments |       0    |       0    |       0    |       0    |\n",
            "|===========================================================================|\n",
            "\n",
            "2022-03-10 05:27:32 | WARNING | fairseq.trainer | attempting to recover from OOM in forward/backward pass\n",
            "epoch 002:  48% 340/712 [01:10<00:51,  7.26it/s, loss=4.464, ppl=22.07, wps=12239.9, ups=4.27, wpb=2869, bsz=128, num_updates=1000, lr=0.0005, gnorm=0.859, train_wall=22, gb_free=10.5, wall=213]2022-03-10 05:27:42 | WARNING | fairseq.trainer | OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 2.51 GiB (GPU 0; 11.17 GiB total capacity; 10.39 GiB already allocated; 117.81 MiB free; 10.54 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
            "2022-03-10 05:27:42 | WARNING | fairseq.trainer | |===========================================================================|\n",
            "|                  PyTorch CUDA memory summary, device ID 0                 |\n",
            "|---------------------------------------------------------------------------|\n",
            "|            CUDA OOMs: 21           |        cudaMalloc retries: 40        |\n",
            "|===========================================================================|\n",
            "|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Allocated memory      |   10601 MB |   10643 MB |    2839 GB |    2829 GB |\n",
            "|       from large pool |   10550 MB |   10592 MB |    2741 GB |    2731 GB |\n",
            "|       from small pool |      51 MB |      60 MB |      98 GB |      98 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Active memory         |   10601 MB |   10643 MB |    2839 GB |    2829 GB |\n",
            "|       from large pool |   10550 MB |   10592 MB |    2741 GB |    2731 GB |\n",
            "|       from small pool |      51 MB |      60 MB |      98 GB |      98 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| GPU reserved memory   |   10790 MB |   10864 MB |   81898 MB |   71108 MB |\n",
            "|       from large pool |   10732 MB |   10732 MB |   80032 MB |   69300 MB |\n",
            "|       from small pool |      58 MB |     132 MB |    1866 MB |    1808 MB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Non-releasable memory |  192960 KB |    1385 MB |    2903 GB |    2903 GB |\n",
            "|       from large pool |  185848 KB |    1377 MB |    2796 GB |    2795 GB |\n",
            "|       from small pool |    7112 KB |      14 MB |     107 GB |     107 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Allocations           |     593    |     595    |    1171 K  |    1171 K  |\n",
            "|       from large pool |     162    |     163    |     408 K  |     407 K  |\n",
            "|       from small pool |     431    |     516    |     763 K  |     763 K  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Active allocs         |     593    |     595    |    1171 K  |    1171 K  |\n",
            "|       from large pool |     162    |     163    |     408 K  |     407 K  |\n",
            "|       from small pool |     431    |     516    |     763 K  |     763 K  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| GPU reserved segments |      76    |     113    |    1094    |    1018    |\n",
            "|       from large pool |      47    |      47    |     161    |     114    |\n",
            "|       from small pool |      29    |      66    |     933    |     904    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Non-releasable allocs |      65    |      65    |  536528    |  536463    |\n",
            "|       from large pool |      40    |      40    |  265100    |  265060    |\n",
            "|       from small pool |      25    |      42    |  271428    |  271403    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Oversize allocations  |       0    |       0    |       0    |       0    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Oversize GPU segments |       0    |       0    |       0    |       0    |\n",
            "|===========================================================================|\n",
            "\n",
            "2022-03-10 05:27:42 | WARNING | fairseq.trainer | attempting to recover from OOM in forward/backward pass\n",
            "epoch 002:  56% 398/712 [01:23<00:45,  6.95it/s, loss=4.464, ppl=22.07, wps=12239.9, ups=4.27, wpb=2869, bsz=128, num_updates=1000, lr=0.0005, gnorm=0.859, train_wall=22, gb_free=10.5, wall=213]2022-03-10 05:27:55 | WARNING | fairseq.trainer | OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 1.83 GiB (GPU 0; 11.17 GiB total capacity; 8.20 GiB already allocated; 1.07 GiB free; 9.58 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
            "2022-03-10 05:27:55 | WARNING | fairseq.trainer | |===========================================================================|\n",
            "|                  PyTorch CUDA memory summary, device ID 0                 |\n",
            "|---------------------------------------------------------------------------|\n",
            "|            CUDA OOMs: 22           |        cudaMalloc retries: 44        |\n",
            "|===========================================================================|\n",
            "|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Allocated memory      |    8396 MB |    8460 MB |    2989 GB |    2981 GB |\n",
            "|       from large pool |    8338 MB |    8403 MB |    2885 GB |    2877 GB |\n",
            "|       from small pool |      57 MB |      61 MB |     104 GB |     104 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Active memory         |    8396 MB |    8460 MB |    2989 GB |    2981 GB |\n",
            "|       from large pool |    8338 MB |    8403 MB |    2885 GB |    2877 GB |\n",
            "|       from small pool |      57 MB |      61 MB |     104 GB |     104 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| GPU reserved memory   |    9812 MB |   10822 MB |   97300 MB |   87488 MB |\n",
            "|       from large pool |    9750 MB |   10684 MB |   95284 MB |   85534 MB |\n",
            "|       from small pool |      62 MB |     138 MB |    2016 MB |    1954 MB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Non-releasable memory |    1415 MB |    1907 MB |    3051 GB |    3049 GB |\n",
            "|       from large pool |    1411 MB |    1903 MB |    2937 GB |    2936 GB |\n",
            "|       from small pool |       4 MB |      21 MB |     113 GB |     113 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Allocations           |     593    |     595    |    1236 K  |    1235 K  |\n",
            "|       from large pool |     150    |     154    |     430 K  |     430 K  |\n",
            "|       from small pool |     443    |     516    |     805 K  |     805 K  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Active allocs         |     593    |     595    |    1236 K  |    1235 K  |\n",
            "|       from large pool |     150    |     154    |     430 K  |     430 K  |\n",
            "|       from small pool |     443    |     516    |     805 K  |     805 K  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| GPU reserved segments |      71    |     111    |    1179    |    1108    |\n",
            "|       from large pool |      40    |      42    |     171    |     131    |\n",
            "|       from small pool |      31    |      69    |    1008    |     977    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Non-releasable allocs |      54    |      55    |  565850    |  565796    |\n",
            "|       from large pool |      32    |      32    |  279456    |  279424    |\n",
            "|       from small pool |      22    |      37    |  286394    |  286372    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Oversize allocations  |       0    |       0    |       0    |       0    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Oversize GPU segments |       0    |       0    |       0    |       0    |\n",
            "|===========================================================================|\n",
            "\n",
            "2022-03-10 05:27:55 | WARNING | fairseq.trainer | attempting to recover from OOM in forward/backward pass\n",
            "epoch 002:  59% 422/712 [01:28<00:52,  5.55it/s, loss=4.392, ppl=20.99, wps=12152.5, ups=4.47, wpb=2721.5, bsz=128, num_updates=1100, lr=0.0005, gnorm=0.888, train_wall=21, gb_free=10.4, wall=236]2022-03-10 05:28:00 | WARNING | fairseq.trainer | OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 1.70 GiB (GPU 0; 11.17 GiB total capacity; 8.03 GiB already allocated; 1.07 GiB free; 9.58 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
            "2022-03-10 05:28:00 | WARNING | fairseq.trainer | |===========================================================================|\n",
            "|                  PyTorch CUDA memory summary, device ID 0                 |\n",
            "|---------------------------------------------------------------------------|\n",
            "|            CUDA OOMs: 23           |        cudaMalloc retries: 45        |\n",
            "|===========================================================================|\n",
            "|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Allocated memory      |    8218 MB |    8281 MB |    3061 GB |    3053 GB |\n",
            "|       from large pool |    8169 MB |    8232 MB |    2955 GB |    2947 GB |\n",
            "|       from small pool |      49 MB |      61 MB |     106 GB |     106 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Active memory         |    8218 MB |    8281 MB |    3061 GB |    3053 GB |\n",
            "|       from large pool |    8169 MB |    8232 MB |    2955 GB |    2947 GB |\n",
            "|       from small pool |      49 MB |      61 MB |     106 GB |     106 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| GPU reserved memory   |    9808 MB |    9888 MB |   97376 MB |   87568 MB |\n",
            "|       from large pool |    9750 MB |    9750 MB |   95284 MB |   85534 MB |\n",
            "|       from small pool |      58 MB |     138 MB |    2092 MB |    2034 MB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Non-releasable memory |    1589 MB |    1788 MB |    3126 GB |    3124 GB |\n",
            "|       from large pool |    1580 MB |    1778 MB |    3010 GB |    3008 GB |\n",
            "|       from small pool |       8 MB |      19 MB |     116 GB |     116 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Allocations           |     593    |     595    |    1262 K  |    1261 K  |\n",
            "|       from large pool |     160    |     164    |     439 K  |     439 K  |\n",
            "|       from small pool |     433    |     516    |     822 K  |     822 K  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Active allocs         |     593    |     595    |    1262 K  |    1261 K  |\n",
            "|       from large pool |     160    |     164    |     439 K  |     439 K  |\n",
            "|       from small pool |     433    |     516    |     822 K  |     822 K  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| GPU reserved segments |      69    |     109    |    1217    |    1148    |\n",
            "|       from large pool |      40    |      40    |     171    |     131    |\n",
            "|       from small pool |      29    |      69    |    1046    |    1017    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Non-releasable allocs |      56    |      57    |  577920    |  577864    |\n",
            "|       from large pool |      33    |      33    |  285282    |  285249    |\n",
            "|       from small pool |      23    |      38    |  292638    |  292615    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Oversize allocations  |       0    |       0    |       0    |       0    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Oversize GPU segments |       0    |       0    |       0    |       0    |\n",
            "|===========================================================================|\n",
            "\n",
            "2022-03-10 05:28:00 | WARNING | fairseq.trainer | attempting to recover from OOM in forward/backward pass\n",
            "epoch 002:  60% 427/712 [01:29<00:59,  4.75it/s, loss=4.392, ppl=20.99, wps=12152.5, ups=4.47, wpb=2721.5, bsz=128, num_updates=1100, lr=0.0005, gnorm=0.888, train_wall=21, gb_free=10.4, wall=236]2022-03-10 05:28:01 | WARNING | fairseq.trainer | OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 1.54 GiB (GPU 0; 11.17 GiB total capacity; 8.24 GiB already allocated; 1.09 GiB free; 9.56 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
            "2022-03-10 05:28:01 | WARNING | fairseq.trainer | |===========================================================================|\n",
            "|                  PyTorch CUDA memory summary, device ID 0                 |\n",
            "|---------------------------------------------------------------------------|\n",
            "|            CUDA OOMs: 24           |        cudaMalloc retries: 47        |\n",
            "|===========================================================================|\n",
            "|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Allocated memory      |    6859 MB |    8433 MB |    3080 GB |    3073 GB |\n",
            "|       from large pool |    6801 MB |    8375 MB |    2973 GB |    2967 GB |\n",
            "|       from small pool |      58 MB |      61 MB |     106 GB |     106 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Active memory         |    6859 MB |    8433 MB |    3080 GB |    3073 GB |\n",
            "|       from large pool |    6801 MB |    8375 MB |    2973 GB |    2967 GB |\n",
            "|       from small pool |      58 MB |      61 MB |     106 GB |     106 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| GPU reserved memory   |    9794 MB |    9824 MB |   98968 MB |   89174 MB |\n",
            "|       from large pool |    9732 MB |    9750 MB |   96860 MB |   87128 MB |\n",
            "|       from small pool |      62 MB |      74 MB |    2108 MB |    2046 MB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Non-releasable memory |    1358 MB |    1360 MB |    3143 GB |    3141 GB |\n",
            "|       from large pool |    1354 MB |    1356 MB |    3026 GB |    3025 GB |\n",
            "|       from small pool |       3 MB |      12 MB |     116 GB |     116 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Allocations           |     592    |     595    |    1267 K  |    1266 K  |\n",
            "|       from large pool |     150    |     154    |     441 K  |     441 K  |\n",
            "|       from small pool |     442    |     516    |     825 K  |     825 K  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Active allocs         |     592    |     595    |    1267 K  |    1266 K  |\n",
            "|       from large pool |     150    |     154    |     441 K  |     441 K  |\n",
            "|       from small pool |     442    |     516    |     825 K  |     825 K  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| GPU reserved segments |      69    |      77    |    1226    |    1157    |\n",
            "|       from large pool |      38    |      40    |     172    |     134    |\n",
            "|       from small pool |      31    |      37    |    1054    |    1023    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Non-releasable allocs |      42    |      43    |  580161    |  580119    |\n",
            "|       from large pool |      18    |      19    |  286529    |  286511    |\n",
            "|       from small pool |      24    |      27    |  293632    |  293608    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Oversize allocations  |       0    |       0    |       0    |       0    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Oversize GPU segments |       0    |       0    |       0    |       0    |\n",
            "|===========================================================================|\n",
            "\n",
            "2022-03-10 05:28:01 | WARNING | fairseq.trainer | attempting to recover from OOM in forward/backward pass\n",
            "epoch 002:  66% 469/712 [01:37<00:41,  5.87it/s, loss=4.392, ppl=20.99, wps=12152.5, ups=4.47, wpb=2721.5, bsz=128, num_updates=1100, lr=0.0005, gnorm=0.888, train_wall=21, gb_free=10.4, wall=236]2022-03-10 05:28:09 | WARNING | fairseq.trainer | OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 1.76 GiB (GPU 0; 11.17 GiB total capacity; 7.35 GiB already allocated; 1.69 GiB free; 8.96 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
            "2022-03-10 05:28:09 | WARNING | fairseq.trainer | |===========================================================================|\n",
            "|                  PyTorch CUDA memory summary, device ID 0                 |\n",
            "|---------------------------------------------------------------------------|\n",
            "|            CUDA OOMs: 25           |        cudaMalloc retries: 48        |\n",
            "|===========================================================================|\n",
            "|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Allocated memory      |    7525 MB |    7549 MB |    3181 GB |    3173 GB |\n",
            "|       from large pool |    7476 MB |    7500 MB |    3070 GB |    3062 GB |\n",
            "|       from small pool |      49 MB |      61 MB |     111 GB |     111 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Active memory         |    7525 MB |    7549 MB |    3181 GB |    3173 GB |\n",
            "|       from large pool |    7476 MB |    7500 MB |    3070 GB |    3062 GB |\n",
            "|       from small pool |      49 MB |      61 MB |     111 GB |     111 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| GPU reserved memory   |    9178 MB |    9266 MB |  100016 MB |   90838 MB |\n",
            "|       from large pool |    9120 MB |    9144 MB |   97848 MB |   88728 MB |\n",
            "|       from small pool |      58 MB |     122 MB |    2168 MB |    2110 MB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Non-releasable memory |    1652 MB |    1910 MB |    3253 GB |    3252 GB |\n",
            "|       from large pool |    1643 MB |    1900 MB |    3132 GB |    3131 GB |\n",
            "|       from small pool |       8 MB |      14 MB |     121 GB |     121 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Allocations           |     592    |     595    |    1313 K  |    1312 K  |\n",
            "|       from large pool |     162    |     164    |     456 K  |     456 K  |\n",
            "|       from small pool |     430    |     516    |     857 K  |     856 K  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Active allocs         |     592    |     595    |    1313 K  |    1312 K  |\n",
            "|       from large pool |     162    |     164    |     456 K  |     456 K  |\n",
            "|       from small pool |     430    |     516    |     857 K  |     856 K  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| GPU reserved segments |      65    |      99    |    1257    |    1192    |\n",
            "|       from large pool |      36    |      38    |     173    |     137    |\n",
            "|       from small pool |      29    |      61    |    1084    |    1055    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Non-releasable allocs |      50    |      51    |  600959    |  600909    |\n",
            "|       from large pool |      28    |      29    |  296143    |  296115    |\n",
            "|       from small pool |      22    |      27    |  304816    |  304794    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Oversize allocations  |       0    |       0    |       0    |       0    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Oversize GPU segments |       0    |       0    |       0    |       0    |\n",
            "|===========================================================================|\n",
            "\n",
            "2022-03-10 05:28:09 | WARNING | fairseq.trainer | attempting to recover from OOM in forward/backward pass\n",
            "epoch 002: 100% 711/712 [02:25<00:00,  4.12it/s, loss=4.134, ppl=17.55, wps=14508.3, ups=5.05, wpb=2872.4, bsz=128, num_updates=1300, lr=0.0005, gnorm=0.858, train_wall=19, gb_free=5.4, wall=277]2022-03-10 05:28:57 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "\n",
            "epoch 002 | valid on 'valid' subset:   0% 0/8 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 002 | valid on 'valid' subset:  12% 1/8 [00:00<00:00,  7.09it/s]\u001b[A\n",
            "epoch 002 | valid on 'valid' subset:  38% 3/8 [00:00<00:00, 10.95it/s]\u001b[A\n",
            "epoch 002 | valid on 'valid' subset:  62% 5/8 [00:00<00:00, 11.32it/s]\u001b[A\n",
            "epoch 002 | valid on 'valid' subset:  88% 7/8 [00:00<00:00,  9.99it/s]\u001b[A\n",
            "                                                                      \u001b[A2022-03-10 05:28:58 | INFO | valid | epoch 002 | valid on 'valid' subset | loss 4.138 | ppl 17.61 | wps 20317.3 | wpb 2800.1 | bsz 125 | num_updates 1399 | best_loss 4.138\n",
            "2022-03-10 05:28:58 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 2 @ 1399 updates\n",
            "2022-03-10 05:28:58 | INFO | fairseq.trainer | Saving checkpoint to /content/checkpoints/checkpoint2.pt\n",
            "2022-03-10 05:28:59 | INFO | fairseq.trainer | Finished saving checkpoint to /content/checkpoints/checkpoint2.pt\n",
            "2022-03-10 05:29:00 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/checkpoint2.pt (epoch 2 @ 1399 updates, score 4.138) (writing took 2.2952427780001017 seconds)\n",
            "2022-03-10 05:29:00 | INFO | fairseq_cli.train | end of epoch 2 (average epoch stats below)\n",
            "2022-03-10 05:29:00 | INFO | train | epoch 002 | loss 4.361 | ppl 20.55 | wps 12987.5 | ups 4.68 | wpb 2772.7 | bsz 127.7 | num_updates 1399 | lr 0.0005 | gnorm 0.889 | train_wall 136 | gb_free 9.1 | wall 299\n",
            "2022-03-10 05:29:00 | INFO | fairseq.data.iterators | grouped total_num_itrs = 712\n",
            "epoch 003:   0% 0/712 [00:00<?, ?it/s]2022-03-10 05:29:01 | INFO | fairseq.trainer | begin training epoch 3\n",
            "2022-03-10 05:29:01 | INFO | fairseq_cli.train | Start iterating over samples\n",
            "epoch 003:   7% 53/712 [00:11<01:45,  6.27it/s, loss=4.096, ppl=17.1, wps=12842.7, ups=4.52, wpb=2839.1, bsz=128, num_updates=1400, lr=0.0005, gnorm=0.85, train_wall=18, gb_free=8, wall=299]2022-03-10 05:29:12 | WARNING | fairseq.trainer | OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 1.76 GiB (GPU 0; 11.17 GiB total capacity; 8.04 GiB already allocated; 1.54 GiB free; 9.11 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
            "2022-03-10 05:29:12 | WARNING | fairseq.trainer | |===========================================================================|\n",
            "|                  PyTorch CUDA memory summary, device ID 0                 |\n",
            "|---------------------------------------------------------------------------|\n",
            "|            CUDA OOMs: 26           |        cudaMalloc retries: 50        |\n",
            "|===========================================================================|\n",
            "|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Allocated memory      |    8231 MB |    8295 MB |    3995 GB |    3987 GB |\n",
            "|       from large pool |    8182 MB |    8246 MB |    3857 GB |    3849 GB |\n",
            "|       from small pool |      48 MB |      61 MB |     138 GB |     138 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Active memory         |    8231 MB |    8295 MB |    3995 GB |    3987 GB |\n",
            "|       from large pool |    8182 MB |    8246 MB |    3857 GB |    3849 GB |\n",
            "|       from small pool |      48 MB |      61 MB |     138 GB |     138 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| GPU reserved memory   |    9330 MB |   10886 MB |  104424 MB |   95094 MB |\n",
            "|       from large pool |    9272 MB |   10748 MB |  102100 MB |   92828 MB |\n",
            "|       from small pool |      58 MB |     138 MB |    2324 MB |    2266 MB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Non-releasable memory |    1098 MB |    1463 MB |    4099 GB |    4098 GB |\n",
            "|       from large pool |    1089 MB |    1454 MB |    3948 GB |    3947 GB |\n",
            "|       from small pool |       9 MB |      16 MB |     150 GB |     150 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Allocations           |     593    |     595    |    1649 K  |    1648 K  |\n",
            "|       from large pool |     160    |     164    |     574 K  |     574 K  |\n",
            "|       from small pool |     433    |     516    |    1074 K  |    1073 K  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Active allocs         |     593    |     595    |    1649 K  |    1648 K  |\n",
            "|       from large pool |     160    |     164    |     574 K  |     574 K  |\n",
            "|       from small pool |     433    |     516    |    1074 K  |    1073 K  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| GPU reserved segments |      65    |     106    |    1338    |    1273    |\n",
            "|       from large pool |      36    |      37    |     176    |     140    |\n",
            "|       from small pool |      29    |      69    |    1162    |    1133    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Non-releasable allocs |      51    |      51    |     751 K  |     751 K  |\n",
            "|       from large pool |      25    |      25    |     370 K  |     370 K  |\n",
            "|       from small pool |      26    |      37    |     380 K  |     380 K  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Oversize allocations  |       0    |       0    |       0    |       0    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Oversize GPU segments |       0    |       0    |       0    |       0    |\n",
            "|===========================================================================|\n",
            "\n",
            "2022-03-10 05:29:12 | WARNING | fairseq.trainer | attempting to recover from OOM in forward/backward pass\n",
            "epoch 003:   8% 58/712 [00:12<01:44,  6.24it/s, loss=4.096, ppl=17.1, wps=12842.7, ups=4.52, wpb=2839.1, bsz=128, num_updates=1400, lr=0.0005, gnorm=0.85, train_wall=18, gb_free=8, wall=299]2022-03-10 05:29:13 | WARNING | fairseq.trainer | OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 1.49 GiB (GPU 0; 11.17 GiB total capacity; 8.10 GiB already allocated; 1.49 GiB free; 9.16 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
            "2022-03-10 05:29:13 | WARNING | fairseq.trainer | |===========================================================================|\n",
            "|                  PyTorch CUDA memory summary, device ID 0                 |\n",
            "|---------------------------------------------------------------------------|\n",
            "|            CUDA OOMs: 27           |        cudaMalloc retries: 51        |\n",
            "|===========================================================================|\n",
            "|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Allocated memory      |    6767 MB |    8296 MB |    4010 GB |    4004 GB |\n",
            "|       from large pool |    6718 MB |    8247 MB |    3872 GB |    3865 GB |\n",
            "|       from small pool |      48 MB |      60 MB |     138 GB |     138 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Active memory         |    6767 MB |    8296 MB |    4010 GB |    4004 GB |\n",
            "|       from large pool |    6718 MB |    8247 MB |    3872 GB |    3865 GB |\n",
            "|       from small pool |      48 MB |      60 MB |     138 GB |     138 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| GPU reserved memory   |    9384 MB |   10906 MB |  106000 MB |   96616 MB |\n",
            "|       from large pool |    9326 MB |   10802 MB |  103630 MB |   94304 MB |\n",
            "|       from small pool |      58 MB |     104 MB |    2370 MB |    2312 MB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Non-releasable memory |    1086 MB |    1087 MB |    4111 GB |    4110 GB |\n",
            "|       from large pool |    1077 MB |    1078 MB |    3960 GB |    3959 GB |\n",
            "|       from small pool |       9 MB |      19 MB |     151 GB |     151 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Allocations           |     592    |     595    |    1653 K  |    1653 K  |\n",
            "|       from large pool |     160    |     164    |     576 K  |     576 K  |\n",
            "|       from small pool |     432    |     516    |    1077 K  |    1077 K  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Active allocs         |     592    |     595    |    1653 K  |    1653 K  |\n",
            "|       from large pool |     160    |     164    |     576 K  |     576 K  |\n",
            "|       from small pool |     432    |     516    |    1077 K  |    1077 K  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| GPU reserved segments |      65    |      89    |    1362    |    1297    |\n",
            "|       from large pool |      36    |      37    |     177    |     141    |\n",
            "|       from small pool |      29    |      52    |    1185    |    1156    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Non-releasable allocs |      43    |      47    |     753 K  |     753 K  |\n",
            "|       from large pool |      21    |      22    |     371 K  |     371 K  |\n",
            "|       from small pool |      22    |      41    |     381 K  |     381 K  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Oversize allocations  |       0    |       0    |       0    |       0    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Oversize GPU segments |       0    |       0    |       0    |       0    |\n",
            "|===========================================================================|\n",
            "\n",
            "2022-03-10 05:29:13 | WARNING | fairseq.trainer | attempting to recover from OOM in forward/backward pass\n",
            "epoch 003:  12% 86/712 [00:19<02:53,  3.61it/s, loss=4.096, ppl=17.1, wps=12842.7, ups=4.52, wpb=2839.1, bsz=128, num_updates=1400, lr=0.0005, gnorm=0.85, train_wall=18, gb_free=8, wall=299]2022-03-10 05:29:20 | WARNING | fairseq.trainer | OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 1.83 GiB (GPU 0; 11.17 GiB total capacity; 8.20 GiB already allocated; 1.77 GiB free; 8.88 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
            "2022-03-10 05:29:20 | WARNING | fairseq.trainer | |===========================================================================|\n",
            "|                  PyTorch CUDA memory summary, device ID 0                 |\n",
            "|---------------------------------------------------------------------------|\n",
            "|            CUDA OOMs: 28           |        cudaMalloc retries: 52        |\n",
            "|===========================================================================|\n",
            "|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Allocated memory      |    8396 MB |    8461 MB |    4105 GB |    4096 GB |\n",
            "|       from large pool |    8338 MB |    8404 MB |    3964 GB |    3956 GB |\n",
            "|       from small pool |      57 MB |      61 MB |     140 GB |     140 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Active memory         |    8396 MB |    8461 MB |    4105 GB |    4096 GB |\n",
            "|       from large pool |    8338 MB |    8404 MB |    3964 GB |    3956 GB |\n",
            "|       from small pool |      57 MB |      61 MB |     140 GB |     140 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| GPU reserved memory   |    9098 MB |   10386 MB |  108532 MB |   99434 MB |\n",
            "|       from large pool |    9036 MB |   10276 MB |  106110 MB |   97074 MB |\n",
            "|       from small pool |      62 MB |     110 MB |    2422 MB |    2360 MB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Non-releasable memory |  718388 KB |    1220 MB |    4202 GB |    4201 GB |\n",
            "|       from large pool |  714064 KB |    1216 MB |    4048 GB |    4048 GB |\n",
            "|       from small pool |    4324 KB |      12 MB |     153 GB |     153 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Allocations           |     593    |     595    |    1684 K  |    1684 K  |\n",
            "|       from large pool |     150    |     154    |     588 K  |     587 K  |\n",
            "|       from small pool |     443    |     516    |    1096 K  |    1096 K  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Active allocs         |     593    |     595    |    1684 K  |    1684 K  |\n",
            "|       from large pool |     150    |     154    |     588 K  |     587 K  |\n",
            "|       from small pool |     443    |     516    |    1096 K  |    1096 K  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| GPU reserved segments |      67    |      92    |    1390    |    1323    |\n",
            "|       from large pool |      36    |      37    |     179    |     143    |\n",
            "|       from small pool |      31    |      55    |    1211    |    1180    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Non-releasable allocs |      50    |      51    |     767 K  |     767 K  |\n",
            "|       from large pool |      27    |      27    |     379 K  |     379 K  |\n",
            "|       from small pool |      23    |      33    |     388 K  |     388 K  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Oversize allocations  |       0    |       0    |       0    |       0    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Oversize GPU segments |       0    |       0    |       0    |       0    |\n",
            "|===========================================================================|\n",
            "\n",
            "2022-03-10 05:29:20 | WARNING | fairseq.trainer | attempting to recover from OOM in forward/backward pass\n",
            "epoch 003:  17% 120/712 [00:25<01:51,  5.31it/s, loss=3.876, ppl=14.68, wps=12850.7, ups=4.47, wpb=2873.2, bsz=128, num_updates=1500, lr=0.0005, gnorm=0.846, train_wall=20, gb_free=10.1, wall=322]2022-03-10 05:29:27 | WARNING | fairseq.trainer | OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 1.52 GiB (GPU 0; 11.17 GiB total capacity; 8.51 GiB already allocated; 413.81 MiB free; 10.25 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
            "2022-03-10 05:29:27 | WARNING | fairseq.trainer | |===========================================================================|\n",
            "|                  PyTorch CUDA memory summary, device ID 0                 |\n",
            "|---------------------------------------------------------------------------|\n",
            "|            CUDA OOMs: 29           |        cudaMalloc retries: 54        |\n",
            "|===========================================================================|\n",
            "|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Allocated memory      |    7166 MB |    8718 MB |    4193 GB |    4186 GB |\n",
            "|       from large pool |    7118 MB |    8670 MB |    4049 GB |    4042 GB |\n",
            "|       from small pool |      48 MB |      61 MB |     143 GB |     143 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Active memory         |    7166 MB |    8718 MB |    4193 GB |    4186 GB |\n",
            "|       from large pool |    7118 MB |    8670 MB |    4049 GB |    4042 GB |\n",
            "|       from small pool |      48 MB |      61 MB |     143 GB |     143 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| GPU reserved memory   |   10494 MB |   10494 MB |  111258 MB |  100764 MB |\n",
            "|       from large pool |   10436 MB |   10436 MB |  108764 MB |   98328 MB |\n",
            "|       from small pool |      58 MB |     134 MB |    2494 MB |    2436 MB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Non-releasable memory |    1775 MB |    1775 MB |    4291 GB |    4289 GB |\n",
            "|       from large pool |    1765 MB |    1765 MB |    4133 GB |    4132 GB |\n",
            "|       from small pool |       9 MB |      13 MB |     157 GB |     157 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Allocations           |     592    |     595    |    1722 K  |    1721 K  |\n",
            "|       from large pool |     160    |     164    |     601 K  |     601 K  |\n",
            "|       from small pool |     432    |     516    |    1120 K  |    1120 K  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Active allocs         |     592    |     595    |    1722 K  |    1721 K  |\n",
            "|       from large pool |     160    |     164    |     601 K  |     601 K  |\n",
            "|       from small pool |     432    |     516    |    1120 K  |    1120 K  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| GPU reserved segments |      65    |     104    |    1428    |    1363    |\n",
            "|       from large pool |      36    |      37    |     181    |     145    |\n",
            "|       from small pool |      29    |      67    |    1247    |    1218    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Non-releasable allocs |      46    |      46    |     784 K  |     784 K  |\n",
            "|       from large pool |      22    |      22    |     387 K  |     387 K  |\n",
            "|       from small pool |      24    |      31    |     396 K  |     396 K  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Oversize allocations  |       0    |       0    |       0    |       0    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Oversize GPU segments |       0    |       0    |       0    |       0    |\n",
            "|===========================================================================|\n",
            "\n",
            "2022-03-10 05:29:27 | WARNING | fairseq.trainer | attempting to recover from OOM in forward/backward pass\n",
            "epoch 003:  23% 164/712 [00:34<01:25,  6.39it/s, loss=3.876, ppl=14.68, wps=12850.7, ups=4.47, wpb=2873.2, bsz=128, num_updates=1500, lr=0.0005, gnorm=0.846, train_wall=20, gb_free=10.1, wall=322]2022-03-10 05:29:36 | WARNING | fairseq.trainer | OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 2.95 GiB (GPU 0; 11.17 GiB total capacity; 10.51 GiB already allocated; 33.81 MiB free; 10.62 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
            "epoch 003:  23% 165/712 [00:35<02:24,  3.77it/s, loss=3.876, ppl=14.68, wps=12850.7, ups=4.47, wpb=2873.2, bsz=128, num_updates=1500, lr=0.0005, gnorm=0.846, train_wall=20, gb_free=10.1, wall=322]2022-03-10 05:29:36 | WARNING | fairseq.trainer | |===========================================================================|\n",
            "|                  PyTorch CUDA memory summary, device ID 0                 |\n",
            "|---------------------------------------------------------------------------|\n",
            "|            CUDA OOMs: 30           |        cudaMalloc retries: 57        |\n",
            "|===========================================================================|\n",
            "|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Allocated memory      |   10712 MB |   10761 MB |    4321 GB |    4311 GB |\n",
            "|       from large pool |   10661 MB |   10710 MB |    4173 GB |    4163 GB |\n",
            "|       from small pool |      51 MB |      61 MB |     148 GB |     147 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Active memory         |   10712 MB |   10761 MB |    4321 GB |    4311 GB |\n",
            "|       from large pool |   10661 MB |   10710 MB |    4173 GB |    4163 GB |\n",
            "|       from small pool |      51 MB |      61 MB |     148 GB |     147 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| GPU reserved memory   |   10874 MB |   10874 MB |  114394 MB |  103520 MB |\n",
            "|       from large pool |   10816 MB |   10816 MB |  111798 MB |  100982 MB |\n",
            "|       from small pool |      58 MB |     116 MB |    2596 MB |    2538 MB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Non-releasable memory |  165768 KB |    2406 MB |    4416 GB |    4416 GB |\n",
            "|       from large pool |  158708 KB |    2397 MB |    4254 GB |    4254 GB |\n",
            "|       from small pool |    7060 KB |      12 MB |     161 GB |     161 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Allocations           |     593    |     595    |    1770 K  |    1770 K  |\n",
            "|       from large pool |     162    |     163    |     617 K  |     617 K  |\n",
            "|       from small pool |     431    |     516    |    1153 K  |    1152 K  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Active allocs         |     593    |     595    |    1770 K  |    1770 K  |\n",
            "|       from large pool |     162    |     163    |     617 K  |     617 K  |\n",
            "|       from small pool |     431    |     516    |    1153 K  |    1152 K  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| GPU reserved segments |      67    |      94    |    1483    |    1416    |\n",
            "|       from large pool |      38    |      38    |     185    |     147    |\n",
            "|       from small pool |      29    |      58    |    1298    |    1269    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Non-releasable allocs |      57    |      57    |     806 K  |     805 K  |\n",
            "|       from large pool |      32    |      32    |     398 K  |     398 K  |\n",
            "|       from small pool |      25    |      30    |     407 K  |     407 K  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Oversize allocations  |       0    |       0    |       0    |       0    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Oversize GPU segments |       0    |       0    |       0    |       0    |\n",
            "|===========================================================================|\n",
            "\n",
            "2022-03-10 05:29:36 | WARNING | fairseq.trainer | attempting to recover from OOM in forward/backward pass\n",
            "epoch 003:  32% 227/712 [00:46<02:10,  3.71it/s, loss=3.705, ppl=13.04, wps=13307.5, ups=4.9, wpb=2716.8, bsz=128, num_updates=1600, lr=0.0005, gnorm=0.911, train_wall=19, gb_free=9.4, wall=342]2022-03-10 05:29:47 | WARNING | fairseq.trainer | OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 2.96 GiB (GPU 0; 11.17 GiB total capacity; 10.42 GiB already allocated; 69.81 MiB free; 10.58 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
            "2022-03-10 05:29:47 | WARNING | fairseq.trainer | |===========================================================================|\n",
            "|                  PyTorch CUDA memory summary, device ID 0                 |\n",
            "|---------------------------------------------------------------------------|\n",
            "|            CUDA OOMs: 31           |        cudaMalloc retries: 60        |\n",
            "|===========================================================================|\n",
            "|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Allocated memory      |   10622 MB |   10672 MB |    4453 GB |    4442 GB |\n",
            "|       from large pool |   10571 MB |   10621 MB |    4299 GB |    4289 GB |\n",
            "|       from small pool |      51 MB |      60 MB |     153 GB |     153 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Active memory         |   10622 MB |   10672 MB |    4453 GB |    4442 GB |\n",
            "|       from large pool |   10571 MB |   10621 MB |    4299 GB |    4289 GB |\n",
            "|       from small pool |      51 MB |      60 MB |     153 GB |     153 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| GPU reserved memory   |   10838 MB |   10906 MB |  126124 MB |  115286 MB |\n",
            "|       from large pool |   10780 MB |   10790 MB |  123470 MB |  112690 MB |\n",
            "|       from small pool |      58 MB |     116 MB |    2654 MB |    2596 MB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Non-releasable memory |  220595 KB |     791 MB |    4539 GB |    4539 GB |\n",
            "|       from large pool |  213504 KB |     783 MB |    4372 GB |    4372 GB |\n",
            "|       from small pool |    7091 KB |      18 MB |     167 GB |     167 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Allocations           |     593    |     595    |    1839 K  |    1839 K  |\n",
            "|       from large pool |     162    |     163    |     642 K  |     642 K  |\n",
            "|       from small pool |     431    |     516    |    1197 K  |    1197 K  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Active allocs         |     593    |     595    |    1839 K  |    1839 K  |\n",
            "|       from large pool |     162    |     163    |     642 K  |     642 K  |\n",
            "|       from small pool |     431    |     516    |    1197 K  |    1197 K  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| GPU reserved segments |      72    |      97    |    1549    |    1477    |\n",
            "|       from large pool |      43    |      43    |     222    |     179    |\n",
            "|       from small pool |      29    |      58    |    1327    |    1298    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Non-releasable allocs |      54    |      54    |     836 K  |     836 K  |\n",
            "|       from large pool |      30    |      30    |     413 K  |     413 K  |\n",
            "|       from small pool |      24    |      42    |     423 K  |     423 K  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Oversize allocations  |       0    |       0    |       0    |       0    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Oversize GPU segments |       0    |       0    |       0    |       0    |\n",
            "|===========================================================================|\n",
            "\n",
            "2022-03-10 05:29:47 | WARNING | fairseq.trainer | attempting to recover from OOM in forward/backward pass\n",
            "epoch 003:  50% 358/712 [01:11<00:55,  6.43it/s, loss=3.823, ppl=14.15, wps=13760.9, ups=5.08, wpb=2710.7, bsz=126.7, num_updates=1700, lr=0.0005, gnorm=1.038, train_wall=19, gb_free=10.6, wall=362]2022-03-10 05:30:12 | WARNING | fairseq.trainer | OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 1.87 GiB (GPU 0; 11.17 GiB total capacity; 8.57 GiB already allocated; 785.81 MiB free; 9.88 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
            "epoch 003:  50% 359/712 [01:11<01:46,  3.30it/s, loss=3.823, ppl=14.15, wps=13760.9, ups=5.08, wpb=2710.7, bsz=126.7, num_updates=1700, lr=0.0005, gnorm=1.038, train_wall=19, gb_free=10.6, wall=362]2022-03-10 05:30:12 | WARNING | fairseq.trainer | |===========================================================================|\n",
            "|                  PyTorch CUDA memory summary, device ID 0                 |\n",
            "|---------------------------------------------------------------------------|\n",
            "|            CUDA OOMs: 32           |        cudaMalloc retries: 64        |\n",
            "|===========================================================================|\n",
            "|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Allocated memory      |    8775 MB |    8842 MB |    4761 GB |    4753 GB |\n",
            "|       from large pool |    8726 MB |    8793 MB |    4594 GB |    4586 GB |\n",
            "|       from small pool |      49 MB |      60 MB |     167 GB |     167 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Active memory         |    8775 MB |    8842 MB |    4761 GB |    4753 GB |\n",
            "|       from large pool |    8726 MB |    8793 MB |    4594 GB |    4586 GB |\n",
            "|       from small pool |      49 MB |      60 MB |     167 GB |     167 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| GPU reserved memory   |   10122 MB |   10770 MB |  139560 MB |  129438 MB |\n",
            "|       from large pool |   10064 MB |   10632 MB |  136726 MB |  126662 MB |\n",
            "|       from small pool |      58 MB |     138 MB |    2834 MB |    2776 MB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Non-releasable memory |    1346 MB |    1597 MB |    4857 GB |    4855 GB |\n",
            "|       from large pool |    1337 MB |    1588 MB |    4674 GB |    4673 GB |\n",
            "|       from small pool |       8 MB |      18 MB |     182 GB |     182 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Allocations           |     593    |     595    |    1987 K  |    1987 K  |\n",
            "|       from large pool |     160    |     164    |     691 K  |     691 K  |\n",
            "|       from small pool |     433    |     516    |    1295 K  |    1295 K  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Active allocs         |     593    |     595    |    1987 K  |    1987 K  |\n",
            "|       from large pool |     160    |     164    |     691 K  |     691 K  |\n",
            "|       from small pool |     433    |     516    |    1295 K  |    1295 K  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| GPU reserved segments |      61    |     102    |    1649    |    1588    |\n",
            "|       from large pool |      32    |      33    |     232    |     200    |\n",
            "|       from small pool |      29    |      69    |    1417    |    1388    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Non-releasable allocs |      42    |      51    |     903 K  |     903 K  |\n",
            "|       from large pool |      21    |      21    |     443 K  |     443 K  |\n",
            "|       from small pool |      21    |      46    |     459 K  |     459 K  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Oversize allocations  |       0    |       0    |       0    |       0    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Oversize GPU segments |       0    |       0    |       0    |       0    |\n",
            "|===========================================================================|\n",
            "\n",
            "2022-03-10 05:30:12 | WARNING | fairseq.trainer | attempting to recover from OOM in forward/backward pass\n",
            "epoch 003:  54% 386/712 [01:18<01:50,  2.96it/s, loss=3.823, ppl=14.15, wps=13760.9, ups=5.08, wpb=2710.7, bsz=126.7, num_updates=1700, lr=0.0005, gnorm=1.038, train_wall=19, gb_free=10.6, wall=362]2022-03-10 05:30:20 | WARNING | fairseq.trainer | OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 1.54 GiB (GPU 0; 11.17 GiB total capacity; 8.23 GiB already allocated; 209.81 MiB free; 10.45 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
            "2022-03-10 05:30:20 | WARNING | fairseq.trainer | |===========================================================================|\n",
            "|                  PyTorch CUDA memory summary, device ID 0                 |\n",
            "|---------------------------------------------------------------------------|\n",
            "|            CUDA OOMs: 33           |        cudaMalloc retries: 68        |\n",
            "|===========================================================================|\n",
            "|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Allocated memory      |    6851 MB |    8426 MB |    4864 GB |    4858 GB |\n",
            "|       from large pool |    6793 MB |    8368 MB |    4695 GB |    4688 GB |\n",
            "|       from small pool |      58 MB |      61 MB |     169 GB |     169 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Active memory         |    6851 MB |    8426 MB |    4864 GB |    4858 GB |\n",
            "|       from large pool |    6793 MB |    8368 MB |    4695 GB |    4688 GB |\n",
            "|       from small pool |      58 MB |      61 MB |     169 GB |     169 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| GPU reserved memory   |   10698 MB |   10698 MB |  146916 MB |  136218 MB |\n",
            "|       from large pool |   10636 MB |   10636 MB |  143902 MB |  133266 MB |\n",
            "|       from small pool |      62 MB |     128 MB |    3014 MB |    2952 MB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Non-releasable memory |    2270 MB |    2271 MB |    4952 GB |    4950 GB |\n",
            "|       from large pool |    2266 MB |    2267 MB |    4767 GB |    4765 GB |\n",
            "|       from small pool |       3 MB |      12 MB |     185 GB |     185 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Allocations           |     592    |     595    |    2018 K  |    2018 K  |\n",
            "|       from large pool |     150    |     154    |     702 K  |     702 K  |\n",
            "|       from small pool |     442    |     516    |    1315 K  |    1315 K  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Active allocs         |     592    |     595    |    2018 K  |    2018 K  |\n",
            "|       from large pool |     150    |     154    |     702 K  |     702 K  |\n",
            "|       from small pool |     442    |     516    |    1315 K  |    1315 K  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| GPU reserved segments |      63    |      96    |    1744    |    1681    |\n",
            "|       from large pool |      32    |      32    |     237    |     205    |\n",
            "|       from small pool |      31    |      64    |    1507    |    1476    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Non-releasable allocs |      48    |      49    |     916 K  |     916 K  |\n",
            "|       from large pool |      26    |      27    |     450 K  |     450 K  |\n",
            "|       from small pool |      22    |      24    |     466 K  |     466 K  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Oversize allocations  |       0    |       0    |       0    |       0    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Oversize GPU segments |       0    |       0    |       0    |       0    |\n",
            "|===========================================================================|\n",
            "\n",
            "2022-03-10 05:30:20 | WARNING | fairseq.trainer | attempting to recover from OOM in forward/backward pass\n",
            "epoch 003:  61% 435/712 [01:27<00:45,  6.12it/s, loss=3.672, ppl=12.75, wps=12885, ups=4.86, wpb=2650.8, bsz=128, num_updates=1800, lr=0.0005, gnorm=0.877, train_wall=19, gb_free=10.3, wall=382]2022-03-10 05:30:29 | WARNING | fairseq.trainer | OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 1.44 GiB (GPU 0; 11.17 GiB total capacity; 7.98 GiB already allocated; 327.81 MiB free; 10.33 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
            "2022-03-10 05:30:29 | WARNING | fairseq.trainer | |===========================================================================|\n",
            "|                  PyTorch CUDA memory summary, device ID 0                 |\n",
            "|---------------------------------------------------------------------------|\n",
            "|            CUDA OOMs: 34           |        cudaMalloc retries: 69        |\n",
            "|===========================================================================|\n",
            "|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Allocated memory      |    6700 MB |    8176 MB |    4981 GB |    4974 GB |\n",
            "|       from large pool |    6651 MB |    8127 MB |    4806 GB |    4800 GB |\n",
            "|       from small pool |      48 MB |      60 MB |     174 GB |     174 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Active memory         |    6700 MB |    8176 MB |    4981 GB |    4974 GB |\n",
            "|       from large pool |    6651 MB |    8127 MB |    4806 GB |    4800 GB |\n",
            "|       from small pool |      48 MB |      60 MB |     174 GB |     174 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| GPU reserved memory   |   10580 MB |   10668 MB |  148462 MB |  137882 MB |\n",
            "|       from large pool |   10522 MB |   10536 MB |  145378 MB |  134856 MB |\n",
            "|       from small pool |      58 MB |     132 MB |    3084 MB |    3026 MB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Non-releasable memory |    2403 MB |    2403 MB |    5077 GB |    5074 GB |\n",
            "|       from large pool |    2394 MB |    2394 MB |    4886 GB |    4884 GB |\n",
            "|       from small pool |       9 MB |      17 MB |     190 GB |     190 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Allocations           |     592    |     595    |    2072 K  |    2072 K  |\n",
            "|       from large pool |     160    |     164    |     721 K  |     720 K  |\n",
            "|       from small pool |     432    |     516    |    1351 K  |    1351 K  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Active allocs         |     592    |     595    |    2072 K  |    2072 K  |\n",
            "|       from large pool |     160    |     164    |     721 K  |     720 K  |\n",
            "|       from small pool |     432    |     516    |    1351 K  |    1351 K  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| GPU reserved segments |      60    |      98    |    1780    |    1720    |\n",
            "|       from large pool |      31    |      32    |     238    |     207    |\n",
            "|       from small pool |      29    |      66    |    1542    |    1513    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Non-releasable allocs |      42    |      46    |     940 K  |     940 K  |\n",
            "|       from large pool |      17    |      17    |     461 K  |     461 K  |\n",
            "|       from small pool |      25    |      41    |     479 K  |     479 K  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Oversize allocations  |       0    |       0    |       0    |       0    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Oversize GPU segments |       0    |       0    |       0    |       0    |\n",
            "|===========================================================================|\n",
            "\n",
            "2022-03-10 05:30:29 | WARNING | fairseq.trainer | attempting to recover from OOM in forward/backward pass\n",
            "epoch 003:  62% 438/712 [01:28<00:57,  4.75it/s, loss=3.672, ppl=12.75, wps=12885, ups=4.86, wpb=2650.8, bsz=128, num_updates=1800, lr=0.0005, gnorm=0.877, train_wall=19, gb_free=10.3, wall=382]2022-03-10 05:30:30 | WARNING | fairseq.trainer | OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 1.70 GiB (GPU 0; 11.17 GiB total capacity; 9.73 GiB already allocated; 57.81 MiB free; 10.60 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
            "2022-03-10 05:30:30 | WARNING | fairseq.trainer | |===========================================================================|\n",
            "|                  PyTorch CUDA memory summary, device ID 0                 |\n",
            "|---------------------------------------------------------------------------|\n",
            "|            CUDA OOMs: 35           |        cudaMalloc retries: 70        |\n",
            "|===========================================================================|\n",
            "|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Allocated memory      |    8219 MB |    9962 MB |    4996 GB |    4988 GB |\n",
            "|       from large pool |    8170 MB |    9913 MB |    4821 GB |    4813 GB |\n",
            "|       from small pool |      49 MB |      60 MB |     174 GB |     174 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Active memory         |    8219 MB |    9962 MB |    4996 GB |    4988 GB |\n",
            "|       from large pool |    8170 MB |    9913 MB |    4821 GB |    4813 GB |\n",
            "|       from small pool |      49 MB |      60 MB |     174 GB |     174 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| GPU reserved memory   |   10850 MB |   10868 MB |  150226 MB |  139376 MB |\n",
            "|       from large pool |   10790 MB |   10790 MB |  147122 MB |  136332 MB |\n",
            "|       from small pool |      60 MB |      78 MB |    3104 MB |    3044 MB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Non-releasable memory |     886 MB |    1521 MB |    5088 GB |    5088 GB |\n",
            "|       from large pool |     875 MB |    1510 MB |    4898 GB |    4897 GB |\n",
            "|       from small pool |      10 MB |      21 MB |     190 GB |     190 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Allocations           |     592    |     595    |    2075 K  |    2074 K  |\n",
            "|       from large pool |     160    |     164    |     722 K  |     722 K  |\n",
            "|       from small pool |     432    |     516    |    1353 K  |    1352 K  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Active allocs         |     592    |     595    |    2075 K  |    2074 K  |\n",
            "|       from large pool |     160    |     164    |     722 K  |     722 K  |\n",
            "|       from small pool |     432    |     516    |    1353 K  |    1352 K  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| GPU reserved segments |      61    |      70    |    1791    |    1730    |\n",
            "|       from large pool |      31    |      31    |     239    |     208    |\n",
            "|       from small pool |      30    |      39    |    1552    |    1522    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Non-releasable allocs |      45    |      54    |     941 K  |     941 K  |\n",
            "|       from large pool |      20    |      21    |     462 K  |     462 K  |\n",
            "|       from small pool |      25    |      46    |     479 K  |     479 K  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Oversize allocations  |       0    |       0    |       0    |       0    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Oversize GPU segments |       0    |       0    |       0    |       0    |\n",
            "|===========================================================================|\n",
            "\n",
            "2022-03-10 05:30:30 | WARNING | fairseq.trainer | attempting to recover from OOM in forward/backward pass\n",
            "epoch 003:  86% 609/712 [02:03<00:15,  6.48it/s, loss=3.748, ppl=13.44, wps=14169.4, ups=4.93, wpb=2873.1, bsz=128, num_updates=1900, lr=0.0005, gnorm=0.899, train_wall=19, gb_free=10.2, wall=403]2022-03-10 05:31:04 | WARNING | fairseq.trainer | OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 1.76 GiB (GPU 0; 11.17 GiB total capacity; 9.04 GiB already allocated; 549.81 MiB free; 10.12 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
            "2022-03-10 05:31:04 | WARNING | fairseq.trainer | |===========================================================================|\n",
            "|                  PyTorch CUDA memory summary, device ID 0                 |\n",
            "|---------------------------------------------------------------------------|\n",
            "|            CUDA OOMs: 36           |        cudaMalloc retries: 72        |\n",
            "|===========================================================================|\n",
            "|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Allocated memory      |    9259 MB |    9328 MB |    5459 GB |    5450 GB |\n",
            "|       from large pool |    9209 MB |    9279 MB |    5269 GB |    5260 GB |\n",
            "|       from small pool |      49 MB |      60 MB |     189 GB |     189 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Active memory         |    9259 MB |    9328 MB |    5459 GB |    5450 GB |\n",
            "|       from large pool |    9209 MB |    9279 MB |    5269 GB |    5260 GB |\n",
            "|       from small pool |      49 MB |      60 MB |     189 GB |     189 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| GPU reserved memory   |   10358 MB |   10426 MB |  152734 MB |  142376 MB |\n",
            "|       from large pool |   10300 MB |   10300 MB |  149494 MB |  139194 MB |\n",
            "|       from small pool |      58 MB |     126 MB |    3240 MB |    3182 MB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Non-releasable memory |    1098 MB |    1545 MB |    5594 GB |    5593 GB |\n",
            "|       from large pool |    1090 MB |    1536 MB |    5386 GB |    5385 GB |\n",
            "|       from small pool |       8 MB |      22 MB |     207 GB |     207 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Allocations           |     593    |     595    |    2267 K  |    2267 K  |\n",
            "|       from large pool |     160    |     164    |     790 K  |     790 K  |\n",
            "|       from small pool |     433    |     516    |    1477 K  |    1477 K  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Active allocs         |     593    |     595    |    2267 K  |    2267 K  |\n",
            "|       from large pool |     160    |     164    |     790 K  |     790 K  |\n",
            "|       from small pool |     433    |     516    |    1477 K  |    1477 K  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| GPU reserved segments |      60    |      94    |    1861    |    1801    |\n",
            "|       from large pool |      31    |      31    |     241    |     210    |\n",
            "|       from small pool |      29    |      63    |    1620    |    1591    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Non-releasable allocs |      51    |      57    |    1025 K  |    1025 K  |\n",
            "|       from large pool |      28    |      28    |     503 K  |     502 K  |\n",
            "|       from small pool |      23    |      52    |     522 K  |     522 K  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Oversize allocations  |       0    |       0    |       0    |       0    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Oversize GPU segments |       0    |       0    |       0    |       0    |\n",
            "|===========================================================================|\n",
            "\n",
            "2022-03-10 05:31:04 | WARNING | fairseq.trainer | attempting to recover from OOM in forward/backward pass\n",
            "epoch 003:  91% 649/712 [02:10<00:11,  5.70it/s, loss=3.699, ppl=12.99, wps=13377.4, ups=4.77, wpb=2806.9, bsz=128, num_updates=2000, lr=0.0005, gnorm=0.878, train_wall=20, gb_free=4.1, wall=424]2022-03-10 05:31:12 | WARNING | fairseq.trainer | OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 2.51 GiB (GPU 0; 11.17 GiB total capacity; 10.39 GiB already allocated; 137.81 MiB free; 10.52 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
            "2022-03-10 05:31:12 | WARNING | fairseq.trainer | |===========================================================================|\n",
            "|                  PyTorch CUDA memory summary, device ID 0                 |\n",
            "|---------------------------------------------------------------------------|\n",
            "|            CUDA OOMs: 37           |        cudaMalloc retries: 73        |\n",
            "|===========================================================================|\n",
            "|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Allocated memory      |   10598 MB |   10640 MB |    5565 GB |    5555 GB |\n",
            "|       from large pool |   10547 MB |   10589 MB |    5372 GB |    5361 GB |\n",
            "|       from small pool |      51 MB |      60 MB |     193 GB |     193 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Active memory         |   10598 MB |   10640 MB |    5565 GB |    5555 GB |\n",
            "|       from large pool |   10547 MB |   10589 MB |    5372 GB |    5361 GB |\n",
            "|       from small pool |      51 MB |      60 MB |     193 GB |     193 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| GPU reserved memory   |   10770 MB |   10840 MB |  153216 MB |  142446 MB |\n",
            "|       from large pool |   10712 MB |   10712 MB |  149906 MB |  139194 MB |\n",
            "|       from small pool |      58 MB |     128 MB |    3310 MB |    3252 MB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Non-releasable memory |  175916 KB |    2012 MB |    5710 GB |    5709 GB |\n",
            "|       from large pool |  168804 KB |    2004 MB |    5498 GB |    5498 GB |\n",
            "|       from small pool |    7112 KB |      21 MB |     211 GB |     211 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Allocations           |     593    |     595    |    2311 K  |    2311 K  |\n",
            "|       from large pool |     162    |     163    |     805 K  |     805 K  |\n",
            "|       from small pool |     431    |     516    |    1506 K  |    1505 K  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Active allocs         |     593    |     595    |    2311 K  |    2311 K  |\n",
            "|       from large pool |     162    |     163    |     805 K  |     805 K  |\n",
            "|       from small pool |     431    |     516    |    1506 K  |    1505 K  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| GPU reserved segments |      68    |     103    |    1904    |    1836    |\n",
            "|       from large pool |      39    |      39    |     249    |     210    |\n",
            "|       from small pool |      29    |      64    |    1655    |    1626    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Non-releasable allocs |      57    |      57    |    1045 K  |    1045 K  |\n",
            "|       from large pool |      32    |      32    |     512 K  |     512 K  |\n",
            "|       from small pool |      25    |      42    |     532 K  |     532 K  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Oversize allocations  |       0    |       0    |       0    |       0    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Oversize GPU segments |       0    |       0    |       0    |       0    |\n",
            "|===========================================================================|\n",
            "\n",
            "2022-03-10 05:31:12 | WARNING | fairseq.trainer | attempting to recover from OOM in forward/backward pass\n",
            "epoch 003: 100% 711/712 [02:24<00:00,  3.92it/s, loss=3.699, ppl=12.99, wps=13377.4, ups=4.77, wpb=2806.9, bsz=128, num_updates=2000, lr=0.0005, gnorm=0.878, train_wall=20, gb_free=4.1, wall=424]2022-03-10 05:31:25 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "\n",
            "epoch 003 | valid on 'valid' subset:   0% 0/8 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 003 | valid on 'valid' subset:  12% 1/8 [00:00<00:01,  5.85it/s]\u001b[A\n",
            "epoch 003 | valid on 'valid' subset:  38% 3/8 [00:00<00:00,  9.51it/s]\u001b[A\n",
            "epoch 003 | valid on 'valid' subset:  62% 5/8 [00:00<00:00, 10.52it/s]\u001b[A\n",
            "epoch 003 | valid on 'valid' subset:  88% 7/8 [00:00<00:00,  9.59it/s]\u001b[A\n",
            "epoch 003 | valid on 'valid' subset: 100% 8/8 [00:01<00:00,  5.37it/s]\u001b[A\n",
            "                                                                      \u001b[A2022-03-10 05:31:26 | INFO | valid | epoch 003 | valid on 'valid' subset | loss 3.914 | ppl 15.08 | wps 19858.1 | wpb 2800.1 | bsz 125 | num_updates 2099 | best_loss 3.914\n",
            "2022-03-10 05:31:26 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 3 @ 2099 updates\n",
            "2022-03-10 05:31:26 | INFO | fairseq.trainer | Saving checkpoint to /content/checkpoints/checkpoint3.pt\n",
            "2022-03-10 05:31:26 | INFO | fairseq.trainer | Finished saving checkpoint to /content/checkpoints/checkpoint3.pt\n",
            "2022-03-10 05:31:28 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/checkpoint3.pt (epoch 3 @ 2099 updates, score 3.914) (writing took 1.9263320889999704 seconds)\n",
            "2022-03-10 05:31:28 | INFO | fairseq_cli.train | end of epoch 3 (average epoch stats below)\n",
            "2022-03-10 05:31:28 | INFO | train | epoch 003 | loss 3.739 | ppl 13.36 | wps 13128.3 | ups 4.75 | wpb 2764.6 | bsz 127.7 | num_updates 2099 | lr 0.0005 | gnorm 0.909 | train_wall 134 | gb_free 10.4 | wall 446\n",
            "2022-03-10 05:31:28 | INFO | fairseq.data.iterators | grouped total_num_itrs = 712\n",
            "epoch 004:   0% 0/712 [00:00<?, ?it/s]2022-03-10 05:31:28 | INFO | fairseq.trainer | begin training epoch 4\n",
            "2022-03-10 05:31:28 | INFO | fairseq_cli.train | Start iterating over samples\n",
            "epoch 004:   3% 23/712 [00:03<01:30,  7.63it/s, loss=3.627, ppl=12.35, wps=11707.6, ups=4.34, wpb=2694.7, bsz=126.9, num_updates=2100, lr=0.0005, gnorm=0.921, train_wall=19, gb_free=8, wall=447]2022-03-10 05:31:32 | WARNING | fairseq.trainer | OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 2.51 GiB (GPU 0; 11.17 GiB total capacity; 10.39 GiB already allocated; 135.81 MiB free; 10.52 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
            "2022-03-10 05:31:32 | WARNING | fairseq.trainer | |===========================================================================|\n",
            "|                  PyTorch CUDA memory summary, device ID 0                 |\n",
            "|---------------------------------------------------------------------------|\n",
            "|            CUDA OOMs: 38           |        cudaMalloc retries: 74        |\n",
            "|===========================================================================|\n",
            "|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Allocated memory      |   10599 MB |   10641 MB |    5818 GB |    5808 GB |\n",
            "|       from large pool |   10548 MB |   10590 MB |    5616 GB |    5606 GB |\n",
            "|       from small pool |      51 MB |      60 MB |     201 GB |     201 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Active memory         |   10599 MB |   10641 MB |    5818 GB |    5808 GB |\n",
            "|       from large pool |   10548 MB |   10590 MB |    5616 GB |    5606 GB |\n",
            "|       from small pool |      51 MB |      60 MB |     201 GB |     201 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| GPU reserved memory   |   10772 MB |   10842 MB |  153288 MB |  142516 MB |\n",
            "|       from large pool |   10712 MB |   10712 MB |  149906 MB |  139194 MB |\n",
            "|       from small pool |      60 MB |     130 MB |    3382 MB |    3322 MB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Non-releasable memory |  176796 KB |    2240 MB |    5968 GB |    5968 GB |\n",
            "|       from large pool |  167636 KB |    2229 MB |    5748 GB |    5747 GB |\n",
            "|       from small pool |    9160 KB |      26 MB |     220 GB |     220 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Allocations           |     593    |     595    |    2410 K  |    2409 K  |\n",
            "|       from large pool |     162    |     163    |     840 K  |     839 K  |\n",
            "|       from small pool |     431    |     516    |    1569 K  |    1569 K  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Active allocs         |     593    |     595    |    2410 K  |    2409 K  |\n",
            "|       from large pool |     162    |     163    |     840 K  |     839 K  |\n",
            "|       from small pool |     431    |     516    |    1569 K  |    1569 K  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| GPU reserved segments |      69    |     104    |    1940    |    1871    |\n",
            "|       from large pool |      39    |      39    |     249    |     210    |\n",
            "|       from small pool |      30    |      65    |    1691    |    1661    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Non-releasable allocs |      61    |      61    |    1089 K  |    1089 K  |\n",
            "|       from large pool |      37    |      37    |     533 K  |     533 K  |\n",
            "|       from small pool |      24    |      47    |     555 K  |     555 K  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Oversize allocations  |       0    |       0    |       0    |       0    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Oversize GPU segments |       0    |       0    |       0    |       0    |\n",
            "|===========================================================================|\n",
            "\n",
            "2022-03-10 05:31:32 | WARNING | fairseq.trainer | attempting to recover from OOM in forward/backward pass\n",
            "epoch 004:   8% 57/712 [00:10<02:00,  5.42it/s, loss=3.627, ppl=12.35, wps=11707.6, ups=4.34, wpb=2694.7, bsz=126.9, num_updates=2100, lr=0.0005, gnorm=0.921, train_wall=19, gb_free=8, wall=447]2022-03-10 05:31:39 | WARNING | fairseq.trainer | OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 100.00 MiB (GPU 0; 11.17 GiB total capacity; 10.17 GiB already allocated; 83.81 MiB free; 10.57 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
            "2022-03-10 05:31:39 | WARNING | fairseq.trainer | |===========================================================================|\n",
            "|                  PyTorch CUDA memory summary, device ID 0                 |\n",
            "|---------------------------------------------------------------------------|\n",
            "|            CUDA OOMs: 39           |        cudaMalloc retries: 75        |\n",
            "|===========================================================================|\n",
            "|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Allocated memory      |   10415 MB |   10415 MB |    5905 GB |    5894 GB |\n",
            "|       from large pool |   10364 MB |   10364 MB |    5700 GB |    5690 GB |\n",
            "|       from small pool |      50 MB |      61 MB |     204 GB |     204 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Active memory         |   10415 MB |   10415 MB |    5905 GB |    5894 GB |\n",
            "|       from large pool |   10364 MB |   10364 MB |    5700 GB |    5690 GB |\n",
            "|       from small pool |      50 MB |      61 MB |     204 GB |     204 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| GPU reserved memory   |   10824 MB |   10870 MB |  153386 MB |  142562 MB |\n",
            "|       from large pool |   10762 MB |   10762 MB |  149956 MB |  139194 MB |\n",
            "|       from small pool |      62 MB |     108 MB |    3430 MB |    3368 MB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Non-releasable memory |  418081 KB |    2533 MB |    6060 GB |    6060 GB |\n",
            "|       from large pool |  406580 KB |    2521 MB |    5837 GB |    5836 GB |\n",
            "|       from small pool |   11501 KB |      16 MB |     223 GB |     223 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Allocations           |     586    |     586    |    2447 K  |    2446 K  |\n",
            "|       from large pool |     156    |     156    |     854 K  |     854 K  |\n",
            "|       from small pool |     430    |     516    |    1593 K  |    1592 K  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Active allocs         |     586    |     586    |    2447 K  |    2446 K  |\n",
            "|       from large pool |     156    |     156    |     854 K  |     854 K  |\n",
            "|       from small pool |     430    |     516    |    1593 K  |    1592 K  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| GPU reserved segments |      71    |      94    |    1965    |    1894    |\n",
            "|       from large pool |      40    |      40    |     250    |     210    |\n",
            "|       from small pool |      31    |      54    |    1715    |    1684    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Non-releasable allocs |      58    |      60    |    1105 K  |    1105 K  |\n",
            "|       from large pool |      33    |      35    |     542 K  |     542 K  |\n",
            "|       from small pool |      25    |      28    |     563 K  |     563 K  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Oversize allocations  |       0    |       0    |       0    |       0    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Oversize GPU segments |       0    |       0    |       0    |       0    |\n",
            "|===========================================================================|\n",
            "\n",
            "2022-03-10 05:31:39 | WARNING | fairseq.trainer | attempting to recover from OOM in forward/backward pass\n",
            "epoch 004:  17% 122/712 [00:23<01:50,  5.35it/s, loss=3.217, ppl=9.3, wps=13132.1, ups=5.28, wpb=2489.4, bsz=128, num_updates=2200, lr=0.0005, gnorm=0.864, train_wall=17, gb_free=10.1, wall=466]2022-03-10 05:31:52 | WARNING | fairseq.trainer | OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 1.52 GiB (GPU 0; 11.17 GiB total capacity; 8.51 GiB already allocated; 1.33 GiB free; 9.32 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
            "2022-03-10 05:31:52 | WARNING | fairseq.trainer | |===========================================================================|\n",
            "|                  PyTorch CUDA memory summary, device ID 0                 |\n",
            "|---------------------------------------------------------------------------|\n",
            "|            CUDA OOMs: 40           |        cudaMalloc retries: 77        |\n",
            "|===========================================================================|\n",
            "|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Allocated memory      |    7166 MB |    8718 MB |    6083 GB |    6076 GB |\n",
            "|       from large pool |    7118 MB |    8670 MB |    5872 GB |    5865 GB |\n",
            "|       from small pool |      48 MB |      60 MB |     210 GB |     210 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Active memory         |    7166 MB |    8718 MB |    6083 GB |    6076 GB |\n",
            "|       from large pool |    7118 MB |    8670 MB |    5872 GB |    5865 GB |\n",
            "|       from small pool |      48 MB |      60 MB |     210 GB |     210 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| GPU reserved memory   |    9546 MB |   10900 MB |  155014 MB |  145468 MB |\n",
            "|       from large pool |    9488 MB |   10762 MB |  151508 MB |  142020 MB |\n",
            "|       from small pool |      58 MB |     138 MB |    3506 MB |    3448 MB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Non-releasable memory |     827 MB |    1186 MB |    6240 GB |    6239 GB |\n",
            "|       from large pool |     817 MB |    1176 MB |    6009 GB |    6008 GB |\n",
            "|       from small pool |       9 MB |      19 MB |     230 GB |     230 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Allocations           |     592    |     595    |    2519 K  |    2519 K  |\n",
            "|       from large pool |     160    |     164    |     878 K  |     878 K  |\n",
            "|       from small pool |     432    |     516    |    1641 K  |    1640 K  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Active allocs         |     592    |     595    |    2519 K  |    2519 K  |\n",
            "|       from large pool |     160    |     164    |     878 K  |     878 K  |\n",
            "|       from small pool |     432    |     516    |    1641 K  |    1640 K  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| GPU reserved segments |      67    |     109    |    2004    |    1937    |\n",
            "|       from large pool |      38    |      40    |     251    |     213    |\n",
            "|       from small pool |      29    |      69    |    1753    |    1724    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Non-releasable allocs |      45    |      48    |    1138 K  |    1138 K  |\n",
            "|       from large pool |      20    |      23    |     557 K  |     557 K  |\n",
            "|       from small pool |      25    |      41    |     580 K  |     580 K  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Oversize allocations  |       0    |       0    |       0    |       0    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Oversize GPU segments |       0    |       0    |       0    |       0    |\n",
            "|===========================================================================|\n",
            "\n",
            "2022-03-10 05:31:52 | WARNING | fairseq.trainer | attempting to recover from OOM in forward/backward pass\n",
            "epoch 004:  20% 141/712 [00:28<01:31,  6.24it/s, loss=3.217, ppl=9.3, wps=13132.1, ups=5.28, wpb=2489.4, bsz=128, num_updates=2200, lr=0.0005, gnorm=0.864, train_wall=17, gb_free=10.1, wall=466]2022-03-10 05:31:57 | WARNING | fairseq.trainer | OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 1.54 GiB (GPU 0; 11.17 GiB total capacity; 8.23 GiB already allocated; 1.17 GiB free; 9.48 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
            "2022-03-10 05:31:57 | WARNING | fairseq.trainer | |===========================================================================|\n",
            "|                  PyTorch CUDA memory summary, device ID 0                 |\n",
            "|---------------------------------------------------------------------------|\n",
            "|            CUDA OOMs: 41           |        cudaMalloc retries: 80        |\n",
            "|===========================================================================|\n",
            "|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Allocated memory      |    6853 MB |    8427 MB |    6149 GB |    6143 GB |\n",
            "|       from large pool |    6795 MB |    8369 MB |    5937 GB |    5930 GB |\n",
            "|       from small pool |      58 MB |      60 MB |     212 GB |     212 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Active memory         |    6853 MB |    8427 MB |    6149 GB |    6143 GB |\n",
            "|       from large pool |    6795 MB |    8369 MB |    5937 GB |    5930 GB |\n",
            "|       from small pool |      58 MB |      60 MB |     212 GB |     212 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| GPU reserved memory   |    9710 MB |    9710 MB |  161306 MB |  151596 MB |\n",
            "|       from large pool |    9648 MB |    9648 MB |  157704 MB |  148056 MB |\n",
            "|       from small pool |      62 MB |     106 MB |    3602 MB |    3540 MB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Non-releasable memory |    1280 MB |    1377 MB |    6300 GB |    6299 GB |\n",
            "|       from large pool |    1276 MB |    1373 MB |    6068 GB |    6067 GB |\n",
            "|       from small pool |       3 MB |      15 MB |     231 GB |     231 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Allocations           |     592    |     595    |    2540 K  |    2540 K  |\n",
            "|       from large pool |     150    |     154    |     886 K  |     886 K  |\n",
            "|       from small pool |     442    |     516    |    1654 K  |    1653 K  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Active allocs         |     592    |     595    |    2540 K  |    2540 K  |\n",
            "|       from large pool |     150    |     154    |     886 K  |     886 K  |\n",
            "|       from small pool |     442    |     516    |    1654 K  |    1653 K  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| GPU reserved segments |      69    |      91    |    2057    |    1988    |\n",
            "|       from large pool |      38    |      38    |     256    |     218    |\n",
            "|       from small pool |      31    |      53    |    1801    |    1770    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Non-releasable allocs |      52    |      53    |    1147 K  |    1147 K  |\n",
            "|       from large pool |      28    |      29    |     562 K  |     562 K  |\n",
            "|       from small pool |      24    |      30    |     585 K  |     585 K  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Oversize allocations  |       0    |       0    |       0    |       0    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Oversize GPU segments |       0    |       0    |       0    |       0    |\n",
            "|===========================================================================|\n",
            "\n",
            "2022-03-10 05:31:57 | WARNING | fairseq.trainer | attempting to recover from OOM in forward/backward pass\n",
            "epoch 004:  39% 275/712 [00:55<01:10,  6.17it/s, loss=3.423, ppl=10.72, wps=13661.2, ups=4.41, wpb=3094.9, bsz=128, num_updates=2300, lr=0.0005, gnorm=0.856, train_wall=21, gb_free=8, wall=488]2022-03-10 05:32:24 | WARNING | fairseq.trainer | OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 1.44 GiB (GPU 0; 11.17 GiB total capacity; 7.98 GiB already allocated; 1.24 GiB free; 9.41 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
            "2022-03-10 05:32:24 | WARNING | fairseq.trainer | |===========================================================================|\n",
            "|                  PyTorch CUDA memory summary, device ID 0                 |\n",
            "|---------------------------------------------------------------------------|\n",
            "|            CUDA OOMs: 42           |        cudaMalloc retries: 83        |\n",
            "|===========================================================================|\n",
            "|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Allocated memory      |    6698 MB |    8174 MB |    6514 GB |    6507 GB |\n",
            "|       from large pool |    6649 MB |    8125 MB |    6290 GB |    6283 GB |\n",
            "|       from small pool |      48 MB |      61 MB |     224 GB |     224 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Active memory         |    6698 MB |    8174 MB |    6514 GB |    6507 GB |\n",
            "|       from large pool |    6649 MB |    8125 MB |    6290 GB |    6283 GB |\n",
            "|       from small pool |      48 MB |      61 MB |     224 GB |     224 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| GPU reserved memory   |    9640 MB |    9640 MB |  166376 MB |  156736 MB |\n",
            "|       from large pool |    9580 MB |    9580 MB |  162654 MB |  153074 MB |\n",
            "|       from small pool |      60 MB |     132 MB |    3722 MB |    3662 MB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Non-releasable memory |    1465 MB |    1465 MB |    6690 GB |    6689 GB |\n",
            "|       from large pool |    1454 MB |    1454 MB |    6445 GB |    6444 GB |\n",
            "|       from small pool |      11 MB |      22 MB |     245 GB |     245 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Allocations           |     592    |     595    |    2691 K  |    2690 K  |\n",
            "|       from large pool |     160    |     164    |     940 K  |     939 K  |\n",
            "|       from small pool |     432    |     516    |    1751 K  |    1750 K  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Active allocs         |     592    |     595    |    2691 K  |    2690 K  |\n",
            "|       from large pool |     160    |     164    |     940 K  |     939 K  |\n",
            "|       from small pool |     432    |     516    |    1751 K  |    1750 K  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| GPU reserved segments |      68    |     104    |    2121    |    2053    |\n",
            "|       from large pool |      38    |      38    |     260    |     222    |\n",
            "|       from small pool |      30    |      66    |    1861    |    1831    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Non-releasable allocs |      50    |      50    |    1214 K  |    1214 K  |\n",
            "|       from large pool |      25    |      25    |     594 K  |     594 K  |\n",
            "|       from small pool |      25    |      42    |     619 K  |     619 K  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Oversize allocations  |       0    |       0    |       0    |       0    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Oversize GPU segments |       0    |       0    |       0    |       0    |\n",
            "|===========================================================================|\n",
            "\n",
            "2022-03-10 05:32:24 | WARNING | fairseq.trainer | attempting to recover from OOM in forward/backward pass\n",
            "epoch 004:  39% 276/712 [00:56<02:07,  3.41it/s, loss=3.423, ppl=10.72, wps=13661.2, ups=4.41, wpb=3094.9, bsz=128, num_updates=2300, lr=0.0005, gnorm=0.856, train_wall=21, gb_free=8, wall=488]2022-03-10 05:32:25 | WARNING | fairseq.trainer | OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 100.00 MiB (GPU 0; 11.17 GiB total capacity; 10.18 GiB already allocated; 33.81 MiB free; 10.62 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
            "2022-03-10 05:32:25 | WARNING | fairseq.trainer | |===========================================================================|\n",
            "|                  PyTorch CUDA memory summary, device ID 0                 |\n",
            "|---------------------------------------------------------------------------|\n",
            "|            CUDA OOMs: 43           |        cudaMalloc retries: 84        |\n",
            "|===========================================================================|\n",
            "|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Allocated memory      |   10421 MB |   10421 MB |    6528 GB |    6518 GB |\n",
            "|       from large pool |   10371 MB |   10371 MB |    6303 GB |    6293 GB |\n",
            "|       from small pool |      50 MB |      61 MB |     224 GB |     224 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Active memory         |   10421 MB |   10421 MB |    6528 GB |    6518 GB |\n",
            "|       from large pool |   10371 MB |   10371 MB |    6303 GB |    6293 GB |\n",
            "|       from small pool |      50 MB |      61 MB |     224 GB |     224 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| GPU reserved memory   |   10874 MB |   10874 MB |  169086 MB |  158212 MB |\n",
            "|       from large pool |   10814 MB |   10814 MB |  165364 MB |  154550 MB |\n",
            "|       from small pool |      60 MB |     132 MB |    3722 MB |    3662 MB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Non-releasable memory |  462908 KB |    2014 MB |    6701 GB |    6700 GB |\n",
            "|       from large pool |  453423 KB |    2003 MB |    6456 GB |    6455 GB |\n",
            "|       from small pool |    9485 KB |      22 MB |     245 GB |     245 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Allocations           |     587    |     595    |    2691 K  |    2691 K  |\n",
            "|       from large pool |     157    |     164    |     940 K  |     940 K  |\n",
            "|       from small pool |     430    |     516    |    1751 K  |    1750 K  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Active allocs         |     587    |     595    |    2691 K  |    2691 K  |\n",
            "|       from large pool |     157    |     164    |     940 K  |     940 K  |\n",
            "|       from small pool |     430    |     516    |    1751 K  |    1750 K  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| GPU reserved segments |      73    |     104    |    2127    |    2054    |\n",
            "|       from large pool |      43    |      43    |     266    |     223    |\n",
            "|       from small pool |      30    |      66    |    1861    |    1831    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Non-releasable allocs |      56    |      57    |    1214 K  |    1214 K  |\n",
            "|       from large pool |      31    |      32    |     595 K  |     595 K  |\n",
            "|       from small pool |      25    |      42    |     619 K  |     619 K  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Oversize allocations  |       0    |       0    |       0    |       0    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Oversize GPU segments |       0    |       0    |       0    |       0    |\n",
            "|===========================================================================|\n",
            "\n",
            "2022-03-10 05:32:25 | WARNING | fairseq.trainer | attempting to recover from OOM in forward/backward pass\n",
            "epoch 004:  49% 347/712 [01:13<01:10,  5.18it/s, loss=3.402, ppl=10.57, wps=13188.3, ups=4.37, wpb=3014.6, bsz=128, num_updates=2400, lr=0.0005, gnorm=0.907, train_wall=22, gb_free=8.5, wall=511]2022-03-10 05:32:42 | WARNING | fairseq.trainer | OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 1.76 GiB (GPU 0; 11.17 GiB total capacity; 9.05 GiB already allocated; 1.26 GiB free; 9.39 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
            "2022-03-10 05:32:42 | WARNING | fairseq.trainer | |===========================================================================|\n",
            "|                  PyTorch CUDA memory summary, device ID 0                 |\n",
            "|---------------------------------------------------------------------------|\n",
            "|            CUDA OOMs: 44           |        cudaMalloc retries: 88        |\n",
            "|===========================================================================|\n",
            "|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Allocated memory      |    9265 MB |    9334 MB |    6744 GB |    6735 GB |\n",
            "|       from large pool |    9215 MB |    9285 MB |    6514 GB |    6505 GB |\n",
            "|       from small pool |      49 MB |      61 MB |     230 GB |     230 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Active memory         |    9265 MB |    9334 MB |    6744 GB |    6735 GB |\n",
            "|       from large pool |    9215 MB |    9285 MB |    6514 GB |    6505 GB |\n",
            "|       from small pool |      49 MB |      61 MB |     230 GB |     230 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| GPU reserved memory   |    9614 MB |   10488 MB |  185466 MB |  175852 MB |\n",
            "|       from large pool |    9556 MB |   10350 MB |  181602 MB |  172046 MB |\n",
            "|       from small pool |      58 MB |     138 MB |    3864 MB |    3806 MB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Non-releasable memory |  357188 KB |    1442 MB |    6907 GB |    6907 GB |\n",
            "|       from large pool |  348376 KB |    1433 MB |    6655 GB |    6655 GB |\n",
            "|       from small pool |    8812 KB |      12 MB |     251 GB |     251 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Allocations           |     593    |     595    |    2771 K  |    2770 K  |\n",
            "|       from large pool |     160    |     164    |     969 K  |     969 K  |\n",
            "|       from small pool |     433    |     516    |    1802 K  |    1801 K  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Active allocs         |     593    |     595    |    2771 K  |    2770 K  |\n",
            "|       from large pool |     160    |     164    |     969 K  |     969 K  |\n",
            "|       from small pool |     433    |     516    |    1802 K  |    1801 K  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| GPU reserved segments |      75    |     116    |    2249    |    2174    |\n",
            "|       from large pool |      46    |      47    |     317    |     271    |\n",
            "|       from small pool |      29    |      69    |    1932    |    1903    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Non-releasable allocs |      57    |      58    |    1252 K  |    1252 K  |\n",
            "|       from large pool |      35    |      35    |     614 K  |     614 K  |\n",
            "|       from small pool |      22    |      25    |     637 K  |     637 K  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Oversize allocations  |       0    |       0    |       0    |       0    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Oversize GPU segments |       0    |       0    |       0    |       0    |\n",
            "|===========================================================================|\n",
            "\n",
            "2022-03-10 05:32:42 | WARNING | fairseq.trainer | attempting to recover from OOM in forward/backward pass\n",
            "epoch 004:  52% 369/712 [01:18<01:03,  5.37it/s, loss=3.402, ppl=10.57, wps=13188.3, ups=4.37, wpb=3014.6, bsz=128, num_updates=2400, lr=0.0005, gnorm=0.907, train_wall=22, gb_free=8.5, wall=511]2022-03-10 05:32:47 | WARNING | fairseq.trainer | OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 1.45 GiB (GPU 0; 11.17 GiB total capacity; 7.96 GiB already allocated; 1.09 GiB free; 9.56 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
            "2022-03-10 05:32:47 | WARNING | fairseq.trainer | |===========================================================================|\n",
            "|                  PyTorch CUDA memory summary, device ID 0                 |\n",
            "|---------------------------------------------------------------------------|\n",
            "|            CUDA OOMs: 45           |        cudaMalloc retries: 91        |\n",
            "|===========================================================================|\n",
            "|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Allocated memory      |    6669 MB |    8153 MB |    6809 GB |    6802 GB |\n",
            "|       from large pool |    6620 MB |    8104 MB |    6576 GB |    6569 GB |\n",
            "|       from small pool |      48 MB |      60 MB |     232 GB |     232 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Active memory         |    6669 MB |    8153 MB |    6809 GB |    6802 GB |\n",
            "|       from large pool |    6620 MB |    8104 MB |    6576 GB |    6569 GB |\n",
            "|       from small pool |      48 MB |      60 MB |     232 GB |     232 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| GPU reserved memory   |    9788 MB |    9788 MB |  189718 MB |  179930 MB |\n",
            "|       from large pool |    9728 MB |    9728 MB |  185734 MB |  176006 MB |\n",
            "|       from small pool |      60 MB |     128 MB |    3984 MB |    3924 MB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Non-releasable memory |    1634 MB |    1634 MB |    6971 GB |    6969 GB |\n",
            "|       from large pool |    1623 MB |    1623 MB |    6717 GB |    6715 GB |\n",
            "|       from small pool |      11 MB |      35 MB |     254 GB |     254 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Allocations           |     592    |     595    |    2795 K  |    2794 K  |\n",
            "|       from large pool |     160    |     164    |     977 K  |     977 K  |\n",
            "|       from small pool |     432    |     516    |    1817 K  |    1817 K  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Active allocs         |     592    |     595    |    2795 K  |    2794 K  |\n",
            "|       from large pool |     160    |     164    |     977 K  |     977 K  |\n",
            "|       from small pool |     432    |     516    |    1817 K  |    1817 K  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| GPU reserved segments |      74    |     108    |    2312    |    2238    |\n",
            "|       from large pool |      44    |      44    |     320    |     276    |\n",
            "|       from small pool |      30    |      64    |    1992    |    1962    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Non-releasable allocs |      47    |      58    |    1263 K  |    1263 K  |\n",
            "|       from large pool |      24    |      24    |     619 K  |     619 K  |\n",
            "|       from small pool |      23    |      52    |     643 K  |     643 K  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Oversize allocations  |       0    |       0    |       0    |       0    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Oversize GPU segments |       0    |       0    |       0    |       0    |\n",
            "|===========================================================================|\n",
            "\n",
            "2022-03-10 05:32:47 | WARNING | fairseq.trainer | attempting to recover from OOM in forward/backward pass\n",
            "epoch 004:  58% 410/712 [01:26<00:48,  6.29it/s, loss=3.32, ppl=9.99, wps=12971.1, ups=4.63, wpb=2804.3, bsz=126.9, num_updates=2500, lr=0.0005, gnorm=0.935, train_wall=20, gb_free=10.8, wall=533]2022-03-10 05:32:55 | WARNING | fairseq.trainer | OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 1.49 GiB (GPU 0; 11.17 GiB total capacity; 8.10 GiB already allocated; 1.07 GiB free; 9.58 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
            "epoch 004:  58% 411/712 [01:27<01:15,  3.99it/s, loss=3.32, ppl=9.99, wps=12971.1, ups=4.63, wpb=2804.3, bsz=126.9, num_updates=2500, lr=0.0005, gnorm=0.935, train_wall=20, gb_free=10.8, wall=533]2022-03-10 05:32:55 | WARNING | fairseq.trainer | |===========================================================================|\n",
            "|                  PyTorch CUDA memory summary, device ID 0                 |\n",
            "|---------------------------------------------------------------------------|\n",
            "|            CUDA OOMs: 46           |        cudaMalloc retries: 92        |\n",
            "|===========================================================================|\n",
            "|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Allocated memory      |    6769 MB |    8298 MB |    6908 GB |    6901 GB |\n",
            "|       from large pool |    6721 MB |    8250 MB |    6671 GB |    6664 GB |\n",
            "|       from small pool |      48 MB |      60 MB |     237 GB |     237 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Active memory         |    6769 MB |    8298 MB |    6908 GB |    6901 GB |\n",
            "|       from large pool |    6721 MB |    8250 MB |    6671 GB |    6664 GB |\n",
            "|       from small pool |      48 MB |      60 MB |     237 GB |     237 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| GPU reserved memory   |    9812 MB |    9908 MB |  191322 MB |  181510 MB |\n",
            "|       from large pool |    9750 MB |    9774 MB |  187264 MB |  177514 MB |\n",
            "|       from small pool |      62 MB |     134 MB |    4058 MB |    3996 MB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Non-releasable memory |    1512 MB |    1513 MB |    7075 GB |    7074 GB |\n",
            "|       from large pool |    1498 MB |    1499 MB |    6816 GB |    6815 GB |\n",
            "|       from small pool |      13 MB |      21 MB |     258 GB |     258 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Allocations           |     592    |     595    |    2840 K  |    2839 K  |\n",
            "|       from large pool |     160    |     164    |     992 K  |     992 K  |\n",
            "|       from small pool |     432    |     515    |    1848 K  |    1847 K  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Active allocs         |     592    |     595    |    2840 K  |    2839 K  |\n",
            "|       from large pool |     160    |     164    |     992 K  |     992 K  |\n",
            "|       from small pool |     432    |     515    |    1848 K  |    1847 K  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| GPU reserved segments |      74    |     111    |    2350    |    2276    |\n",
            "|       from large pool |      43    |      44    |     321    |     278    |\n",
            "|       from small pool |      31    |      67    |    2029    |    1998    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Non-releasable allocs |      53    |      54    |    1284 K  |    1284 K  |\n",
            "|       from large pool |      28    |      29    |     629 K  |     629 K  |\n",
            "|       from small pool |      25    |      44    |     654 K  |     654 K  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Oversize allocations  |       0    |       0    |       0    |       0    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Oversize GPU segments |       0    |       0    |       0    |       0    |\n",
            "|===========================================================================|\n",
            "\n",
            "2022-03-10 05:32:55 | WARNING | fairseq.trainer | attempting to recover from OOM in forward/backward pass\n",
            "epoch 004:  60% 428/712 [01:29<00:46,  6.07it/s, loss=3.32, ppl=9.99, wps=12971.1, ups=4.63, wpb=2804.3, bsz=126.9, num_updates=2500, lr=0.0005, gnorm=0.935, train_wall=20, gb_free=10.8, wall=533]2022-03-10 05:32:58 | WARNING | fairseq.trainer | OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 1.36 GiB (GPU 0; 11.17 GiB total capacity; 7.33 GiB already allocated; 1.21 GiB free; 9.44 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
            "2022-03-10 05:32:58 | WARNING | fairseq.trainer | |===========================================================================|\n",
            "|                  PyTorch CUDA memory summary, device ID 0                 |\n",
            "|---------------------------------------------------------------------------|\n",
            "|            CUDA OOMs: 47           |        cudaMalloc retries: 93        |\n",
            "|===========================================================================|\n",
            "|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Allocated memory      |    6115 MB |    7507 MB |    6950 GB |    6944 GB |\n",
            "|       from large pool |    6067 MB |    7459 MB |    6711 GB |    6705 GB |\n",
            "|       from small pool |      48 MB |      61 MB |     238 GB |     238 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Active memory         |    6115 MB |    7507 MB |    6950 GB |    6944 GB |\n",
            "|       from large pool |    6067 MB |    7459 MB |    6711 GB |    6705 GB |\n",
            "|       from small pool |      48 MB |      61 MB |     238 GB |     238 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| GPU reserved memory   |    9670 MB |    9708 MB |  192748 MB |  183078 MB |\n",
            "|       from large pool |    9612 MB |    9612 MB |  188656 MB |  179044 MB |\n",
            "|       from small pool |      58 MB |      96 MB |    4092 MB |    4034 MB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Non-releasable memory |    2162 MB |    2162 MB |    7115 GB |    7113 GB |\n",
            "|       from large pool |    2152 MB |    2152 MB |    6855 GB |    6853 GB |\n",
            "|       from small pool |       9 MB |      15 MB |     260 GB |     260 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Allocations           |     592    |     595    |    2859 K  |    2859 K  |\n",
            "|       from large pool |     160    |     164    |     999 K  |     999 K  |\n",
            "|       from small pool |     432    |     516    |    1860 K  |    1860 K  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Active allocs         |     592    |     595    |    2859 K  |    2859 K  |\n",
            "|       from large pool |     160    |     164    |     999 K  |     999 K  |\n",
            "|       from small pool |     432    |     516    |    1860 K  |    1860 K  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| GPU reserved segments |      72    |      91    |    2368    |    2296    |\n",
            "|       from large pool |      43    |      43    |     322    |     279    |\n",
            "|       from small pool |      29    |      48    |    2046    |    2017    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Non-releasable allocs |      48    |      48    |    1293 K  |    1293 K  |\n",
            "|       from large pool |      25    |      25    |     633 K  |     633 K  |\n",
            "|       from small pool |      23    |      30    |     659 K  |     659 K  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Oversize allocations  |       0    |       0    |       0    |       0    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Oversize GPU segments |       0    |       0    |       0    |       0    |\n",
            "|===========================================================================|\n",
            "\n",
            "2022-03-10 05:32:58 | WARNING | fairseq.trainer | attempting to recover from OOM in forward/backward pass\n",
            "epoch 004:  72% 515/712 [01:46<00:31,  6.29it/s, loss=3.318, ppl=9.97, wps=13897, ups=5.29, wpb=2627, bsz=128, num_updates=2600, lr=0.0005, gnorm=0.991, train_wall=17, gb_free=10.6, wall=552]2022-03-10 05:33:14 | WARNING | fairseq.trainer | OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 1.44 GiB (GPU 0; 11.17 GiB total capacity; 7.70 GiB already allocated; 1.31 GiB free; 9.34 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
            "2022-03-10 05:33:14 | WARNING | fairseq.trainer | |===========================================================================|\n",
            "|                  PyTorch CUDA memory summary, device ID 0                 |\n",
            "|---------------------------------------------------------------------------|\n",
            "|            CUDA OOMs: 48           |        cudaMalloc retries: 95        |\n",
            "|===========================================================================|\n",
            "|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Allocated memory      |    6404 MB |    7880 MB |    7159 GB |    7153 GB |\n",
            "|       from large pool |    6356 MB |    7832 MB |    6912 GB |    6906 GB |\n",
            "|       from small pool |      48 MB |      61 MB |     246 GB |     246 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Active memory         |    6404 MB |    7880 MB |    7159 GB |    7153 GB |\n",
            "|       from large pool |    6356 MB |    7832 MB |    6912 GB |    6906 GB |\n",
            "|       from small pool |      48 MB |      61 MB |     246 GB |     246 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| GPU reserved memory   |    9562 MB |    9562 MB |  195430 MB |  185868 MB |\n",
            "|       from large pool |    9504 MB |    9504 MB |  191264 MB |  181760 MB |\n",
            "|       from small pool |      58 MB |     132 MB |    4166 MB |    4108 MB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Non-releasable memory |    1681 MB |    1681 MB |    7325 GB |    7324 GB |\n",
            "|       from large pool |    1671 MB |    1671 MB |    7056 GB |    7054 GB |\n",
            "|       from small pool |       9 MB |      16 MB |     269 GB |     269 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Allocations           |     592    |     595    |    2957 K  |    2956 K  |\n",
            "|       from large pool |     160    |     164    |    1032 K  |    1032 K  |\n",
            "|       from small pool |     432    |     516    |    1924 K  |    1924 K  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Active allocs         |     592    |     595    |    2957 K  |    2956 K  |\n",
            "|       from large pool |     160    |     164    |    1032 K  |    1032 K  |\n",
            "|       from small pool |     432    |     516    |    1924 K  |    1924 K  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| GPU reserved segments |      72    |     109    |    2407    |    2335    |\n",
            "|       from large pool |      43    |      43    |     324    |     281    |\n",
            "|       from small pool |      29    |      66    |    2083    |    2054    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Non-releasable allocs |      51    |      51    |    1337 K  |    1337 K  |\n",
            "|       from large pool |      30    |      30    |     655 K  |     655 K  |\n",
            "|       from small pool |      21    |      37    |     681 K  |     681 K  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Oversize allocations  |       0    |       0    |       0    |       0    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Oversize GPU segments |       0    |       0    |       0    |       0    |\n",
            "|===========================================================================|\n",
            "\n",
            "2022-03-10 05:33:14 | WARNING | fairseq.trainer | attempting to recover from OOM in forward/backward pass\n",
            "epoch 004:  75% 536/712 [01:50<00:30,  5.80it/s, loss=3.318, ppl=9.97, wps=13897, ups=5.29, wpb=2627, bsz=128, num_updates=2600, lr=0.0005, gnorm=0.991, train_wall=17, gb_free=10.6, wall=552]2022-03-10 05:33:19 | WARNING | fairseq.trainer | OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 1.83 GiB (GPU 0; 11.17 GiB total capacity; 8.20 GiB already allocated; 927.81 MiB free; 9.75 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
            "2022-03-10 05:33:19 | WARNING | fairseq.trainer | |===========================================================================|\n",
            "|                  PyTorch CUDA memory summary, device ID 0                 |\n",
            "|---------------------------------------------------------------------------|\n",
            "|            CUDA OOMs: 49           |        cudaMalloc retries: 97        |\n",
            "|===========================================================================|\n",
            "|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Allocated memory      |    8397 MB |    8462 MB |    7223 GB |    7214 GB |\n",
            "|       from large pool |    8340 MB |    8404 MB |    6974 GB |    6966 GB |\n",
            "|       from small pool |      57 MB |      60 MB |     248 GB |     248 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Active memory         |    8397 MB |    8462 MB |    7223 GB |    7214 GB |\n",
            "|       from large pool |    8340 MB |    8404 MB |    6974 GB |    6966 GB |\n",
            "|       from small pool |      57 MB |      60 MB |     248 GB |     248 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| GPU reserved memory   |    9980 MB |   10032 MB |  199250 MB |  189270 MB |\n",
            "|       from large pool |    9918 MB |    9952 MB |  195012 MB |  185094 MB |\n",
            "|       from small pool |      62 MB |     130 MB |    4238 MB |    4176 MB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Non-releasable memory |    1582 MB |    1878 MB |    7386 GB |    7384 GB |\n",
            "|       from large pool |    1577 MB |    1873 MB |    7114 GB |    7113 GB |\n",
            "|       from small pool |       4 MB |      14 MB |     271 GB |     271 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Allocations           |     593    |     595    |    2979 K  |    2979 K  |\n",
            "|       from large pool |     150    |     154    |    1040 K  |    1040 K  |\n",
            "|       from small pool |     443    |     516    |    1939 K  |    1938 K  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Active allocs         |     593    |     595    |    2979 K  |    2979 K  |\n",
            "|       from large pool |     150    |     154    |    1040 K  |    1040 K  |\n",
            "|       from small pool |     443    |     516    |    1939 K  |    1938 K  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| GPU reserved segments |      72    |     108    |    2445    |    2373    |\n",
            "|       from large pool |      41    |      43    |     326    |     285    |\n",
            "|       from small pool |      31    |      65    |    2119    |    2088    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Non-releasable allocs |      54    |      57    |    1347 K  |    1347 K  |\n",
            "|       from large pool |      31    |      32    |     661 K  |     661 K  |\n",
            "|       from small pool |      23    |      34    |     686 K  |     686 K  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Oversize allocations  |       0    |       0    |       0    |       0    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Oversize GPU segments |       0    |       0    |       0    |       0    |\n",
            "|===========================================================================|\n",
            "\n",
            "2022-03-10 05:33:19 | WARNING | fairseq.trainer | attempting to recover from OOM in forward/backward pass\n",
            "epoch 004:  77% 546/712 [01:52<00:23,  7.08it/s, loss=3.318, ppl=9.97, wps=13897, ups=5.29, wpb=2627, bsz=128, num_updates=2600, lr=0.0005, gnorm=0.991, train_wall=17, gb_free=10.6, wall=552]2022-03-10 05:33:21 | WARNING | fairseq.trainer | OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 1.70 GiB (GPU 0; 11.17 GiB total capacity; 8.02 GiB already allocated; 929.81 MiB free; 9.74 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
            "2022-03-10 05:33:21 | WARNING | fairseq.trainer | |===========================================================================|\n",
            "|                  PyTorch CUDA memory summary, device ID 0                 |\n",
            "|---------------------------------------------------------------------------|\n",
            "|            CUDA OOMs: 50           |        cudaMalloc retries: 98        |\n",
            "|===========================================================================|\n",
            "|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Allocated memory      |    8215 MB |    8279 MB |    7245 GB |    7237 GB |\n",
            "|       from large pool |    8166 MB |    8230 MB |    6996 GB |    6988 GB |\n",
            "|       from small pool |      49 MB |      60 MB |     249 GB |     249 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Active memory         |    8215 MB |    8279 MB |    7245 GB |    7237 GB |\n",
            "|       from large pool |    8166 MB |    8230 MB |    6996 GB |    6988 GB |\n",
            "|       from small pool |      49 MB |      60 MB |     249 GB |     249 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| GPU reserved memory   |    9978 MB |   10024 MB |  199294 MB |  189316 MB |\n",
            "|       from large pool |    9918 MB |    9918 MB |  195012 MB |  185094 MB |\n",
            "|       from small pool |      60 MB |     106 MB |    4282 MB |    4222 MB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Non-releasable memory |    1762 MB |    1959 MB |    7409 GB |    7407 GB |\n",
            "|       from large pool |    1751 MB |    1948 MB |    7137 GB |    7135 GB |\n",
            "|       from small pool |      10 MB |      18 MB |     272 GB |     272 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Allocations           |     593    |     595    |    2990 K  |    2989 K  |\n",
            "|       from large pool |     160    |     164    |    1044 K  |    1044 K  |\n",
            "|       from small pool |     433    |     516    |    1946 K  |    1945 K  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Active allocs         |     593    |     595    |    2990 K  |    2989 K  |\n",
            "|       from large pool |     160    |     164    |    1044 K  |    1044 K  |\n",
            "|       from small pool |     433    |     516    |    1946 K  |    1945 K  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| GPU reserved segments |      71    |      94    |    2467    |    2396    |\n",
            "|       from large pool |      41    |      41    |     326    |     285    |\n",
            "|       from small pool |      30    |      53    |    2141    |    2111    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Non-releasable allocs |      60    |      61    |    1352 K  |    1352 K  |\n",
            "|       from large pool |      38    |      38    |     663 K  |     663 K  |\n",
            "|       from small pool |      22    |      39    |     689 K  |     689 K  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Oversize allocations  |       0    |       0    |       0    |       0    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Oversize GPU segments |       0    |       0    |       0    |       0    |\n",
            "|===========================================================================|\n",
            "\n",
            "2022-03-10 05:33:21 | WARNING | fairseq.trainer | attempting to recover from OOM in forward/backward pass\n",
            "epoch 004:  87% 618/712 [02:06<00:20,  4.58it/s, loss=3.337, ppl=10.1, wps=12894.9, ups=4.98, wpb=2587.8, bsz=126.7, num_updates=2700, lr=0.0005, gnorm=1.008, train_wall=18, gb_free=10.4, wall=572]2022-03-10 05:33:35 | WARNING | fairseq.trainer | OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 1.76 GiB (GPU 0; 11.17 GiB total capacity; 8.04 GiB already allocated; 963.81 MiB free; 9.71 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
            "2022-03-10 05:33:35 | WARNING | fairseq.trainer | |===========================================================================|\n",
            "|                  PyTorch CUDA memory summary, device ID 0                 |\n",
            "|---------------------------------------------------------------------------|\n",
            "|            CUDA OOMs: 51           |        cudaMalloc retries: 99        |\n",
            "|===========================================================================|\n",
            "|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Allocated memory      |    8235 MB |    8299 MB |    7429 GB |    7421 GB |\n",
            "|       from large pool |    8186 MB |    8250 MB |    7172 GB |    7164 GB |\n",
            "|       from small pool |      48 MB |      60 MB |     256 GB |     256 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Active memory         |    8235 MB |    8299 MB |    7429 GB |    7421 GB |\n",
            "|       from large pool |    8186 MB |    8250 MB |    7172 GB |    7164 GB |\n",
            "|       from small pool |      48 MB |      60 MB |     256 GB |     256 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| GPU reserved memory   |    9944 MB |   10056 MB |  199372 MB |  189428 MB |\n",
            "|       from large pool |    9884 MB |    9918 MB |  195012 MB |  185128 MB |\n",
            "|       from small pool |      60 MB |     138 MB |    4360 MB |    4300 MB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Non-releasable memory |    1708 MB |    2017 MB |    7595 GB |    7594 GB |\n",
            "|       from large pool |    1697 MB |    2005 MB |    7315 GB |    7313 GB |\n",
            "|       from small pool |      11 MB |      35 MB |     280 GB |     280 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Allocations           |     593    |     595    |    3071 K  |    3070 K  |\n",
            "|       from large pool |     160    |     164    |    1071 K  |    1071 K  |\n",
            "|       from small pool |     433    |     516    |    1999 K  |    1999 K  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Active allocs         |     593    |     595    |    3071 K  |    3070 K  |\n",
            "|       from large pool |     160    |     164    |    1071 K  |    1071 K  |\n",
            "|       from small pool |     433    |     516    |    1999 K  |    1999 K  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| GPU reserved segments |      70    |     110    |    2506    |    2436    |\n",
            "|       from large pool |      40    |      41    |     326    |     286    |\n",
            "|       from small pool |      30    |      69    |    2180    |    2150    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Non-releasable allocs |      41    |      60    |    1389 K  |    1389 K  |\n",
            "|       from large pool |      17    |      18    |     681 K  |     681 K  |\n",
            "|       from small pool |      24    |      55    |     708 K  |     708 K  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Oversize allocations  |       0    |       0    |       0    |       0    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Oversize GPU segments |       0    |       0    |       0    |       0    |\n",
            "|===========================================================================|\n",
            "\n",
            "2022-03-10 05:33:35 | WARNING | fairseq.trainer | attempting to recover from OOM in forward/backward pass\n",
            "epoch 004:  90% 638/712 [02:10<00:14,  5.16it/s, loss=3.337, ppl=10.1, wps=12894.9, ups=4.98, wpb=2587.8, bsz=126.7, num_updates=2700, lr=0.0005, gnorm=1.008, train_wall=18, gb_free=10.4, wall=572]2022-03-10 05:33:39 | WARNING | fairseq.trainer | OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 1.87 GiB (GPU 0; 11.17 GiB total capacity; 8.57 GiB already allocated; 923.81 MiB free; 9.75 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
            "2022-03-10 05:33:39 | WARNING | fairseq.trainer | |===========================================================================|\n",
            "|                  PyTorch CUDA memory summary, device ID 0                 |\n",
            "|---------------------------------------------------------------------------|\n",
            "|            CUDA OOMs: 52           |        cudaMalloc retries: 101       |\n",
            "|===========================================================================|\n",
            "|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Allocated memory      |    8776 MB |    8843 MB |    7486 GB |    7477 GB |\n",
            "|       from large pool |    8727 MB |    8794 MB |    7228 GB |    7219 GB |\n",
            "|       from small pool |      49 MB |      60 MB |     258 GB |     258 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Active memory         |    8776 MB |    8843 MB |    7486 GB |    7477 GB |\n",
            "|       from large pool |    8727 MB |    8794 MB |    7228 GB |    7219 GB |\n",
            "|       from small pool |      49 MB |      60 MB |     258 GB |     258 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| GPU reserved memory   |    9984 MB |   10018 MB |  203248 MB |  193264 MB |\n",
            "|       from large pool |    9926 MB |    9960 MB |  198836 MB |  188910 MB |\n",
            "|       from small pool |      58 MB |     112 MB |    4412 MB |    4354 MB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Non-releasable memory |    1207 MB |    2020 MB |    7657 GB |    7656 GB |\n",
            "|       from large pool |    1198 MB |    2010 MB |    7375 GB |    7374 GB |\n",
            "|       from small pool |       8 MB |      21 MB |     281 GB |     281 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Allocations           |     593    |     595    |    3092 K  |    3091 K  |\n",
            "|       from large pool |     160    |     164    |    1079 K  |    1079 K  |\n",
            "|       from small pool |     433    |     516    |    2012 K  |    2012 K  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Active allocs         |     593    |     595    |    3092 K  |    3091 K  |\n",
            "|       from large pool |     160    |     164    |    1079 K  |    1079 K  |\n",
            "|       from small pool |     433    |     516    |    2012 K  |    2012 K  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| GPU reserved segments |      68    |      96    |    2534    |    2466    |\n",
            "|       from large pool |      39    |      40    |     328    |     289    |\n",
            "|       from small pool |      29    |      56    |    2206    |    2177    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Non-releasable allocs |      45    |      48    |    1399 K  |    1399 K  |\n",
            "|       from large pool |      23    |      25    |     686 K  |     686 K  |\n",
            "|       from small pool |      22    |      40    |     712 K  |     712 K  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Oversize allocations  |       0    |       0    |       0    |       0    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Oversize GPU segments |       0    |       0    |       0    |       0    |\n",
            "|===========================================================================|\n",
            "\n",
            "2022-03-10 05:33:39 | WARNING | fairseq.trainer | attempting to recover from OOM in forward/backward pass\n",
            "epoch 004: 100% 711/712 [02:23<00:00,  3.03it/s, loss=3.337, ppl=10.1, wps=12894.9, ups=4.98, wpb=2587.8, bsz=126.7, num_updates=2700, lr=0.0005, gnorm=1.008, train_wall=18, gb_free=10.4, wall=572]2022-03-10 05:33:52 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "\n",
            "epoch 004 | valid on 'valid' subset:   0% 0/8 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 004 | valid on 'valid' subset:  12% 1/8 [00:00<00:01,  6.35it/s]\u001b[A\n",
            "epoch 004 | valid on 'valid' subset:  38% 3/8 [00:00<00:00, 10.11it/s]\u001b[A\n",
            "epoch 004 | valid on 'valid' subset:  62% 5/8 [00:00<00:00, 10.79it/s]\u001b[A\n",
            "epoch 004 | valid on 'valid' subset:  88% 7/8 [00:00<00:00,  9.67it/s]\u001b[A\n",
            "epoch 004 | valid on 'valid' subset: 100% 8/8 [00:01<00:00,  5.46it/s]\u001b[A\n",
            "                                                                      \u001b[A2022-03-10 05:33:53 | INFO | valid | epoch 004 | valid on 'valid' subset | loss 3.751 | ppl 13.46 | wps 19885.4 | wpb 2800.1 | bsz 125 | num_updates 2796 | best_loss 3.751\n",
            "2022-03-10 05:33:53 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 4 @ 2796 updates\n",
            "2022-03-10 05:33:53 | INFO | fairseq.trainer | Saving checkpoint to /content/checkpoints/checkpoint4.pt\n",
            "2022-03-10 05:33:54 | INFO | fairseq.trainer | Finished saving checkpoint to /content/checkpoints/checkpoint4.pt\n",
            "2022-03-10 05:33:55 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/checkpoint4.pt (epoch 4 @ 2796 updates, score 3.751) (writing took 1.9745601439999518 seconds)\n",
            "2022-03-10 05:33:55 | INFO | fairseq_cli.train | end of epoch 4 (average epoch stats below)\n",
            "2022-03-10 05:33:55 | INFO | train | epoch 004 | loss 3.335 | ppl 10.09 | wps 12993.9 | ups 4.74 | wpb 2743.3 | bsz 127.7 | num_updates 2796 | lr 0.0005 | gnorm 0.93 | train_wall 132 | gb_free 10.4 | wall 594\n",
            "2022-03-10 05:33:55 | INFO | fairseq.data.iterators | grouped total_num_itrs = 712\n",
            "epoch 005:   0% 0/712 [00:00<?, ?it/s]2022-03-10 05:33:55 | INFO | fairseq.trainer | begin training epoch 5\n",
            "2022-03-10 05:33:55 | INFO | fairseq_cli.train | Start iterating over samples\n",
            "epoch 005:   4% 25/712 [00:05<03:23,  3.37it/s, loss=3.305, ppl=9.89, wps=11536.5, ups=4.37, wpb=2637.1, bsz=128, num_updates=2800, lr=0.0005, gnorm=0.942, train_wall=18, gb_free=10.2, wall=595]2022-03-10 05:34:02 | WARNING | fairseq.trainer | OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 50.00 MiB (GPU 0; 11.17 GiB total capacity; 10.44 GiB already allocated; 13.81 MiB free; 10.64 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
            "2022-03-10 05:34:02 | WARNING | fairseq.trainer | |===========================================================================|\n",
            "|                  PyTorch CUDA memory summary, device ID 0                 |\n",
            "|---------------------------------------------------------------------------|\n",
            "|            CUDA OOMs: 53           |        cudaMalloc retries: 103       |\n",
            "|===========================================================================|\n",
            "|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Allocated memory      |   10688 MB |   10688 MB |    7755 GB |    7744 GB |\n",
            "|       from large pool |   10637 MB |   10637 MB |    7487 GB |    7477 GB |\n",
            "|       from small pool |      50 MB |      61 MB |     267 GB |     267 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Active memory         |   10688 MB |   10688 MB |    7755 GB |    7744 GB |\n",
            "|       from large pool |   10637 MB |   10637 MB |    7487 GB |    7477 GB |\n",
            "|       from small pool |      50 MB |      61 MB |     267 GB |     267 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| GPU reserved memory   |   10894 MB |   10894 MB |  204230 MB |  193336 MB |\n",
            "|       from large pool |   10836 MB |   10836 MB |  199746 MB |  188910 MB |\n",
            "|       from small pool |      58 MB |     130 MB |    4484 MB |    4426 MB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Non-releasable memory |  210842 KB |    2409 MB |    7936 GB |    7936 GB |\n",
            "|       from large pool |  203437 KB |    2399 MB |    7643 GB |    7643 GB |\n",
            "|       from small pool |    7405 KB |      17 MB |     292 GB |     292 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Allocations           |     590    |     590    |    3204 K  |    3204 K  |\n",
            "|       from large pool |     160    |     160    |    1118 K  |    1118 K  |\n",
            "|       from small pool |     430    |     516    |    2086 K  |    2085 K  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Active allocs         |     590    |     590    |    3204 K  |    3204 K  |\n",
            "|       from large pool |     160    |     160    |    1118 K  |    1118 K  |\n",
            "|       from small pool |     430    |     516    |    2086 K  |    2085 K  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| GPU reserved segments |      73    |     107    |    2575    |    2502    |\n",
            "|       from large pool |      44    |      44    |     333    |     289    |\n",
            "|       from small pool |      29    |      65    |    2242    |    2213    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Non-releasable allocs |      56    |      58    |    1452 K  |    1452 K  |\n",
            "|       from large pool |      34    |      36    |     712 K  |     712 K  |\n",
            "|       from small pool |      22    |      34    |     739 K  |     739 K  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Oversize allocations  |       0    |       0    |       0    |       0    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Oversize GPU segments |       0    |       0    |       0    |       0    |\n",
            "|===========================================================================|\n",
            "\n",
            "2022-03-10 05:34:02 | WARNING | fairseq.trainer | attempting to recover from OOM in forward/backward pass\n",
            "epoch 005:   6% 40/712 [00:09<01:54,  5.89it/s, loss=3.305, ppl=9.89, wps=11536.5, ups=4.37, wpb=2637.1, bsz=128, num_updates=2800, lr=0.0005, gnorm=0.942, train_wall=18, gb_free=10.2, wall=595]2022-03-10 05:34:05 | WARNING | fairseq.trainer | OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 2.96 GiB (GPU 0; 11.17 GiB total capacity; 10.42 GiB already allocated; 53.81 MiB free; 10.60 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
            "2022-03-10 05:34:05 | WARNING | fairseq.trainer | |===========================================================================|\n",
            "|                  PyTorch CUDA memory summary, device ID 0                 |\n",
            "|---------------------------------------------------------------------------|\n",
            "|            CUDA OOMs: 54           |        cudaMalloc retries: 105       |\n",
            "|===========================================================================|\n",
            "|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Allocated memory      |   10622 MB |   10672 MB |    7795 GB |    7785 GB |\n",
            "|       from large pool |   10571 MB |   10621 MB |    7526 GB |    7516 GB |\n",
            "|       from small pool |      51 MB |      60 MB |     268 GB |     268 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Active memory         |   10622 MB |   10672 MB |    7795 GB |    7785 GB |\n",
            "|       from large pool |   10571 MB |   10621 MB |    7526 GB |    7516 GB |\n",
            "|       from small pool |      51 MB |      60 MB |     268 GB |     268 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| GPU reserved memory   |   10854 MB |   10898 MB |  214880 MB |  204026 MB |\n",
            "|       from large pool |   10796 MB |   10796 MB |  210352 MB |  199556 MB |\n",
            "|       from small pool |      58 MB |     102 MB |    4528 MB |    4470 MB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Non-releasable memory |  236955 KB |     807 MB |    7971 GB |    7971 GB |\n",
            "|       from large pool |  229864 KB |     799 MB |    7678 GB |    7677 GB |\n",
            "|       from small pool |    7091 KB |      20 MB |     293 GB |     293 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Allocations           |     593    |     595    |    3220 K  |    3220 K  |\n",
            "|       from large pool |     162    |     163    |    1124 K  |    1124 K  |\n",
            "|       from small pool |     431    |     516    |    2096 K  |    2095 K  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Active allocs         |     593    |     595    |    3220 K  |    3220 K  |\n",
            "|       from large pool |     162    |     163    |    1124 K  |    1124 K  |\n",
            "|       from small pool |     431    |     516    |    2096 K  |    2095 K  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| GPU reserved segments |      73    |      95    |    2635    |    2562    |\n",
            "|       from large pool |      44    |      44    |     371    |     327    |\n",
            "|       from small pool |      29    |      51    |    2264    |    2235    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Non-releasable allocs |      52    |      52    |    1458 K  |    1458 K  |\n",
            "|       from large pool |      28    |      28    |     716 K  |     716 K  |\n",
            "|       from small pool |      24    |      46    |     742 K  |     742 K  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Oversize allocations  |       0    |       0    |       0    |       0    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Oversize GPU segments |       0    |       0    |       0    |       0    |\n",
            "|===========================================================================|\n",
            "\n",
            "2022-03-10 05:34:05 | WARNING | fairseq.trainer | attempting to recover from OOM in forward/backward pass\n",
            "epoch 005:  20% 139/712 [00:29<01:30,  6.34it/s, loss=3.011, ppl=8.06, wps=11789.6, ups=4.42, wpb=2669.1, bsz=128, num_updates=2900, lr=0.0005, gnorm=0.893, train_wall=21, gb_free=10.4, wall=617]2022-03-10 05:34:25 | WARNING | fairseq.trainer | OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 1.87 GiB (GPU 0; 11.17 GiB total capacity; 8.57 GiB already allocated; 1.38 GiB free; 9.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
            "2022-03-10 05:34:25 | WARNING | fairseq.trainer | |===========================================================================|\n",
            "|                  PyTorch CUDA memory summary, device ID 0                 |\n",
            "|---------------------------------------------------------------------------|\n",
            "|            CUDA OOMs: 55           |        cudaMalloc retries: 108       |\n",
            "|===========================================================================|\n",
            "|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Allocated memory      |    8772 MB |    8839 MB |    8041 GB |    8032 GB |\n",
            "|       from large pool |    8723 MB |    8790 MB |    7762 GB |    7753 GB |\n",
            "|       from small pool |      49 MB |      60 MB |     278 GB |     278 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Active memory         |    8772 MB |    8839 MB |    8041 GB |    8032 GB |\n",
            "|       from large pool |    8723 MB |    8790 MB |    7762 GB |    7753 GB |\n",
            "|       from small pool |      49 MB |      60 MB |     278 GB |     278 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| GPU reserved memory   |    9496 MB |   10472 MB |  228928 MB |  219432 MB |\n",
            "|       from large pool |    9438 MB |   10342 MB |  224328 MB |  214890 MB |\n",
            "|       from small pool |      58 MB |     130 MB |    4600 MB |    4542 MB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Non-releasable memory |  741045 KB |    1501 MB |    8204 GB |    8203 GB |\n",
            "|       from large pool |  731936 KB |    1492 MB |    7900 GB |    7899 GB |\n",
            "|       from small pool |    9109 KB |      27 MB |     304 GB |     304 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Allocations           |     593    |     595    |    3331 K  |    3330 K  |\n",
            "|       from large pool |     160    |     164    |    1161 K  |    1161 K  |\n",
            "|       from small pool |     433    |     516    |    2169 K  |    2169 K  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Active allocs         |     593    |     595    |    3331 K  |    3330 K  |\n",
            "|       from large pool |     160    |     164    |    1161 K  |    1161 K  |\n",
            "|       from small pool |     433    |     516    |    2169 K  |    2169 K  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| GPU reserved segments |      58    |      95    |    2697    |    2639    |\n",
            "|       from large pool |      29    |      30    |     397    |     368    |\n",
            "|       from small pool |      29    |      65    |    2300    |    2271    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Non-releasable allocs |      38    |      56    |    1508 K  |    1508 K  |\n",
            "|       from large pool |      16    |      18    |     738 K  |     738 K  |\n",
            "|       from small pool |      22    |      49    |     769 K  |     769 K  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Oversize allocations  |       0    |       0    |       0    |       0    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Oversize GPU segments |       0    |       0    |       0    |       0    |\n",
            "|===========================================================================|\n",
            "\n",
            "2022-03-10 05:34:25 | WARNING | fairseq.trainer | attempting to recover from OOM in forward/backward pass\n",
            "epoch 005:  20% 140/712 [00:30<02:58,  3.21it/s, loss=3.011, ppl=8.06, wps=11789.6, ups=4.42, wpb=2669.1, bsz=128, num_updates=2900, lr=0.0005, gnorm=0.893, train_wall=21, gb_free=10.4, wall=617]2022-03-10 05:34:25 | WARNING | fairseq.trainer | OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 1.76 GiB (GPU 0; 11.17 GiB total capacity; 8.04 GiB already allocated; 1.38 GiB free; 9.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
            "2022-03-10 05:34:25 | WARNING | fairseq.trainer | |===========================================================================|\n",
            "|                  PyTorch CUDA memory summary, device ID 0                 |\n",
            "|---------------------------------------------------------------------------|\n",
            "|            CUDA OOMs: 56           |        cudaMalloc retries: 109       |\n",
            "|===========================================================================|\n",
            "|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Allocated memory      |    8234 MB |    8839 MB |    8051 GB |    8043 GB |\n",
            "|       from large pool |    8185 MB |    8790 MB |    7772 GB |    7764 GB |\n",
            "|       from small pool |      48 MB |      60 MB |     278 GB |     278 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Active memory         |    8234 MB |    8839 MB |    8051 GB |    8043 GB |\n",
            "|       from large pool |    8185 MB |    8790 MB |    7772 GB |    7764 GB |\n",
            "|       from small pool |      48 MB |      60 MB |     278 GB |     278 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| GPU reserved memory   |    9496 MB |   10472 MB |  228928 MB |  219432 MB |\n",
            "|       from large pool |    9438 MB |   10342 MB |  224328 MB |  214890 MB |\n",
            "|       from small pool |      58 MB |     130 MB |    4600 MB |    4542 MB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Non-releasable memory |    1261 MB |    1880 MB |    8212 GB |    8211 GB |\n",
            "|       from large pool |    1252 MB |    1871 MB |    7908 GB |    7907 GB |\n",
            "|       from small pool |       9 MB |      27 MB |     304 GB |     304 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Allocations           |     593    |     595    |    3331 K  |    3331 K  |\n",
            "|       from large pool |     160    |     164    |    1162 K  |    1161 K  |\n",
            "|       from small pool |     433    |     516    |    2169 K  |    2169 K  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Active allocs         |     593    |     595    |    3331 K  |    3331 K  |\n",
            "|       from large pool |     160    |     164    |    1162 K  |    1161 K  |\n",
            "|       from small pool |     433    |     516    |    2169 K  |    2169 K  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| GPU reserved segments |      58    |      95    |    2697    |    2639    |\n",
            "|       from large pool |      29    |      30    |     397    |     368    |\n",
            "|       from small pool |      29    |      65    |    2300    |    2271    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Non-releasable allocs |      38    |      56    |    1508 K  |    1508 K  |\n",
            "|       from large pool |      15    |      20    |     738 K  |     738 K  |\n",
            "|       from small pool |      23    |      49    |     769 K  |     769 K  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Oversize allocations  |       0    |       0    |       0    |       0    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Oversize GPU segments |       0    |       0    |       0    |       0    |\n",
            "|===========================================================================|\n",
            "\n",
            "2022-03-10 05:34:25 | WARNING | fairseq.trainer | attempting to recover from OOM in forward/backward pass\n",
            "epoch 005:  29% 207/712 [00:43<02:49,  2.97it/s, loss=3.011, ppl=8.06, wps=11789.6, ups=4.42, wpb=2669.1, bsz=128, num_updates=2900, lr=0.0005, gnorm=0.893, train_wall=21, gb_free=10.4, wall=617]2022-03-10 05:34:40 | WARNING | fairseq.trainer | OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 2.51 GiB (GPU 0; 11.17 GiB total capacity; 10.39 GiB already allocated; 111.81 MiB free; 10.54 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
            "2022-03-10 05:34:40 | WARNING | fairseq.trainer | |===========================================================================|\n",
            "|                  PyTorch CUDA memory summary, device ID 0                 |\n",
            "|---------------------------------------------------------------------------|\n",
            "|            CUDA OOMs: 57           |        cudaMalloc retries: 110       |\n",
            "|===========================================================================|\n",
            "|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Allocated memory      |   10598 MB |   10640 MB |    8241 GB |    8231 GB |\n",
            "|       from large pool |   10547 MB |   10589 MB |    7956 GB |    7946 GB |\n",
            "|       from small pool |      51 MB |      61 MB |     285 GB |     285 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Active memory         |   10598 MB |   10640 MB |    8241 GB |    8231 GB |\n",
            "|       from large pool |   10547 MB |   10589 MB |    7956 GB |    7946 GB |\n",
            "|       from small pool |      51 MB |      61 MB |     285 GB |     285 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| GPU reserved memory   |   10796 MB |   10876 MB |  230308 MB |  219512 MB |\n",
            "|       from large pool |   10738 MB |   10738 MB |  225628 MB |  214890 MB |\n",
            "|       from small pool |      58 MB |     138 MB |    4680 MB |    4622 MB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Non-releasable memory |  202652 KB |    2005 MB |    8407 GB |    8407 GB |\n",
            "|       from large pool |  195540 KB |    1996 MB |    8096 GB |    8095 GB |\n",
            "|       from small pool |    7112 KB |      12 MB |     311 GB |     311 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Allocations           |     593    |     595    |    3406 K  |    3406 K  |\n",
            "|       from large pool |     162    |     163    |    1187 K  |    1187 K  |\n",
            "|       from small pool |     431    |     516    |    2218 K  |    2218 K  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Active allocs         |     593    |     595    |    3406 K  |    3406 K  |\n",
            "|       from large pool |     162    |     163    |    1187 K  |    1187 K  |\n",
            "|       from small pool |     431    |     516    |    2218 K  |    2218 K  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| GPU reserved segments |      61    |     101    |    2740    |    2679    |\n",
            "|       from large pool |      32    |      32    |     400    |     368    |\n",
            "|       from small pool |      29    |      69    |    2340    |    2311    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Non-releasable allocs |      55    |      55    |    1541 K  |    1541 K  |\n",
            "|       from large pool |      30    |      30    |     753 K  |     753 K  |\n",
            "|       from small pool |      25    |      27    |     787 K  |     787 K  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Oversize allocations  |       0    |       0    |       0    |       0    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Oversize GPU segments |       0    |       0    |       0    |       0    |\n",
            "|===========================================================================|\n",
            "\n",
            "2022-03-10 05:34:40 | WARNING | fairseq.trainer | attempting to recover from OOM in forward/backward pass\n",
            "epoch 005:  38% 268/712 [00:57<01:26,  5.16it/s, loss=2.994, ppl=7.97, wps=12623.9, ups=4.76, wpb=2651.4, bsz=128, num_updates=3000, lr=0.0005, gnorm=0.927, train_wall=19, gb_free=0.8, wall=638]2022-03-10 05:34:53 | WARNING | fairseq.trainer | OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 1.45 GiB (GPU 0; 11.17 GiB total capacity; 7.96 GiB already allocated; 795.81 MiB free; 9.88 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
            "2022-03-10 05:34:53 | WARNING | fairseq.trainer | |===========================================================================|\n",
            "|                  PyTorch CUDA memory summary, device ID 0                 |\n",
            "|---------------------------------------------------------------------------|\n",
            "|            CUDA OOMs: 58           |        cudaMalloc retries: 112       |\n",
            "|===========================================================================|\n",
            "|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Allocated memory      |    6666 MB |    8149 MB |    8418 GB |    8411 GB |\n",
            "|       from large pool |    6617 MB |    8100 MB |    8127 GB |    8121 GB |\n",
            "|       from small pool |      48 MB |      61 MB |     290 GB |     290 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Active memory         |    6666 MB |    8149 MB |    8418 GB |    8411 GB |\n",
            "|       from large pool |    6617 MB |    8100 MB |    8127 GB |    8121 GB |\n",
            "|       from small pool |      48 MB |      61 MB |     290 GB |     290 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| GPU reserved memory   |   10112 MB |   10180 MB |  231732 MB |  221620 MB |\n",
            "|       from large pool |   10054 MB |   10054 MB |  226928 MB |  216874 MB |\n",
            "|       from small pool |      58 MB |     126 MB |    4804 MB |    4746 MB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Non-releasable memory |    1533 MB |    1962 MB |    8597 GB |    8595 GB |\n",
            "|       from large pool |    1524 MB |    1953 MB |    8279 GB |    8278 GB |\n",
            "|       from small pool |       9 MB |      13 MB |     317 GB |     317 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Allocations           |     592    |     595    |    3474 K  |    3474 K  |\n",
            "|       from large pool |     160    |     164    |    1212 K  |    1212 K  |\n",
            "|       from small pool |     432    |     516    |    2262 K  |    2261 K  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Active allocs         |     592    |     595    |    3474 K  |    3474 K  |\n",
            "|       from large pool |     160    |     164    |    1212 K  |    1212 K  |\n",
            "|       from small pool |     432    |     516    |    2262 K  |    2261 K  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| GPU reserved segments |      59    |      93    |    2803    |    2744    |\n",
            "|       from large pool |      30    |      30    |     401    |     371    |\n",
            "|       from small pool |      29    |      63    |    2402    |    2373    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Non-releasable allocs |      49    |      50    |    1571 K  |    1571 K  |\n",
            "|       from large pool |      25    |      26    |     768 K  |     768 K  |\n",
            "|       from small pool |      24    |      33    |     802 K  |     802 K  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Oversize allocations  |       0    |       0    |       0    |       0    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Oversize GPU segments |       0    |       0    |       0    |       0    |\n",
            "|===========================================================================|\n",
            "\n",
            "2022-03-10 05:34:53 | WARNING | fairseq.trainer | attempting to recover from OOM in forward/backward pass\n",
            "epoch 005:  50% 357/712 [01:14<01:24,  4.22it/s, loss=3.139, ppl=8.81, wps=13721.7, ups=4.55, wpb=3012.6, bsz=128, num_updates=3100, lr=0.0005, gnorm=0.921, train_wall=21, gb_free=9.4, wall=660]2022-03-10 05:35:11 | WARNING | fairseq.trainer | OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 1.52 GiB (GPU 0; 11.17 GiB total capacity; 8.51 GiB already allocated; 1.47 GiB free; 9.18 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
            "2022-03-10 05:35:11 | WARNING | fairseq.trainer | |===========================================================================|\n",
            "|                  PyTorch CUDA memory summary, device ID 0                 |\n",
            "|---------------------------------------------------------------------------|\n",
            "|            CUDA OOMs: 59           |        cudaMalloc retries: 114       |\n",
            "|===========================================================================|\n",
            "|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Allocated memory      |    7162 MB |    8714 MB |    8651 GB |    8644 GB |\n",
            "|       from large pool |    7113 MB |    8665 MB |    8351 GB |    8345 GB |\n",
            "|       from small pool |      48 MB |      60 MB |     299 GB |     299 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Active memory         |    7162 MB |    8714 MB |    8651 GB |    8644 GB |\n",
            "|       from large pool |    7113 MB |    8665 MB |    8351 GB |    8345 GB |\n",
            "|       from small pool |      48 MB |      60 MB |     299 GB |     299 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| GPU reserved memory   |    9400 MB |   10798 MB |  237060 MB |  227660 MB |\n",
            "|       from large pool |    9342 MB |   10666 MB |  232116 MB |  222774 MB |\n",
            "|       from small pool |      58 MB |     132 MB |    4944 MB |    4886 MB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Non-releasable memory |  702161 KB |    1381 MB |    8832 GB |    8832 GB |\n",
            "|       from large pool |  692642 KB |    1371 MB |    8506 GB |    8505 GB |\n",
            "|       from small pool |    9519 KB |      15 MB |     326 GB |     326 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Allocations           |     592    |     595    |    3574 K  |    3573 K  |\n",
            "|       from large pool |     160    |     164    |    1246 K  |    1245 K  |\n",
            "|       from small pool |     432    |     516    |    2327 K  |    2327 K  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Active allocs         |     592    |     595    |    3574 K  |    3573 K  |\n",
            "|       from large pool |     160    |     164    |    1246 K  |    1245 K  |\n",
            "|       from small pool |     432    |     516    |    2327 K  |    2327 K  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| GPU reserved segments |      58    |      96    |    2877    |    2819    |\n",
            "|       from large pool |      29    |      30    |     405    |     376    |\n",
            "|       from small pool |      29    |      66    |    2472    |    2443    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Non-releasable allocs |      47    |      47    |    1615 K  |    1615 K  |\n",
            "|       from large pool |      22    |      22    |     788 K  |     788 K  |\n",
            "|       from small pool |      25    |      38    |     826 K  |     826 K  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Oversize allocations  |       0    |       0    |       0    |       0    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Oversize GPU segments |       0    |       0    |       0    |       0    |\n",
            "|===========================================================================|\n",
            "\n",
            "2022-03-10 05:35:11 | WARNING | fairseq.trainer | attempting to recover from OOM in forward/backward pass\n",
            "epoch 005:  57% 406/712 [01:24<00:56,  5.38it/s, loss=3.139, ppl=8.81, wps=13721.7, ups=4.55, wpb=3012.6, bsz=128, num_updates=3100, lr=0.0005, gnorm=0.921, train_wall=21, gb_free=9.4, wall=660]2022-03-10 05:35:20 | WARNING | fairseq.trainer | OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 1.70 GiB (GPU 0; 11.17 GiB total capacity; 8.03 GiB already allocated; 1.58 GiB free; 9.07 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
            "2022-03-10 05:35:20 | WARNING | fairseq.trainer | |===========================================================================|\n",
            "|                  PyTorch CUDA memory summary, device ID 0                 |\n",
            "|---------------------------------------------------------------------------|\n",
            "|            CUDA OOMs: 60           |        cudaMalloc retries: 116       |\n",
            "|===========================================================================|\n",
            "|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Allocated memory      |    8219 MB |    8283 MB |    8774 GB |    8766 GB |\n",
            "|       from large pool |    8170 MB |    8234 MB |    8471 GB |    8463 GB |\n",
            "|       from small pool |      49 MB |      60 MB |     303 GB |     303 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Active memory         |    8219 MB |    8283 MB |    8774 GB |    8766 GB |\n",
            "|       from large pool |    8170 MB |    8234 MB |    8471 GB |    8463 GB |\n",
            "|       from small pool |      49 MB |      60 MB |     303 GB |     303 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| GPU reserved memory   |    9292 MB |   10506 MB |  241462 MB |  232170 MB |\n",
            "|       from large pool |    9234 MB |   10390 MB |  236460 MB |  227226 MB |\n",
            "|       from small pool |      58 MB |     116 MB |    5002 MB |    4944 MB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Non-releasable memory |    1072 MB |    1476 MB |    8961 GB |    8959 GB |\n",
            "|       from large pool |    1063 MB |    1465 MB |    8629 GB |    8628 GB |\n",
            "|       from small pool |       8 MB |      17 MB |     331 GB |     331 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Allocations           |     593    |     595    |    3628 K  |    3627 K  |\n",
            "|       from large pool |     160    |     164    |    1265 K  |    1265 K  |\n",
            "|       from small pool |     433    |     516    |    2363 K  |    2362 K  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Active allocs         |     593    |     595    |    3628 K  |    3627 K  |\n",
            "|       from large pool |     160    |     164    |    1265 K  |    1265 K  |\n",
            "|       from small pool |     433    |     516    |    2363 K  |    2362 K  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| GPU reserved segments |      58    |      88    |    2909    |    2851    |\n",
            "|       from large pool |      29    |      30    |     408    |     379    |\n",
            "|       from small pool |      29    |      58    |    2501    |    2472    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Non-releasable allocs |      43    |      46    |    1639 K  |    1639 K  |\n",
            "|       from large pool |      20    |      20    |     800 K  |     800 K  |\n",
            "|       from small pool |      23    |      38    |     839 K  |     839 K  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Oversize allocations  |       0    |       0    |       0    |       0    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Oversize GPU segments |       0    |       0    |       0    |       0    |\n",
            "|===========================================================================|\n",
            "\n",
            "2022-03-10 05:35:20 | WARNING | fairseq.trainer | attempting to recover from OOM in forward/backward pass\n",
            "epoch 005:  63% 452/712 [01:33<00:47,  5.44it/s, loss=2.973, ppl=7.85, wps=13319.8, ups=5.18, wpb=2573, bsz=128, num_updates=3200, lr=0.0005, gnorm=0.948, train_wall=18, gb_free=9.9, wall=680]2022-03-10 05:35:29 | WARNING | fairseq.trainer | OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 1.44 GiB (GPU 0; 11.17 GiB total capacity; 7.69 GiB already allocated; 1.41 GiB free; 9.25 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
            "2022-03-10 05:35:29 | WARNING | fairseq.trainer | |===========================================================================|\n",
            "|                  PyTorch CUDA memory summary, device ID 0                 |\n",
            "|---------------------------------------------------------------------------|\n",
            "|            CUDA OOMs: 61           |        cudaMalloc retries: 117       |\n",
            "|===========================================================================|\n",
            "|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Allocated memory      |    6401 MB |    7877 MB |    8887 GB |    8881 GB |\n",
            "|       from large pool |    6353 MB |    7829 MB |    8579 GB |    8573 GB |\n",
            "|       from small pool |      48 MB |      61 MB |     307 GB |     307 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Active memory         |    6401 MB |    7877 MB |    8887 GB |    8881 GB |\n",
            "|       from large pool |    6353 MB |    7829 MB |    8579 GB |    8573 GB |\n",
            "|       from small pool |      48 MB |      61 MB |     307 GB |     307 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| GPU reserved memory   |    9468 MB |   10848 MB |  243018 MB |  233550 MB |\n",
            "|       from large pool |    9410 MB |   10710 MB |  237936 MB |  228526 MB |\n",
            "|       from small pool |      58 MB |     138 MB |    5082 MB |    5024 MB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Non-releasable memory |    1590 MB |    1590 MB |    9071 GB |    9070 GB |\n",
            "|       from large pool |    1580 MB |    1580 MB |    8735 GB |    8734 GB |\n",
            "|       from small pool |       9 MB |      17 MB |     336 GB |     336 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Allocations           |     592    |     595    |    3679 K  |    3678 K  |\n",
            "|       from large pool |     160    |     164    |    1283 K  |    1282 K  |\n",
            "|       from small pool |     432    |     516    |    2396 K  |    2395 K  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Active allocs         |     592    |     595    |    3679 K  |    3678 K  |\n",
            "|       from large pool |     160    |     164    |    1283 K  |    1282 K  |\n",
            "|       from small pool |     432    |     516    |    2396 K  |    2395 K  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| GPU reserved segments |      58    |      99    |    2950    |    2892    |\n",
            "|       from large pool |      29    |      30    |     409    |     380    |\n",
            "|       from small pool |      29    |      69    |    2541    |    2512    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Non-releasable allocs |      43    |      47    |    1662 K  |    1662 K  |\n",
            "|       from large pool |      18    |      18    |     811 K  |     811 K  |\n",
            "|       from small pool |      25    |      42    |     850 K  |     850 K  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Oversize allocations  |       0    |       0    |       0    |       0    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Oversize GPU segments |       0    |       0    |       0    |       0    |\n",
            "|===========================================================================|\n",
            "\n",
            "2022-03-10 05:35:29 | WARNING | fairseq.trainer | attempting to recover from OOM in forward/backward pass\n",
            "epoch 005:  71% 506/712 [01:42<00:36,  5.68it/s, loss=2.973, ppl=7.85, wps=13319.8, ups=5.18, wpb=2573, bsz=128, num_updates=3200, lr=0.0005, gnorm=0.948, train_wall=18, gb_free=9.9, wall=680]2022-03-10 05:35:38 | WARNING | fairseq.trainer | OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 1.44 GiB (GPU 0; 11.17 GiB total capacity; 7.98 GiB already allocated; 1.41 GiB free; 9.25 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
            "2022-03-10 05:35:38 | WARNING | fairseq.trainer | |===========================================================================|\n",
            "|                  PyTorch CUDA memory summary, device ID 0                 |\n",
            "|---------------------------------------------------------------------------|\n",
            "|            CUDA OOMs: 62           |        cudaMalloc retries: 118       |\n",
            "|===========================================================================|\n",
            "|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Allocated memory      |    6700 MB |    8176 MB |    9004 GB |    8997 GB |\n",
            "|       from large pool |    6651 MB |    8127 MB |    8691 GB |    8684 GB |\n",
            "|       from small pool |      48 MB |      60 MB |     312 GB |     312 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Active memory         |    6700 MB |    8176 MB |    9004 GB |    8997 GB |\n",
            "|       from large pool |    6651 MB |    8127 MB |    8691 GB |    8684 GB |\n",
            "|       from small pool |      48 MB |      60 MB |     312 GB |     312 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| GPU reserved memory   |    9468 MB |    9542 MB |  244568 MB |  235100 MB |\n",
            "|       from large pool |    9410 MB |    9410 MB |  239412 MB |  230002 MB |\n",
            "|       from small pool |      58 MB |     132 MB |    5156 MB |    5098 MB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Non-releasable memory |    1291 MB |    1337 MB |    9196 GB |    9195 GB |\n",
            "|       from large pool |    1282 MB |    1327 MB |    8854 GB |    8853 GB |\n",
            "|       from small pool |       9 MB |      21 MB |     341 GB |     341 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Allocations           |     592    |     595    |    3739 K  |    3738 K  |\n",
            "|       from large pool |     160    |     164    |    1303 K  |    1303 K  |\n",
            "|       from small pool |     432    |     516    |    2435 K  |    2435 K  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Active allocs         |     592    |     595    |    3739 K  |    3738 K  |\n",
            "|       from large pool |     160    |     164    |    1303 K  |    1303 K  |\n",
            "|       from small pool |     432    |     516    |    2435 K  |    2435 K  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| GPU reserved segments |      58    |      95    |    2988    |    2930    |\n",
            "|       from large pool |      29    |      29    |     410    |     381    |\n",
            "|       from small pool |      29    |      66    |    2578    |    2549    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Non-releasable allocs |      41    |      47    |    1688 K  |    1688 K  |\n",
            "|       from large pool |      18    |      18    |     823 K  |     823 K  |\n",
            "|       from small pool |      23    |      41    |     864 K  |     864 K  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Oversize allocations  |       0    |       0    |       0    |       0    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Oversize GPU segments |       0    |       0    |       0    |       0    |\n",
            "|===========================================================================|\n",
            "\n",
            "2022-03-10 05:35:38 | WARNING | fairseq.trainer | attempting to recover from OOM in forward/backward pass\n",
            "epoch 005:  75% 534/712 [01:47<00:34,  5.09it/s, loss=3.003, ppl=8.02, wps=14515.2, ups=5.48, wpb=2648.3, bsz=126.7, num_updates=3300, lr=0.0005, gnorm=1.058, train_wall=17, gb_free=10.6, wall=698]2022-03-10 05:35:43 | WARNING | fairseq.trainer | OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 1.83 GiB (GPU 0; 11.17 GiB total capacity; 8.20 GiB already allocated; 1.01 GiB free; 9.64 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
            "2022-03-10 05:35:43 | WARNING | fairseq.trainer | |===========================================================================|\n",
            "|                  PyTorch CUDA memory summary, device ID 0                 |\n",
            "|---------------------------------------------------------------------------|\n",
            "|            CUDA OOMs: 63           |        cudaMalloc retries: 119       |\n",
            "|===========================================================================|\n",
            "|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Allocated memory      |    8398 MB |    8463 MB |    9069 GB |    9061 GB |\n",
            "|       from large pool |    8340 MB |    8405 MB |    8753 GB |    8745 GB |\n",
            "|       from small pool |      57 MB |      61 MB |     315 GB |     315 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Active memory         |    8398 MB |    8463 MB |    9069 GB |    9061 GB |\n",
            "|       from large pool |    8340 MB |    8405 MB |    8753 GB |    8745 GB |\n",
            "|       from small pool |      57 MB |      61 MB |     315 GB |     315 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| GPU reserved memory   |    9870 MB |    9936 MB |  246512 MB |  236642 MB |\n",
            "|       from large pool |    9808 MB |    9808 MB |  241286 MB |  231478 MB |\n",
            "|       from small pool |      62 MB |     128 MB |    5226 MB |    5164 MB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Non-releasable memory |    1471 MB |    1926 MB |    9261 GB |    9260 GB |\n",
            "|       from large pool |    1467 MB |    1921 MB |    8916 GB |    8915 GB |\n",
            "|       from small pool |       4 MB |      12 MB |     344 GB |     344 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Allocations           |     593    |     595    |    3769 K  |    3769 K  |\n",
            "|       from large pool |     150    |     154    |    1314 K  |    1313 K  |\n",
            "|       from small pool |     443    |     516    |    2455 K  |    2455 K  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Active allocs         |     593    |     595    |    3769 K  |    3769 K  |\n",
            "|       from large pool |     150    |     154    |    1314 K  |    1313 K  |\n",
            "|       from small pool |     443    |     516    |    2455 K  |    2455 K  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| GPU reserved segments |      60    |      93    |    3024    |    2964    |\n",
            "|       from large pool |      29    |      29    |     411    |     382    |\n",
            "|       from small pool |      31    |      64    |    2613    |    2582    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Non-releasable allocs |      43    |      43    |    1702 K  |    1701 K  |\n",
            "|       from large pool |      21    |      21    |     829 K  |     829 K  |\n",
            "|       from small pool |      22    |      25    |     872 K  |     872 K  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Oversize allocations  |       0    |       0    |       0    |       0    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Oversize GPU segments |       0    |       0    |       0    |       0    |\n",
            "|===========================================================================|\n",
            "\n",
            "2022-03-10 05:35:43 | WARNING | fairseq.trainer | attempting to recover from OOM in forward/backward pass\n",
            "epoch 005:  75% 535/712 [01:48<00:49,  3.55it/s, loss=3.003, ppl=8.02, wps=14515.2, ups=5.48, wpb=2648.3, bsz=126.7, num_updates=3300, lr=0.0005, gnorm=1.058, train_wall=17, gb_free=10.6, wall=698]2022-03-10 05:35:44 | WARNING | fairseq.trainer | OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 1.36 GiB (GPU 0; 11.17 GiB total capacity; 7.33 GiB already allocated; 1.02 GiB free; 9.63 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
            "2022-03-10 05:35:44 | WARNING | fairseq.trainer | |===========================================================================|\n",
            "|                  PyTorch CUDA memory summary, device ID 0                 |\n",
            "|---------------------------------------------------------------------------|\n",
            "|            CUDA OOMs: 64           |        cudaMalloc retries: 120       |\n",
            "|===========================================================================|\n",
            "|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Allocated memory      |    6112 MB |    8463 MB |    9078 GB |    9072 GB |\n",
            "|       from large pool |    6063 MB |    8405 MB |    8762 GB |    8756 GB |\n",
            "|       from small pool |      48 MB |      61 MB |     315 GB |     315 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Active memory         |    6112 MB |    8463 MB |    9078 GB |    9072 GB |\n",
            "|       from large pool |    6063 MB |    8405 MB |    8762 GB |    8756 GB |\n",
            "|       from small pool |      48 MB |      61 MB |     315 GB |     315 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| GPU reserved memory   |    9866 MB |    9936 MB |  246512 MB |  236646 MB |\n",
            "|       from large pool |    9808 MB |    9808 MB |  241286 MB |  231478 MB |\n",
            "|       from small pool |      58 MB |     128 MB |    5226 MB |    5168 MB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Non-releasable memory |    1841 MB |    2471 MB |    9269 GB |    9267 GB |\n",
            "|       from large pool |    1832 MB |    2466 MB |    8925 GB |    8923 GB |\n",
            "|       from small pool |       9 MB |      13 MB |     344 GB |     344 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Allocations           |     592    |     595    |    3770 K  |    3769 K  |\n",
            "|       from large pool |     160    |     164    |    1314 K  |    1314 K  |\n",
            "|       from small pool |     432    |     516    |    2455 K  |    2455 K  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Active allocs         |     592    |     595    |    3770 K  |    3769 K  |\n",
            "|       from large pool |     160    |     164    |    1314 K  |    1314 K  |\n",
            "|       from small pool |     432    |     516    |    2455 K  |    2455 K  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| GPU reserved segments |      58    |      93    |    3024    |    2966    |\n",
            "|       from large pool |      29    |      29    |     411    |     382    |\n",
            "|       from small pool |      29    |      64    |    2613    |    2584    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Non-releasable allocs |      43    |      45    |    1702 K  |    1702 K  |\n",
            "|       from large pool |      19    |      21    |     829 K  |     829 K  |\n",
            "|       from small pool |      24    |      27    |     872 K  |     872 K  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Oversize allocations  |       0    |       0    |       0    |       0    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Oversize GPU segments |       0    |       0    |       0    |       0    |\n",
            "|===========================================================================|\n",
            "\n",
            "2022-03-10 05:35:44 | WARNING | fairseq.trainer | attempting to recover from OOM in forward/backward pass\n",
            "epoch 005:  83% 589/712 [01:59<00:22,  5.52it/s, loss=3.003, ppl=8.02, wps=14515.2, ups=5.48, wpb=2648.3, bsz=126.7, num_updates=3300, lr=0.0005, gnorm=1.058, train_wall=17, gb_free=10.6, wall=698]2022-03-10 05:35:55 | WARNING | fairseq.trainer | OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 1.76 GiB (GPU 0; 11.17 GiB total capacity; 9.04 GiB already allocated; 11.81 MiB free; 10.64 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
            "epoch 005:  83% 590/712 [01:59<00:33,  3.69it/s, loss=3.003, ppl=8.02, wps=14515.2, ups=5.48, wpb=2648.3, bsz=126.7, num_updates=3300, lr=0.0005, gnorm=1.058, train_wall=17, gb_free=10.6, wall=698]2022-03-10 05:35:55 | WARNING | fairseq.trainer | |===========================================================================|\n",
            "|                  PyTorch CUDA memory summary, device ID 0                 |\n",
            "|---------------------------------------------------------------------------|\n",
            "|            CUDA OOMs: 65           |        cudaMalloc retries: 122       |\n",
            "|===========================================================================|\n",
            "|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Allocated memory      |    9257 MB |    9326 MB |    9228 GB |    9219 GB |\n",
            "|       from large pool |    9207 MB |    9277 MB |    8907 GB |    8898 GB |\n",
            "|       from small pool |      49 MB |      60 MB |     320 GB |     320 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Active memory         |    9257 MB |    9326 MB |    9228 GB |    9219 GB |\n",
            "|       from large pool |    9207 MB |    9277 MB |    8907 GB |    8898 GB |\n",
            "|       from small pool |      49 MB |      60 MB |     320 GB |     320 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| GPU reserved memory   |   10896 MB |   10896 MB |  249530 MB |  238634 MB |\n",
            "|       from large pool |   10834 MB |   10834 MB |  244224 MB |  233390 MB |\n",
            "|       from small pool |      62 MB |     138 MB |    5306 MB |    5244 MB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Non-releasable memory |    1638 MB |    1801 MB |    9431 GB |    9429 GB |\n",
            "|       from large pool |    1626 MB |    1788 MB |    9081 GB |    9079 GB |\n",
            "|       from small pool |      12 MB |      25 MB |     350 GB |     350 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Allocations           |     593    |     595    |    3830 K  |    3829 K  |\n",
            "|       from large pool |     160    |     164    |    1335 K  |    1334 K  |\n",
            "|       from small pool |     433    |     516    |    2495 K  |    2494 K  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Active allocs         |     593    |     595    |    3830 K  |    3829 K  |\n",
            "|       from large pool |     160    |     164    |    1335 K  |    1334 K  |\n",
            "|       from small pool |     433    |     516    |    2495 K  |    2494 K  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| GPU reserved segments |      61    |      98    |    3066    |    3005    |\n",
            "|       from large pool |      30    |      30    |     413    |     383    |\n",
            "|       from small pool |      31    |      69    |    2653    |    2622    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Non-releasable allocs |      53    |      53    |    1729 K  |    1728 K  |\n",
            "|       from large pool |      27    |      27    |     842 K  |     842 K  |\n",
            "|       from small pool |      26    |      45    |     886 K  |     886 K  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Oversize allocations  |       0    |       0    |       0    |       0    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Oversize GPU segments |       0    |       0    |       0    |       0    |\n",
            "|===========================================================================|\n",
            "\n",
            "2022-03-10 05:35:55 | WARNING | fairseq.trainer | attempting to recover from OOM in forward/backward pass\n",
            "epoch 005: 100% 711/712 [02:25<00:00,  5.33it/s, loss=3.177, ppl=9.04, wps=12919.4, ups=4.47, wpb=2892.4, bsz=126.9, num_updates=3400, lr=0.0005, gnorm=0.972, train_wall=21, gb_free=10.4, wall=720]2022-03-10 05:36:21 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "\n",
            "epoch 005 | valid on 'valid' subset:   0% 0/8 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 005 | valid on 'valid' subset:  12% 1/8 [00:00<00:01,  6.28it/s]\u001b[A\n",
            "epoch 005 | valid on 'valid' subset:  38% 3/8 [00:00<00:00, 10.35it/s]\u001b[A\n",
            "epoch 005 | valid on 'valid' subset:  62% 5/8 [00:00<00:00, 10.94it/s]\u001b[A\n",
            "epoch 005 | valid on 'valid' subset:  88% 7/8 [00:00<00:00,  9.88it/s]\u001b[A\n",
            "                                                                      \u001b[A2022-03-10 05:36:22 | INFO | valid | epoch 005 | valid on 'valid' subset | loss 3.691 | ppl 12.91 | wps 20393.6 | wpb 2800.1 | bsz 125 | num_updates 3495 | best_loss 3.691\n",
            "2022-03-10 05:36:22 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 5 @ 3495 updates\n",
            "2022-03-10 05:36:22 | INFO | fairseq.trainer | Saving checkpoint to /content/checkpoints/checkpoint5.pt\n",
            "2022-03-10 05:36:22 | INFO | fairseq.trainer | Finished saving checkpoint to /content/checkpoints/checkpoint5.pt\n",
            "2022-03-10 05:36:24 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/checkpoint5.pt (epoch 5 @ 3495 updates, score 3.691) (writing took 1.9776517630000399 seconds)\n",
            "2022-03-10 05:36:24 | INFO | fairseq_cli.train | end of epoch 5 (average epoch stats below)\n",
            "2022-03-10 05:36:24 | INFO | train | epoch 005 | loss 3.064 | ppl 8.37 | wps 12911.3 | ups 4.69 | wpb 2750.2 | bsz 127.7 | num_updates 3495 | lr 0.0005 | gnorm 0.954 | train_wall 136 | gb_free 10.1 | wall 742\n",
            "2022-03-10 05:36:24 | INFO | fairseq.data.iterators | grouped total_num_itrs = 712\n",
            "epoch 006:   0% 0/712 [00:00<?, ?it/s]2022-03-10 05:36:24 | INFO | fairseq.trainer | begin training epoch 6\n",
            "2022-03-10 05:36:24 | INFO | fairseq_cli.train | Start iterating over samples\n",
            "epoch 006:  13% 90/712 [00:18<01:45,  5.89it/s, loss=3.077, ppl=8.44, wps=11834.1, ups=4.31, wpb=2748.1, bsz=128, num_updates=3500, lr=0.0005, gnorm=0.959, train_wall=19, gb_free=9.9, wall=743]2022-03-10 05:36:43 | WARNING | fairseq.trainer | OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 1.76 GiB (GPU 0; 11.17 GiB total capacity; 9.04 GiB already allocated; 11.81 MiB free; 10.64 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
            "2022-03-10 05:36:43 | WARNING | fairseq.trainer | |===========================================================================|\n",
            "|                  PyTorch CUDA memory summary, device ID 0                 |\n",
            "|---------------------------------------------------------------------------|\n",
            "|            CUDA OOMs: 66           |        cudaMalloc retries: 126       |\n",
            "|===========================================================================|\n",
            "|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Allocated memory      |    9260 MB |    9329 MB |    9819 GB |    9810 GB |\n",
            "|       from large pool |    9210 MB |    9280 MB |    9478 GB |    9469 GB |\n",
            "|       from small pool |      49 MB |      60 MB |     340 GB |     340 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Active memory         |    9260 MB |    9329 MB |    9819 GB |    9810 GB |\n",
            "|       from large pool |    9210 MB |    9280 MB |    9478 GB |    9469 GB |\n",
            "|       from small pool |      49 MB |      60 MB |     340 GB |     340 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| GPU reserved memory   |   10896 MB |   10896 MB |  269438 MB |  258542 MB |\n",
            "|       from large pool |   10836 MB |   10836 MB |  263980 MB |  253144 MB |\n",
            "|       from small pool |      60 MB |     138 MB |    5458 MB |    5398 MB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Non-releasable memory |    1635 MB |    1635 MB |   10019 GB |   10017 GB |\n",
            "|       from large pool |    1625 MB |    1625 MB |    9647 GB |    9645 GB |\n",
            "|       from small pool |      10 MB |      19 MB |     372 GB |     372 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Allocations           |     593    |     595    |    4072 K  |    4071 K  |\n",
            "|       from large pool |     160    |     164    |    1420 K  |    1420 K  |\n",
            "|       from small pool |     433    |     516    |    2651 K  |    2651 K  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Active allocs         |     593    |     595    |    4072 K  |    4071 K  |\n",
            "|       from large pool |     160    |     164    |    1420 K  |    1420 K  |\n",
            "|       from small pool |     433    |     516    |    2651 K  |    2651 K  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| GPU reserved segments |      57    |      96    |    3168    |    3111    |\n",
            "|       from large pool |      27    |      27    |     439    |     412    |\n",
            "|       from small pool |      30    |      69    |    2729    |    2699    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Non-releasable allocs |      50    |      50    |    1836 K  |    1836 K  |\n",
            "|       from large pool |      25    |      25    |     892 K  |     892 K  |\n",
            "|       from small pool |      25    |      38    |     943 K  |     943 K  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Oversize allocations  |       0    |       0    |       0    |       0    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Oversize GPU segments |       0    |       0    |       0    |       0    |\n",
            "|===========================================================================|\n",
            "\n",
            "2022-03-10 05:36:43 | WARNING | fairseq.trainer | attempting to recover from OOM in forward/backward pass\n",
            "epoch 006:  24% 172/712 [00:34<01:52,  4.78it/s, loss=2.803, ppl=6.98, wps=13543.2, ups=4.46, wpb=3038.5, bsz=128, num_updates=3600, lr=0.0005, gnorm=0.867, train_wall=21, gb_free=4.3, wall=766]2022-03-10 05:37:00 | WARNING | fairseq.trainer | OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 1.87 GiB (GPU 0; 11.17 GiB total capacity; 8.58 GiB already allocated; 1.47 GiB free; 9.18 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
            "2022-03-10 05:37:00 | WARNING | fairseq.trainer | |===========================================================================|\n",
            "|                  PyTorch CUDA memory summary, device ID 0                 |\n",
            "|---------------------------------------------------------------------------|\n",
            "|            CUDA OOMs: 67           |        cudaMalloc retries: 129       |\n",
            "|===========================================================================|\n",
            "|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Allocated memory      |    8781 MB |    8849 MB |   10024 GB |   10015 GB |\n",
            "|       from large pool |    8732 MB |    8800 MB |    9675 GB |    9667 GB |\n",
            "|       from small pool |      49 MB |      61 MB |     348 GB |     348 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Active memory         |    8781 MB |    8849 MB |   10024 GB |   10015 GB |\n",
            "|       from large pool |    8732 MB |    8800 MB |    9675 GB |    9667 GB |\n",
            "|       from small pool |      49 MB |      61 MB |     348 GB |     348 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| GPU reserved memory   |    9400 MB |   10572 MB |  283584 MB |  274184 MB |\n",
            "|       from large pool |    9342 MB |   10458 MB |  278072 MB |  268730 MB |\n",
            "|       from small pool |      58 MB |     114 MB |    5512 MB |    5454 MB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Non-releasable memory |  633397 KB |    1612 MB |   10253 GB |   10252 GB |\n",
            "|       from large pool |  624288 KB |    1602 MB |    9872 GB |    9871 GB |\n",
            "|       from small pool |    9109 KB |      12 MB |     380 GB |     380 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Allocations           |     593    |     595    |    4163 K  |    4163 K  |\n",
            "|       from large pool |     160    |     164    |    1452 K  |    1452 K  |\n",
            "|       from small pool |     433    |     516    |    2711 K  |    2711 K  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Active allocs         |     593    |     595    |    4163 K  |    4163 K  |\n",
            "|       from large pool |     160    |     164    |    1452 K  |    1452 K  |\n",
            "|       from small pool |     433    |     516    |    2711 K  |    2711 K  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| GPU reserved segments |      87    |     139    |    3273    |    3186    |\n",
            "|       from large pool |      58    |      82    |     517    |     459    |\n",
            "|       from small pool |      29    |      57    |    2756    |    2727    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Non-releasable allocs |      54    |      56    |    1879 K  |    1879 K  |\n",
            "|       from large pool |      34    |      34    |     914 K  |     914 K  |\n",
            "|       from small pool |      20    |      25    |     965 K  |     965 K  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Oversize allocations  |       0    |       0    |       0    |       0    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Oversize GPU segments |       0    |       0    |       0    |       0    |\n",
            "|===========================================================================|\n",
            "\n",
            "2022-03-10 05:37:00 | WARNING | fairseq.trainer | attempting to recover from OOM in forward/backward pass\n",
            "epoch 006:  27% 192/712 [00:39<01:43,  5.03it/s, loss=2.803, ppl=6.98, wps=13543.2, ups=4.46, wpb=3038.5, bsz=128, num_updates=3600, lr=0.0005, gnorm=0.867, train_wall=21, gb_free=4.3, wall=766]2022-03-10 05:37:04 | WARNING | fairseq.trainer | OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 1.76 GiB (GPU 0; 11.17 GiB total capacity; 9.79 GiB already allocated; 367.81 MiB free; 10.29 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
            "2022-03-10 05:37:04 | WARNING | fairseq.trainer | |===========================================================================|\n",
            "|                  PyTorch CUDA memory summary, device ID 0                 |\n",
            "|---------------------------------------------------------------------------|\n",
            "|            CUDA OOMs: 68           |        cudaMalloc retries: 132       |\n",
            "|===========================================================================|\n",
            "|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Allocated memory      |    8231 MB |   10028 MB |   10086 GB |   10078 GB |\n",
            "|       from large pool |    8182 MB |    9979 MB |    9736 GB |    9728 GB |\n",
            "|       from small pool |      48 MB |      60 MB |     350 GB |     350 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Active memory         |    8231 MB |   10028 MB |   10086 GB |   10078 GB |\n",
            "|       from large pool |    8182 MB |    9979 MB |    9736 GB |    9728 GB |\n",
            "|       from small pool |      48 MB |      60 MB |     350 GB |     350 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| GPU reserved memory   |   10540 MB |   10540 MB |  287974 MB |  277434 MB |\n",
            "|       from large pool |   10480 MB |   10480 MB |  282350 MB |  271870 MB |\n",
            "|       from small pool |      60 MB |     128 MB |    5624 MB |    5564 MB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Non-releasable memory |  522606 KB |    1285 MB |   10318 GB |   10317 GB |\n",
            "|       from large pool |  511311 KB |    1273 MB |    9935 GB |    9935 GB |\n",
            "|       from small pool |   11295 KB |      19 MB |     382 GB |     382 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Allocations           |     592    |     595    |    4185 K  |    4184 K  |\n",
            "|       from large pool |     160    |     164    |    1459 K  |    1459 K  |\n",
            "|       from small pool |     432    |     516    |    2725 K  |    2725 K  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Active allocs         |     592    |     595    |    4185 K  |    4184 K  |\n",
            "|       from large pool |     160    |     164    |    1459 K  |    1459 K  |\n",
            "|       from small pool |     432    |     516    |    2725 K  |    2725 K  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| GPU reserved segments |      87    |     122    |    3332    |    3245    |\n",
            "|       from large pool |      57    |      58    |     520    |     463    |\n",
            "|       from small pool |      30    |      64    |    2812    |    2782    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Non-releasable allocs |      69    |      70    |    1889 K  |    1888 K  |\n",
            "|       from large pool |      44    |      45    |     918 K  |     918 K  |\n",
            "|       from small pool |      25    |      44    |     970 K  |     970 K  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Oversize allocations  |       0    |       0    |       0    |       0    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Oversize GPU segments |       0    |       0    |       0    |       0    |\n",
            "|===========================================================================|\n",
            "\n",
            "2022-03-10 05:37:04 | WARNING | fairseq.trainer | attempting to recover from OOM in forward/backward pass\n",
            "epoch 006:  39% 279/712 [00:57<01:22,  5.26it/s, loss=2.734, ppl=6.65, wps=13300.6, ups=5.11, wpb=2600.7, bsz=126.7, num_updates=3700, lr=0.0005, gnorm=1.031, train_wall=17, gb_free=10.3, wall=785]2022-03-10 05:37:22 | WARNING | fairseq.trainer | OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 1.52 GiB (GPU 0; 11.17 GiB total capacity; 8.51 GiB already allocated; 559.81 MiB free; 10.11 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
            "2022-03-10 05:37:22 | WARNING | fairseq.trainer | |===========================================================================|\n",
            "|                  PyTorch CUDA memory summary, device ID 0                 |\n",
            "|---------------------------------------------------------------------------|\n",
            "|            CUDA OOMs: 69           |        cudaMalloc retries: 135       |\n",
            "|===========================================================================|\n",
            "|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Allocated memory      |    7164 MB |    8716 MB |   10319 GB |   10312 GB |\n",
            "|       from large pool |    7115 MB |    8667 MB |    9961 GB |    9954 GB |\n",
            "|       from small pool |      48 MB |      61 MB |     358 GB |     358 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Active memory         |    7164 MB |    8716 MB |   10319 GB |   10312 GB |\n",
            "|       from large pool |    7115 MB |    8667 MB |    9961 GB |    9954 GB |\n",
            "|       from small pool |      48 MB |      61 MB |     358 GB |     358 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| GPU reserved memory   |   10348 MB |   10348 MB |  293560 MB |  283212 MB |\n",
            "|       from large pool |   10290 MB |   10290 MB |  287788 MB |  277498 MB |\n",
            "|       from small pool |      58 MB |     134 MB |    5772 MB |    5714 MB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Non-releasable memory |    1631 MB |    1631 MB |   10604 GB |   10602 GB |\n",
            "|       from large pool |    1622 MB |    1622 MB |   10213 GB |   10211 GB |\n",
            "|       from small pool |       9 MB |      12 MB |     391 GB |     391 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Allocations           |     592    |     595    |    4282 K  |    4282 K  |\n",
            "|       from large pool |     160    |     164    |    1493 K  |    1493 K  |\n",
            "|       from small pool |     432    |     516    |    2788 K  |    2788 K  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Active allocs         |     592    |     595    |    4282 K  |    4282 K  |\n",
            "|       from large pool |     160    |     164    |    1493 K  |    1493 K  |\n",
            "|       from small pool |     432    |     516    |    2788 K  |    2788 K  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| GPU reserved segments |      80    |     118    |    3410    |    3330    |\n",
            "|       from large pool |      51    |      51    |     524    |     473    |\n",
            "|       from small pool |      29    |      67    |    2886    |    2857    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Non-releasable allocs |      60    |      60    |    1933 K  |    1933 K  |\n",
            "|       from large pool |      36    |      36    |     941 K  |     941 K  |\n",
            "|       from small pool |      24    |      27    |     992 K  |     992 K  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Oversize allocations  |       0    |       0    |       0    |       0    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Oversize GPU segments |       0    |       0    |       0    |       0    |\n",
            "|===========================================================================|\n",
            "\n",
            "2022-03-10 05:37:22 | WARNING | fairseq.trainer | attempting to recover from OOM in forward/backward pass\n",
            "epoch 006:  49% 350/712 [01:09<00:55,  6.47it/s, loss=2.861, ppl=7.27, wps=13877.8, ups=5.05, wpb=2746.2, bsz=126.9, num_updates=3800, lr=0.0005, gnorm=0.989, train_wall=19, gb_free=10.5, wall=805]2022-03-10 05:37:34 | WARNING | fairseq.trainer | OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 50.00 MiB (GPU 0; 11.17 GiB total capacity; 10.35 GiB already allocated; 15.81 MiB free; 10.64 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
            "2022-03-10 05:37:34 | WARNING | fairseq.trainer | |===========================================================================|\n",
            "|                  PyTorch CUDA memory summary, device ID 0                 |\n",
            "|---------------------------------------------------------------------------|\n",
            "|            CUDA OOMs: 70           |        cudaMalloc retries: 137       |\n",
            "|===========================================================================|\n",
            "|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Allocated memory      |   10594 MB |   10594 MB |   10472 GB |   10461 GB |\n",
            "|       from large pool |   10543 MB |   10543 MB |   10106 GB |   10096 GB |\n",
            "|       from small pool |      50 MB |      60 MB |     365 GB |     365 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Active memory         |   10594 MB |   10594 MB |   10472 GB |   10461 GB |\n",
            "|       from large pool |   10543 MB |   10543 MB |   10106 GB |   10096 GB |\n",
            "|       from small pool |      50 MB |      60 MB |     365 GB |     365 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| GPU reserved memory   |   10892 MB |   10892 MB |  295722 MB |  284830 MB |\n",
            "|       from large pool |   10830 MB |   10830 MB |  289880 MB |  279050 MB |\n",
            "|       from small pool |      62 MB |     128 MB |    5842 MB |    5780 MB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Non-releasable memory |  304556 KB |    2328 MB |   10801 GB |   10801 GB |\n",
            "|       from large pool |  293023 KB |    2315 MB |   10401 GB |   10401 GB |\n",
            "|       from small pool |   11533 KB |      39 MB |     399 GB |     399 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Allocations           |     590    |     590    |    4362 K  |    4362 K  |\n",
            "|       from large pool |     160    |     160    |    1520 K  |    1519 K  |\n",
            "|       from small pool |     430    |     516    |    2842 K  |    2842 K  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Active allocs         |     590    |     590    |    4362 K  |    4362 K  |\n",
            "|       from large pool |     160    |     160    |    1520 K  |    1519 K  |\n",
            "|       from small pool |     430    |     516    |    2842 K  |    2842 K  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| GPU reserved segments |      87    |     119    |    3451    |    3364    |\n",
            "|       from large pool |      56    |      56    |     530    |     474    |\n",
            "|       from small pool |      31    |      64    |    2921    |    2890    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Non-releasable allocs |      70    |      71    |    1970 K  |    1969 K  |\n",
            "|       from large pool |      45    |      46    |     958 K  |     958 K  |\n",
            "|       from small pool |      25    |      46    |    1011 K  |    1011 K  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Oversize allocations  |       0    |       0    |       0    |       0    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Oversize GPU segments |       0    |       0    |       0    |       0    |\n",
            "|===========================================================================|\n",
            "\n",
            "2022-03-10 05:37:34 | WARNING | fairseq.trainer | attempting to recover from OOM in forward/backward pass\n",
            "epoch 006:  57% 409/712 [01:23<00:45,  6.68it/s, loss=2.861, ppl=7.27, wps=13877.8, ups=5.05, wpb=2746.2, bsz=126.9, num_updates=3800, lr=0.0005, gnorm=0.989, train_wall=19, gb_free=10.5, wall=805]2022-03-10 05:37:48 | WARNING | fairseq.trainer | OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 2.51 GiB (GPU 0; 11.17 GiB total capacity; 10.39 GiB already allocated; 133.81 MiB free; 10.52 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
            "2022-03-10 05:37:48 | WARNING | fairseq.trainer | |===========================================================================|\n",
            "|                  PyTorch CUDA memory summary, device ID 0                 |\n",
            "|---------------------------------------------------------------------------|\n",
            "|            CUDA OOMs: 71           |        cudaMalloc retries: 141       |\n",
            "|===========================================================================|\n",
            "|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Allocated memory      |   10601 MB |   10643 MB |   10647 GB |   10637 GB |\n",
            "|       from large pool |   10550 MB |   10592 MB |   10276 GB |   10265 GB |\n",
            "|       from small pool |      51 MB |      60 MB |     371 GB |     371 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Active memory         |   10601 MB |   10643 MB |   10647 GB |   10637 GB |\n",
            "|       from large pool |   10550 MB |   10592 MB |   10276 GB |   10265 GB |\n",
            "|       from small pool |      51 MB |      60 MB |     371 GB |     371 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| GPU reserved memory   |   10774 MB |   10876 MB |  310522 MB |  299748 MB |\n",
            "|       from large pool |   10716 MB |   10744 MB |  304540 MB |  293824 MB |\n",
            "|       from small pool |      58 MB |     132 MB |    5982 MB |    5924 MB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Non-releasable memory |  176240 KB |    1953 MB |   10968 GB |   10968 GB |\n",
            "|       from large pool |  169128 KB |    1943 MB |   10563 GB |   10562 GB |\n",
            "|       from small pool |    7112 KB |      16 MB |     405 GB |     405 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Allocations           |     593    |     595    |    4427 K  |    4426 K  |\n",
            "|       from large pool |     162    |     163    |    1542 K  |    1542 K  |\n",
            "|       from small pool |     431    |     516    |    2884 K  |    2884 K  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Active allocs         |     593    |     595    |    4427 K  |    4426 K  |\n",
            "|       from large pool |     162    |     163    |    1542 K  |    1542 K  |\n",
            "|       from small pool |     431    |     516    |    2884 K  |    2884 K  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| GPU reserved segments |      70    |     111    |    3555    |    3485    |\n",
            "|       from large pool |      41    |      45    |     564    |     523    |\n",
            "|       from small pool |      29    |      66    |    2991    |    2962    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Non-releasable allocs |      51    |      51    |    1999 K  |    1999 K  |\n",
            "|       from large pool |      28    |      28    |     972 K  |     972 K  |\n",
            "|       from small pool |      23    |      36    |    1026 K  |    1026 K  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Oversize allocations  |       0    |       0    |       0    |       0    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Oversize GPU segments |       0    |       0    |       0    |       0    |\n",
            "|===========================================================================|\n",
            "\n",
            "2022-03-10 05:37:48 | WARNING | fairseq.trainer | attempting to recover from OOM in forward/backward pass\n",
            "epoch 006:  66% 467/712 [01:34<01:10,  3.46it/s, loss=2.86, ppl=7.26, wps=12516.4, ups=4.66, wpb=2687.6, bsz=128, num_updates=3900, lr=0.0005, gnorm=0.972, train_wall=20, gb_free=0.8, wall=827]2022-03-10 05:37:59 | WARNING | fairseq.trainer | OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 1.70 GiB (GPU 0; 11.17 GiB total capacity; 8.03 GiB already allocated; 1.07 GiB free; 9.58 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
            "2022-03-10 05:37:59 | WARNING | fairseq.trainer | |===========================================================================|\n",
            "|                  PyTorch CUDA memory summary, device ID 0                 |\n",
            "|---------------------------------------------------------------------------|\n",
            "|            CUDA OOMs: 72           |        cudaMalloc retries: 143       |\n",
            "|===========================================================================|\n",
            "|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Allocated memory      |    8220 MB |    8283 MB |   10789 GB |   10781 GB |\n",
            "|       from large pool |    8171 MB |    8234 MB |   10412 GB |   10404 GB |\n",
            "|       from small pool |      49 MB |      61 MB |     377 GB |     377 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Active memory         |    8220 MB |    8283 MB |   10789 GB |   10781 GB |\n",
            "|       from large pool |    8171 MB |    8234 MB |   10412 GB |   10404 GB |\n",
            "|       from small pool |      49 MB |      61 MB |     377 GB |     377 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| GPU reserved memory   |    9810 MB |   10844 MB |  314080 MB |  304270 MB |\n",
            "|       from large pool |    9752 MB |   10716 MB |  308028 MB |  298276 MB |\n",
            "|       from small pool |      58 MB |     128 MB |    6052 MB |    5994 MB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Non-releasable memory |    1589 MB |    2047 MB |   11111 GB |   11109 GB |\n",
            "|       from large pool |    1580 MB |    2038 MB |   10699 GB |   10698 GB |\n",
            "|       from small pool |       8 MB |      12 MB |     411 GB |     411 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Allocations           |     593    |     595    |    4491 K  |    4491 K  |\n",
            "|       from large pool |     160    |     164    |    1564 K  |    1564 K  |\n",
            "|       from small pool |     433    |     516    |    2927 K  |    2926 K  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Active allocs         |     593    |     595    |    4491 K  |    4491 K  |\n",
            "|       from large pool |     160    |     164    |    1564 K  |    1564 K  |\n",
            "|       from small pool |     433    |     516    |    2927 K  |    2926 K  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| GPU reserved segments |      69    |     105    |    3592    |    3523    |\n",
            "|       from large pool |      40    |      41    |     566    |     526    |\n",
            "|       from small pool |      29    |      64    |    3026    |    2997    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Non-releasable allocs |      49    |      49    |    2028 K  |    2028 K  |\n",
            "|       from large pool |      28    |      28    |     985 K  |     985 K  |\n",
            "|       from small pool |      21    |      32    |    1042 K  |    1042 K  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Oversize allocations  |       0    |       0    |       0    |       0    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Oversize GPU segments |       0    |       0    |       0    |       0    |\n",
            "|===========================================================================|\n",
            "\n",
            "2022-03-10 05:37:59 | WARNING | fairseq.trainer | attempting to recover from OOM in forward/backward pass\n",
            "epoch 006:  72% 512/712 [01:44<00:32,  6.21it/s, loss=2.831, ppl=7.12, wps=13403.7, ups=4.84, wpb=2766.7, bsz=128, num_updates=4000, lr=0.0005, gnorm=0.968, train_wall=20, gb_free=10.2, wall=847]2022-03-10 05:38:09 | WARNING | fairseq.trainer | OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 1.44 GiB (GPU 0; 11.17 GiB total capacity; 7.98 GiB already allocated; 1.27 GiB free; 9.38 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
            "2022-03-10 05:38:09 | WARNING | fairseq.trainer | |===========================================================================|\n",
            "|                  PyTorch CUDA memory summary, device ID 0                 |\n",
            "|---------------------------------------------------------------------------|\n",
            "|            CUDA OOMs: 73           |        cudaMalloc retries: 145       |\n",
            "|===========================================================================|\n",
            "|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Allocated memory      |    6699 MB |    8175 MB |   10921 GB |   10914 GB |\n",
            "|       from large pool |    6651 MB |    8127 MB |   10539 GB |   10533 GB |\n",
            "|       from small pool |      48 MB |      60 MB |     381 GB |     381 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Active memory         |    6699 MB |    8175 MB |   10921 GB |   10914 GB |\n",
            "|       from large pool |    6651 MB |    8127 MB |   10539 GB |   10533 GB |\n",
            "|       from small pool |      48 MB |      60 MB |     381 GB |     381 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| GPU reserved memory   |    9606 MB |    9680 MB |  317612 MB |  308006 MB |\n",
            "|       from large pool |    9548 MB |    9548 MB |  311416 MB |  301868 MB |\n",
            "|       from small pool |      58 MB |     132 MB |    6196 MB |    6138 MB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Non-releasable memory |    1430 MB |    1474 MB |   11242 GB |   11240 GB |\n",
            "|       from large pool |    1420 MB |    1464 MB |   10825 GB |   10824 GB |\n",
            "|       from small pool |       9 MB |      15 MB |     416 GB |     416 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Allocations           |     590    |     593    |    4541 K  |    4541 K  |\n",
            "|       from large pool |     160    |     164    |    1582 K  |    1582 K  |\n",
            "|       from small pool |     430    |     516    |    2959 K  |    2958 K  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Active allocs         |     590    |     593    |    4541 K  |    4541 K  |\n",
            "|       from large pool |     160    |     164    |    1582 K  |    1582 K  |\n",
            "|       from small pool |     430    |     516    |    2959 K  |    2958 K  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| GPU reserved segments |      64    |     101    |    3667    |    3603    |\n",
            "|       from large pool |      35    |      35    |     569    |     534    |\n",
            "|       from small pool |      29    |      66    |    3098    |    3069    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Non-releasable allocs |      48    |      48    |    2051 K  |    2051 K  |\n",
            "|       from large pool |      26    |      26    |     997 K  |     997 K  |\n",
            "|       from small pool |      22    |      32    |    1054 K  |    1054 K  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Oversize allocations  |       0    |       0    |       0    |       0    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Oversize GPU segments |       0    |       0    |       0    |       0    |\n",
            "|===========================================================================|\n",
            "\n",
            "2022-03-10 05:38:09 | WARNING | fairseq.trainer | attempting to recover from OOM in forward/backward pass\n",
            "epoch 006:  73% 520/712 [01:47<00:32,  5.99it/s, loss=2.831, ppl=7.12, wps=13403.7, ups=4.84, wpb=2766.7, bsz=128, num_updates=4000, lr=0.0005, gnorm=0.968, train_wall=20, gb_free=10.2, wall=847]2022-03-10 05:38:12 | WARNING | fairseq.trainer | OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 610.00 MiB (GPU 0; 11.17 GiB total capacity; 9.33 GiB already allocated; 233.81 MiB free; 10.42 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
            "2022-03-10 05:38:12 | WARNING | fairseq.trainer | |===========================================================================|\n",
            "|                  PyTorch CUDA memory summary, device ID 0                 |\n",
            "|---------------------------------------------------------------------------|\n",
            "|            CUDA OOMs: 74           |        cudaMalloc retries: 146       |\n",
            "|===========================================================================|\n",
            "|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Allocated memory      |    9551 MB |    9551 MB |   10958 GB |   10949 GB |\n",
            "|       from large pool |    9501 MB |    9501 MB |   10576 GB |   10567 GB |\n",
            "|       from small pool |      50 MB |      60 MB |     382 GB |     381 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Active memory         |    9551 MB |    9551 MB |   10958 GB |   10949 GB |\n",
            "|       from large pool |    9501 MB |    9501 MB |   10576 GB |   10567 GB |\n",
            "|       from small pool |      50 MB |      60 MB |     382 GB |     381 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| GPU reserved memory   |   10674 MB |   10718 MB |  320200 MB |  309526 MB |\n",
            "|       from large pool |   10616 MB |   10616 MB |  313960 MB |  303344 MB |\n",
            "|       from small pool |      58 MB |     102 MB |    6240 MB |    6182 MB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Non-releasable memory |    1122 MB |    2925 MB |   11276 GB |   11275 GB |\n",
            "|       from large pool |    1114 MB |    2916 MB |   10859 GB |   10858 GB |\n",
            "|       from small pool |       7 MB |      19 MB |     417 GB |     417 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Allocations           |     569    |     569    |    4550 K  |    4549 K  |\n",
            "|       from large pool |     141    |     141    |    1585 K  |    1585 K  |\n",
            "|       from small pool |     428    |     516    |    2964 K  |    2964 K  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Active allocs         |     569    |     569    |    4550 K  |    4549 K  |\n",
            "|       from large pool |     141    |     141    |    1585 K  |    1585 K  |\n",
            "|       from small pool |     428    |     516    |    2964 K  |    2964 K  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| GPU reserved segments |      66    |      88    |    3692    |    3626    |\n",
            "|       from large pool |      37    |      37    |     572    |     535    |\n",
            "|       from small pool |      29    |      51    |    3120    |    3091    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Non-releasable allocs |      49    |      53    |    2054 K  |    2054 K  |\n",
            "|       from large pool |      27    |      29    |     998 K  |     998 K  |\n",
            "|       from small pool |      22    |      49    |    1055 K  |    1055 K  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Oversize allocations  |       0    |       0    |       0    |       0    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Oversize GPU segments |       0    |       0    |       0    |       0    |\n",
            "|===========================================================================|\n",
            "\n",
            "2022-03-10 05:38:12 | WARNING | fairseq.trainer | attempting to recover from OOM in forward/backward pass\n",
            "epoch 006:  81% 576/712 [02:01<00:26,  5.12it/s, loss=2.831, ppl=7.12, wps=13403.7, ups=4.84, wpb=2766.7, bsz=128, num_updates=4000, lr=0.0005, gnorm=0.968, train_wall=20, gb_free=10.2, wall=847]2022-03-10 05:38:26 | WARNING | fairseq.trainer | OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 1.54 GiB (GPU 0; 11.17 GiB total capacity; 8.23 GiB already allocated; 1.43 GiB free; 9.22 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
            "2022-03-10 05:38:26 | WARNING | fairseq.trainer | |===========================================================================|\n",
            "|                  PyTorch CUDA memory summary, device ID 0                 |\n",
            "|---------------------------------------------------------------------------|\n",
            "|            CUDA OOMs: 75           |        cudaMalloc retries: 148       |\n",
            "|===========================================================================|\n",
            "|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Allocated memory      |    6855 MB |    8430 MB |   11156 GB |   11149 GB |\n",
            "|       from large pool |    6797 MB |    8372 MB |   10770 GB |   10763 GB |\n",
            "|       from small pool |      58 MB |      60 MB |     386 GB |     386 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Active memory         |    6855 MB |    8430 MB |   11156 GB |   11149 GB |\n",
            "|       from large pool |    6797 MB |    8372 MB |   10770 GB |   10763 GB |\n",
            "|       from small pool |      58 MB |      60 MB |     386 GB |     386 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| GPU reserved memory   |    9446 MB |   10746 MB |  321848 MB |  312402 MB |\n",
            "|       from large pool |    9384 MB |   10616 MB |  315536 MB |  306152 MB |\n",
            "|       from small pool |      62 MB |     130 MB |    6312 MB |    6250 MB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Non-releasable memory |    1014 MB |    1089 MB |   11478 GB |   11477 GB |\n",
            "|       from large pool |    1010 MB |    1084 MB |   11056 GB |   11055 GB |\n",
            "|       from small pool |       3 MB |      18 MB |     421 GB |     421 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Allocations           |     592    |     595    |    4612 K  |    4612 K  |\n",
            "|       from large pool |     150    |     154    |    1608 K  |    1608 K  |\n",
            "|       from small pool |     442    |     516    |    3004 K  |    3003 K  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Active allocs         |     592    |     595    |    4612 K  |    4612 K  |\n",
            "|       from large pool |     150    |     154    |    1608 K  |    1608 K  |\n",
            "|       from small pool |     442    |     516    |    3004 K  |    3003 K  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| GPU reserved segments |      67    |     102    |    3729    |    3662    |\n",
            "|       from large pool |      36    |      37    |     573    |     537    |\n",
            "|       from small pool |      31    |      65    |    3156    |    3125    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Non-releasable allocs |      48    |      49    |    2082 K  |    2082 K  |\n",
            "|       from large pool |      22    |      23    |    1012 K  |    1012 K  |\n",
            "|       from small pool |      26    |      43    |    1069 K  |    1069 K  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Oversize allocations  |       0    |       0    |       0    |       0    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Oversize GPU segments |       0    |       0    |       0    |       0    |\n",
            "|===========================================================================|\n",
            "\n",
            "2022-03-10 05:38:26 | WARNING | fairseq.trainer | attempting to recover from OOM in forward/backward pass\n",
            "epoch 006:  83% 593/712 [02:05<00:30,  3.88it/s, loss=2.831, ppl=7.12, wps=13403.7, ups=4.84, wpb=2766.7, bsz=128, num_updates=4000, lr=0.0005, gnorm=0.968, train_wall=20, gb_free=10.2, wall=847]2022-03-10 05:38:30 | WARNING | fairseq.trainer | OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 1.83 GiB (GPU 0; 11.17 GiB total capacity; 8.20 GiB already allocated; 1.70 GiB free; 8.96 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
            "2022-03-10 05:38:30 | WARNING | fairseq.trainer | |===========================================================================|\n",
            "|                  PyTorch CUDA memory summary, device ID 0                 |\n",
            "|---------------------------------------------------------------------------|\n",
            "|            CUDA OOMs: 76           |        cudaMalloc retries: 150       |\n",
            "|===========================================================================|\n",
            "|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Allocated memory      |    8399 MB |    8464 MB |   11218 GB |   11209 GB |\n",
            "|       from large pool |    8342 MB |    8406 MB |   10830 GB |   10822 GB |\n",
            "|       from small pool |      57 MB |      60 MB |     387 GB |     387 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Active memory         |    8399 MB |    8464 MB |   11218 GB |   11209 GB |\n",
            "|       from large pool |    8342 MB |    8406 MB |   10830 GB |   10822 GB |\n",
            "|       from small pool |      57 MB |      60 MB |     387 GB |     387 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| GPU reserved memory   |    9170 MB |    9994 MB |  327720 MB |  318550 MB |\n",
            "|       from large pool |    9108 MB |    9888 MB |  321364 MB |  312256 MB |\n",
            "|       from small pool |      62 MB |     106 MB |    6356 MB |    6294 MB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Non-releasable memory |     770 MB |    1173 MB |   11535 GB |   11534 GB |\n",
            "|       from large pool |     765 MB |    1168 MB |   11112 GB |   11111 GB |\n",
            "|       from small pool |       4 MB |      13 MB |     423 GB |     422 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Allocations           |     593    |     595    |    4631 K  |    4630 K  |\n",
            "|       from large pool |     150    |     154    |    1615 K  |    1615 K  |\n",
            "|       from small pool |     443    |     516    |    3015 K  |    3015 K  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Active allocs         |     593    |     595    |    4631 K  |    4630 K  |\n",
            "|       from large pool |     150    |     154    |    1615 K  |    1615 K  |\n",
            "|       from small pool |     443    |     516    |    3015 K  |    3015 K  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| GPU reserved segments |      67    |      90    |    3755    |    3688    |\n",
            "|       from large pool |      36    |      37    |     577    |     541    |\n",
            "|       from small pool |      31    |      53    |    3178    |    3147    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Non-releasable allocs |      47    |      48    |    2090 K  |    2090 K  |\n",
            "|       from large pool |      25    |      25    |    1017 K  |    1017 K  |\n",
            "|       from small pool |      22    |      37    |    1073 K  |    1073 K  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Oversize allocations  |       0    |       0    |       0    |       0    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Oversize GPU segments |       0    |       0    |       0    |       0    |\n",
            "|===========================================================================|\n",
            "\n",
            "2022-03-10 05:38:30 | WARNING | fairseq.trainer | attempting to recover from OOM in forward/backward pass\n",
            "epoch 006:  90% 641/712 [02:14<00:11,  6.15it/s, loss=2.995, ppl=7.97, wps=11836, ups=4.07, wpb=2906.9, bsz=128, num_updates=4100, lr=0.0005, gnorm=0.993, train_wall=22, gb_free=10.5, wall=872]2022-03-10 05:38:39 | WARNING | fairseq.trainer | OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 1.44 GiB (GPU 0; 11.17 GiB total capacity; 7.69 GiB already allocated; 999.81 MiB free; 9.68 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
            "2022-03-10 05:38:39 | WARNING | fairseq.trainer | |===========================================================================|\n",
            "|                  PyTorch CUDA memory summary, device ID 0                 |\n",
            "|---------------------------------------------------------------------------|\n",
            "|            CUDA OOMs: 77           |        cudaMalloc retries: 153       |\n",
            "|===========================================================================|\n",
            "|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Allocated memory      |    6400 MB |    7876 MB |   11323 GB |   11316 GB |\n",
            "|       from large pool |    6351 MB |    7827 MB |   10930 GB |   10924 GB |\n",
            "|       from small pool |      48 MB |      60 MB |     392 GB |     392 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Active memory         |    6400 MB |    7876 MB |   11323 GB |   11316 GB |\n",
            "|       from large pool |    6351 MB |    7827 MB |   10930 GB |   10924 GB |\n",
            "|       from small pool |      48 MB |      60 MB |     392 GB |     392 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| GPU reserved memory   |    9908 MB |    9908 MB |  331940 MB |  322032 MB |\n",
            "|       from large pool |    9850 MB |    9850 MB |  325440 MB |  315590 MB |\n",
            "|       from small pool |      58 MB |     128 MB |    6500 MB |    6442 MB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Non-releasable memory |    2031 MB |    2031 MB |   11642 GB |   11640 GB |\n",
            "|       from large pool |    2022 MB |    2022 MB |   11213 GB |   11211 GB |\n",
            "|       from small pool |       9 MB |      18 MB |     428 GB |     428 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Allocations           |     592    |     595    |    4684 K  |    4683 K  |\n",
            "|       from large pool |     160    |     164    |    1632 K  |    1632 K  |\n",
            "|       from small pool |     432    |     516    |    3051 K  |    3050 K  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Active allocs         |     592    |     595    |    4684 K  |    4683 K  |\n",
            "|       from large pool |     160    |     164    |    1632 K  |    1632 K  |\n",
            "|       from small pool |     432    |     516    |    3051 K  |    3050 K  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| GPU reserved segments |      65    |     100    |    3830    |    3765    |\n",
            "|       from large pool |      36    |      36    |     580    |     544    |\n",
            "|       from small pool |      29    |      64    |    3250    |    3221    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Non-releasable allocs |      50    |      50    |    2114 K  |    2114 K  |\n",
            "|       from large pool |      25    |      25    |    1028 K  |    1028 K  |\n",
            "|       from small pool |      25    |      45    |    1086 K  |    1086 K  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Oversize allocations  |       0    |       0    |       0    |       0    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Oversize GPU segments |       0    |       0    |       0    |       0    |\n",
            "|===========================================================================|\n",
            "\n",
            "2022-03-10 05:38:39 | WARNING | fairseq.trainer | attempting to recover from OOM in forward/backward pass\n",
            "epoch 006: 100% 711/712 [02:26<00:00,  6.90it/s, loss=2.995, ppl=7.97, wps=11836, ups=4.07, wpb=2906.9, bsz=128, num_updates=4100, lr=0.0005, gnorm=0.993, train_wall=22, gb_free=10.5, wall=872]2022-03-10 05:38:51 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "\n",
            "epoch 006 | valid on 'valid' subset:   0% 0/8 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 006 | valid on 'valid' subset:  12% 1/8 [00:00<00:01,  5.39it/s]\u001b[A\n",
            "epoch 006 | valid on 'valid' subset:  38% 3/8 [00:00<00:00,  9.55it/s]\u001b[A\n",
            "epoch 006 | valid on 'valid' subset:  62% 5/8 [00:00<00:00, 10.36it/s]\u001b[A\n",
            "epoch 006 | valid on 'valid' subset:  88% 7/8 [00:00<00:00,  9.57it/s]\u001b[A\n",
            "epoch 006 | valid on 'valid' subset: 100% 8/8 [00:01<00:00,  5.40it/s]\u001b[A\n",
            "                                                                      \u001b[A2022-03-10 05:38:52 | INFO | valid | epoch 006 | valid on 'valid' subset | loss 3.712 | ppl 13.11 | wps 20005 | wpb 2800.1 | bsz 125 | num_updates 4195 | best_loss 3.691\n",
            "2022-03-10 05:38:52 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 6 @ 4195 updates\n",
            "2022-03-10 05:38:52 | INFO | fairseq.trainer | Saving checkpoint to /content/checkpoints/checkpoint6.pt\n",
            "2022-03-10 05:38:53 | INFO | fairseq.trainer | Finished saving checkpoint to /content/checkpoints/checkpoint6.pt\n",
            "2022-03-10 05:38:53 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/checkpoint6.pt (epoch 6 @ 4195 updates, score 3.712) (writing took 1.1448477099997945 seconds)\n",
            "2022-03-10 05:38:53 | INFO | fairseq_cli.train | end of epoch 6 (average epoch stats below)\n",
            "2022-03-10 05:38:53 | INFO | train | epoch 006 | loss 2.836 | ppl 7.14 | wps 12936.2 | ups 4.68 | wpb 2761.4 | bsz 127.7 | num_updates 4195 | lr 0.0005 | gnorm 0.972 | train_wall 136 | gb_free 10.2 | wall 892\n",
            "epoch 007:   0% 0/712 [00:00<?, ?it/s]2022-03-10 05:38:53 | INFO | fairseq.data.iterators | grouped total_num_itrs = 712\n",
            "2022-03-10 05:38:53 | INFO | fairseq.trainer | begin training epoch 7\n",
            "2022-03-10 05:38:53 | INFO | fairseq_cli.train | Start iterating over samples\n",
            "epoch 007:   9% 61/712 [00:14<01:51,  5.85it/s, loss=2.814, ppl=7.03, wps=12221.7, ups=4.58, wpb=2667.3, bsz=128, num_updates=4200, lr=0.0005, gnorm=0.986, train_wall=18, gb_free=10.4, wall=894]2022-03-10 05:39:08 | WARNING | fairseq.trainer | OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 1.49 GiB (GPU 0; 11.17 GiB total capacity; 8.11 GiB already allocated; 1.14 GiB free; 9.51 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
            "2022-03-10 05:39:08 | WARNING | fairseq.trainer | |===========================================================================|\n",
            "|                  PyTorch CUDA memory summary, device ID 0                 |\n",
            "|---------------------------------------------------------------------------|\n",
            "|            CUDA OOMs: 78           |        cudaMalloc retries: 155       |\n",
            "|===========================================================================|\n",
            "|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Allocated memory      |    6773 MB |    8302 MB |   11698 GB |   11691 GB |\n",
            "|       from large pool |    6724 MB |    8253 MB |   11293 GB |   11287 GB |\n",
            "|       from small pool |      48 MB |      60 MB |     404 GB |     404 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Active memory         |    6773 MB |    8302 MB |   11698 GB |   11691 GB |\n",
            "|       from large pool |    6724 MB |    8253 MB |   11293 GB |   11287 GB |\n",
            "|       from small pool |      48 MB |      60 MB |     404 GB |     404 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| GPU reserved memory   |    9736 MB |   10874 MB |  335912 MB |  326176 MB |\n",
            "|       from large pool |    9676 MB |   10746 MB |  329342 MB |  319666 MB |\n",
            "|       from small pool |      60 MB |     128 MB |    6570 MB |    6510 MB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Non-releasable memory |    1432 MB |    1433 MB |   12015 GB |   12014 GB |\n",
            "|       from large pool |    1421 MB |    1422 MB |   11573 GB |   11572 GB |\n",
            "|       from small pool |      11 MB |      21 MB |     441 GB |     441 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Allocations           |     592    |     595    |    4833 K  |    4833 K  |\n",
            "|       from large pool |     160    |     164    |    1685 K  |    1685 K  |\n",
            "|       from small pool |     432    |     516    |    3148 K  |    3147 K  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Active allocs         |     592    |     595    |    4833 K  |    4833 K  |\n",
            "|       from large pool |     160    |     164    |    1685 K  |    1685 K  |\n",
            "|       from small pool |     432    |     516    |    3148 K  |    3147 K  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| GPU reserved segments |      66    |     101    |    3868    |    3802    |\n",
            "|       from large pool |      36    |      37    |     583    |     547    |\n",
            "|       from small pool |      30    |      64    |    3285    |    3255    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Non-releasable allocs |      43    |      46    |    2181 K  |    2181 K  |\n",
            "|       from large pool |      19    |      20    |    1061 K  |    1061 K  |\n",
            "|       from small pool |      24    |      40    |    1120 K  |    1120 K  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Oversize allocations  |       0    |       0    |       0    |       0    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Oversize GPU segments |       0    |       0    |       0    |       0    |\n",
            "|===========================================================================|\n",
            "\n",
            "2022-03-10 05:39:08 | WARNING | fairseq.trainer | attempting to recover from OOM in forward/backward pass\n",
            "epoch 007:  34% 239/712 [00:49<01:52,  4.19it/s, loss=2.575, ppl=5.96, wps=13781.3, ups=5.08, wpb=2714.8, bsz=128, num_updates=4400, lr=0.0005, gnorm=0.964, train_wall=19, gb_free=10.4, wall=934]2022-03-10 05:39:43 | WARNING | fairseq.trainer | OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 1.70 GiB (GPU 0; 11.17 GiB total capacity; 9.73 GiB already allocated; 63.81 MiB free; 10.59 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
            "2022-03-10 05:39:43 | WARNING | fairseq.trainer | |===========================================================================|\n",
            "|                  PyTorch CUDA memory summary, device ID 0                 |\n",
            "|---------------------------------------------------------------------------|\n",
            "|            CUDA OOMs: 79           |        cudaMalloc retries: 159       |\n",
            "|===========================================================================|\n",
            "|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Allocated memory      |    8221 MB |    9964 MB |   12150 GB |   12142 GB |\n",
            "|       from large pool |    8172 MB |    9915 MB |   11729 GB |   11721 GB |\n",
            "|       from small pool |      49 MB |      60 MB |     421 GB |     421 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Active memory         |    8221 MB |    9964 MB |   12150 GB |   12142 GB |\n",
            "|       from large pool |    8172 MB |    9915 MB |   11729 GB |   11721 GB |\n",
            "|       from small pool |      49 MB |      60 MB |     421 GB |     421 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| GPU reserved memory   |   10844 MB |   10844 MB |  343984 MB |  333140 MB |\n",
            "|       from large pool |   10786 MB |   10786 MB |  337236 MB |  326450 MB |\n",
            "|       from small pool |      58 MB |     122 MB |    6748 MB |    6690 MB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Non-releasable memory |     878 MB |    1562 MB |   12484 GB |   12483 GB |\n",
            "|       from large pool |     869 MB |    1552 MB |   12024 GB |   12023 GB |\n",
            "|       from small pool |       8 MB |      23 MB |     460 GB |     460 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Allocations           |     592    |     595    |    5033 K  |    5033 K  |\n",
            "|       from large pool |     160    |     164    |    1755 K  |    1755 K  |\n",
            "|       from small pool |     432    |     516    |    3278 K  |    3278 K  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Active allocs         |     592    |     595    |    5033 K  |    5033 K  |\n",
            "|       from large pool |     160    |     164    |    1755 K  |    1755 K  |\n",
            "|       from small pool |     432    |     516    |    3278 K  |    3278 K  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| GPU reserved segments |      65    |      97    |    3963    |    3898    |\n",
            "|       from large pool |      36    |      36    |     589    |     553    |\n",
            "|       from small pool |      29    |      61    |    3374    |    3345    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Non-releasable allocs |      45    |      55    |    2271 K  |    2271 K  |\n",
            "|       from large pool |      22    |      23    |    1104 K  |    1104 K  |\n",
            "|       from small pool |      23    |      50    |    1167 K  |    1167 K  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Oversize allocations  |       0    |       0    |       0    |       0    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Oversize GPU segments |       0    |       0    |       0    |       0    |\n",
            "|===========================================================================|\n",
            "\n",
            "2022-03-10 05:39:43 | WARNING | fairseq.trainer | attempting to recover from OOM in forward/backward pass\n",
            "epoch 007:  34% 241/712 [00:50<02:33,  3.06it/s, loss=2.575, ppl=5.96, wps=13781.3, ups=5.08, wpb=2714.8, bsz=128, num_updates=4400, lr=0.0005, gnorm=0.964, train_wall=19, gb_free=10.4, wall=934]2022-03-10 05:39:44 | WARNING | fairseq.trainer | OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 1.52 GiB (GPU 0; 11.17 GiB total capacity; 8.51 GiB already allocated; 255.81 MiB free; 10.40 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
            "2022-03-10 05:39:44 | WARNING | fairseq.trainer | |===========================================================================|\n",
            "|                  PyTorch CUDA memory summary, device ID 0                 |\n",
            "|---------------------------------------------------------------------------|\n",
            "|            CUDA OOMs: 80           |        cudaMalloc retries: 160       |\n",
            "|===========================================================================|\n",
            "|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Allocated memory      |    7163 MB |    8715 MB |   12162 GB |   12155 GB |\n",
            "|       from large pool |    7115 MB |    8667 MB |   11740 GB |   11734 GB |\n",
            "|       from small pool |      48 MB |      60 MB |     421 GB |     421 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Active memory         |    7163 MB |    8715 MB |   12162 GB |   12155 GB |\n",
            "|       from large pool |    7115 MB |    8667 MB |   11740 GB |   11734 GB |\n",
            "|       from small pool |      48 MB |      60 MB |     421 GB |     421 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| GPU reserved memory   |   10652 MB |   10658 MB |  345542 MB |  334890 MB |\n",
            "|       from large pool |   10594 MB |   10594 MB |  338788 MB |  328194 MB |\n",
            "|       from small pool |      58 MB |      64 MB |    6754 MB |    6696 MB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Non-releasable memory |    1936 MB |    1936 MB |   12496 GB |   12494 GB |\n",
            "|       from large pool |    1926 MB |    1926 MB |   12035 GB |   12034 GB |\n",
            "|       from small pool |       9 MB |      15 MB |     460 GB |     460 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Allocations           |     592    |     595    |    5035 K  |    5034 K  |\n",
            "|       from large pool |     160    |     164    |    1755 K  |    1755 K  |\n",
            "|       from small pool |     432    |     516    |    3279 K  |    3279 K  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Active allocs         |     592    |     595    |    5035 K  |    5034 K  |\n",
            "|       from large pool |     160    |     164    |    1755 K  |    1755 K  |\n",
            "|       from small pool |     432    |     516    |    3279 K  |    3279 K  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| GPU reserved segments |      65    |      68    |    3967    |    3902    |\n",
            "|       from large pool |      36    |      36    |     590    |     554    |\n",
            "|       from small pool |      29    |      32    |    3377    |    3348    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Non-releasable allocs |      47    |      47    |    2272 K  |    2272 K  |\n",
            "|       from large pool |      22    |      22    |    1104 K  |    1104 K  |\n",
            "|       from small pool |      25    |      34    |    1167 K  |    1167 K  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Oversize allocations  |       0    |       0    |       0    |       0    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Oversize GPU segments |       0    |       0    |       0    |       0    |\n",
            "|===========================================================================|\n",
            "\n",
            "2022-03-10 05:39:44 | WARNING | fairseq.trainer | attempting to recover from OOM in forward/backward pass\n",
            "epoch 007:  39% 279/712 [00:59<01:10,  6.11it/s, loss=2.575, ppl=5.96, wps=13781.3, ups=5.08, wpb=2714.8, bsz=128, num_updates=4400, lr=0.0005, gnorm=0.964, train_wall=19, gb_free=10.4, wall=934]2022-03-10 05:39:54 | WARNING | fairseq.trainer | OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 1.83 GiB (GPU 0; 11.17 GiB total capacity; 10.03 GiB already allocated; 101.81 MiB free; 10.55 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
            "2022-03-10 05:39:54 | WARNING | fairseq.trainer | |===========================================================================|\n",
            "|                  PyTorch CUDA memory summary, device ID 0                 |\n",
            "|---------------------------------------------------------------------------|\n",
            "|            CUDA OOMs: 81           |        cudaMalloc retries: 162       |\n",
            "|===========================================================================|\n",
            "|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Allocated memory      |    8398 MB |   10271 MB |   12307 GB |   12299 GB |\n",
            "|       from large pool |    8340 MB |   10213 MB |   11882 GB |   11874 GB |\n",
            "|       from small pool |      57 MB |      60 MB |     424 GB |     424 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Active memory         |    8398 MB |   10271 MB |   12307 GB |   12299 GB |\n",
            "|       from large pool |    8340 MB |   10213 MB |   11882 GB |   11874 GB |\n",
            "|       from small pool |      57 MB |      60 MB |     424 GB |     424 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| GPU reserved memory   |   10806 MB |   10806 MB |  348790 MB |  337984 MB |\n",
            "|       from large pool |   10740 MB |   10740 MB |  341962 MB |  331222 MB |\n",
            "|       from small pool |      66 MB |     132 MB |    6828 MB |    6762 MB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Non-releasable memory |  546091 KB |    1495 MB |   12642 GB |   12641 GB |\n",
            "|       from large pool |  537671 KB |    1485 MB |   12178 GB |   12177 GB |\n",
            "|       from small pool |    8420 KB |      29 MB |     463 GB |     463 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Allocations           |     592    |     595    |    5078 K  |    5078 K  |\n",
            "|       from large pool |     150    |     154    |    1771 K  |    1771 K  |\n",
            "|       from small pool |     442    |     516    |    3307 K  |    3307 K  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Active allocs         |     592    |     595    |    5078 K  |    5078 K  |\n",
            "|       from large pool |     150    |     154    |    1771 K  |    1771 K  |\n",
            "|       from small pool |     442    |     516    |    3307 K  |    3307 K  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| GPU reserved segments |      69    |     102    |    4006    |    3937    |\n",
            "|       from large pool |      36    |      36    |     592    |     556    |\n",
            "|       from small pool |      33    |      66    |    3414    |    3381    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Non-releasable allocs |      54    |      55    |    2291 K  |    2291 K  |\n",
            "|       from large pool |      25    |      26    |    1114 K  |    1114 K  |\n",
            "|       from small pool |      29    |      50    |    1177 K  |    1177 K  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Oversize allocations  |       0    |       0    |       0    |       0    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Oversize GPU segments |       0    |       0    |       0    |       0    |\n",
            "|===========================================================================|\n",
            "\n",
            "2022-03-10 05:39:54 | WARNING | fairseq.trainer | attempting to recover from OOM in forward/backward pass\n",
            "epoch 007:  40% 282/712 [01:01<02:00,  3.57it/s, loss=2.575, ppl=5.96, wps=13781.3, ups=5.08, wpb=2714.8, bsz=128, num_updates=4400, lr=0.0005, gnorm=0.964, train_wall=19, gb_free=10.4, wall=934]2022-03-10 05:39:55 | WARNING | fairseq.trainer | OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 2.95 GiB (GPU 0; 11.17 GiB total capacity; 10.51 GiB already allocated; 23.81 MiB free; 10.63 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
            "2022-03-10 05:39:55 | WARNING | fairseq.trainer | |===========================================================================|\n",
            "|                  PyTorch CUDA memory summary, device ID 0                 |\n",
            "|---------------------------------------------------------------------------|\n",
            "|            CUDA OOMs: 82           |        cudaMalloc retries: 163       |\n",
            "|===========================================================================|\n",
            "|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Allocated memory      |   10715 MB |   10765 MB |   12325 GB |   12314 GB |\n",
            "|       from large pool |   10664 MB |   10713 MB |   11900 GB |   11890 GB |\n",
            "|       from small pool |      51 MB |      61 MB |     424 GB |     424 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Active memory         |   10715 MB |   10765 MB |   12325 GB |   12314 GB |\n",
            "|       from large pool |   10664 MB |   10713 MB |   11900 GB |   11890 GB |\n",
            "|       from small pool |      51 MB |      61 MB |     424 GB |     424 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| GPU reserved memory   |   10884 MB |   10888 MB |  350746 MB |  339862 MB |\n",
            "|       from large pool |   10822 MB |   10822 MB |  343918 MB |  333096 MB |\n",
            "|       from small pool |      62 MB |      66 MB |    6828 MB |    6766 MB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Non-releasable memory |  172393 KB |    2399 MB |   12658 GB |   12658 GB |\n",
            "|       from large pool |  161237 KB |    2386 MB |   12194 GB |   12194 GB |\n",
            "|       from small pool |   11156 KB |      18 MB |     463 GB |     463 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Allocations           |     593    |     595    |    5080 K  |    5079 K  |\n",
            "|       from large pool |     162    |     163    |    1771 K  |    1771 K  |\n",
            "|       from small pool |     431    |     516    |    3308 K  |    3307 K  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Active allocs         |     593    |     595    |    5080 K  |    5079 K  |\n",
            "|       from large pool |     162    |     163    |    1771 K  |    1771 K  |\n",
            "|       from small pool |     431    |     516    |    3308 K  |    3307 K  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| GPU reserved segments |      72    |      74    |    4012    |    3940    |\n",
            "|       from large pool |      41    |      41    |     598    |     557    |\n",
            "|       from small pool |      31    |      33    |    3414    |    3383    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Non-releasable allocs |      55    |      55    |    2292 K  |    2292 K  |\n",
            "|       from large pool |      29    |      29    |    1114 K  |    1114 K  |\n",
            "|       from small pool |      26    |      30    |    1177 K  |    1177 K  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Oversize allocations  |       0    |       0    |       0    |       0    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Oversize GPU segments |       0    |       0    |       0    |       0    |\n",
            "|===========================================================================|\n",
            "\n",
            "2022-03-10 05:39:55 | WARNING | fairseq.trainer | attempting to recover from OOM in forward/backward pass\n",
            "epoch 007:  46% 326/712 [01:09<00:59,  6.53it/s, loss=2.727, ppl=6.62, wps=11736.4, ups=3.99, wpb=2939.5, bsz=126.7, num_updates=4500, lr=0.0005, gnorm=1.027, train_wall=22, gb_free=9.9, wall=959]2022-03-10 05:40:04 | WARNING | fairseq.trainer | OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 1.76 GiB (GPU 0; 11.17 GiB total capacity; 9.04 GiB already allocated; 1.25 GiB free; 9.40 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
            "2022-03-10 05:40:04 | WARNING | fairseq.trainer | |===========================================================================|\n",
            "|                  PyTorch CUDA memory summary, device ID 0                 |\n",
            "|---------------------------------------------------------------------------|\n",
            "|            CUDA OOMs: 83           |        cudaMalloc retries: 166       |\n",
            "|===========================================================================|\n",
            "|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Allocated memory      |    9261 MB |    9330 MB |   12422 GB |   12413 GB |\n",
            "|       from large pool |    9211 MB |    9281 MB |   11992 GB |   11983 GB |\n",
            "|       from small pool |      49 MB |      61 MB |     430 GB |     430 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Active memory         |    9261 MB |    9330 MB |   12422 GB |   12413 GB |\n",
            "|       from large pool |    9211 MB |    9281 MB |   11992 GB |   11983 GB |\n",
            "|       from small pool |      49 MB |      61 MB |     430 GB |     430 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| GPU reserved memory   |    9624 MB |   10600 MB |  364674 MB |  355050 MB |\n",
            "|       from large pool |    9566 MB |   10462 MB |  357770 MB |  348204 MB |\n",
            "|       from small pool |      58 MB |     138 MB |    6904 MB |    6846 MB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Non-releasable memory |  371540 KB |    1214 MB |   12749 GB |   12748 GB |\n",
            "|       from large pool |  362728 KB |    1205 MB |   12279 GB |   12279 GB |\n",
            "|       from small pool |    8812 KB |      15 MB |     469 GB |     469 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Allocations           |     593    |     595    |    5128 K  |    5127 K  |\n",
            "|       from large pool |     160    |     164    |    1786 K  |    1786 K  |\n",
            "|       from small pool |     433    |     516    |    3341 K  |    3341 K  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Active allocs         |     593    |     595    |    5128 K  |    5127 K  |\n",
            "|       from large pool |     160    |     164    |    1786 K  |    1786 K  |\n",
            "|       from small pool |     433    |     516    |    3341 K  |    3341 K  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| GPU reserved segments |      60    |     103    |    4078    |    4018    |\n",
            "|       from large pool |      31    |      34    |     626    |     595    |\n",
            "|       from small pool |      29    |      69    |    3452    |    3423    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Non-releasable allocs |      50    |      50    |    2313 K  |    2313 K  |\n",
            "|       from large pool |      26    |      26    |    1123 K  |    1123 K  |\n",
            "|       from small pool |      24    |      39    |    1190 K  |    1190 K  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Oversize allocations  |       0    |       0    |       0    |       0    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Oversize GPU segments |       0    |       0    |       0    |       0    |\n",
            "|===========================================================================|\n",
            "\n",
            "2022-03-10 05:40:04 | WARNING | fairseq.trainer | attempting to recover from OOM in forward/backward pass\n",
            "epoch 007:  49% 350/712 [01:14<00:48,  7.39it/s, loss=2.727, ppl=6.62, wps=11736.4, ups=3.99, wpb=2939.5, bsz=126.7, num_updates=4500, lr=0.0005, gnorm=1.027, train_wall=22, gb_free=9.9, wall=959]2022-03-10 05:40:09 | WARNING | fairseq.trainer | OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 1.87 GiB (GPU 0; 11.17 GiB total capacity; 8.57 GiB already allocated; 1.08 GiB free; 9.58 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
            "2022-03-10 05:40:09 | WARNING | fairseq.trainer | |===========================================================================|\n",
            "|                  PyTorch CUDA memory summary, device ID 0                 |\n",
            "|---------------------------------------------------------------------------|\n",
            "|            CUDA OOMs: 84           |        cudaMalloc retries: 169       |\n",
            "|===========================================================================|\n",
            "|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Allocated memory      |    8774 MB |    8841 MB |   12482 GB |   12473 GB |\n",
            "|       from large pool |    8725 MB |    8792 MB |   12049 GB |   12041 GB |\n",
            "|       from small pool |      49 MB |      60 MB |     432 GB |     432 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Active memory         |    8774 MB |    8841 MB |   12482 GB |   12473 GB |\n",
            "|       from large pool |    8725 MB |    8792 MB |   12049 GB |   12041 GB |\n",
            "|       from small pool |      49 MB |      60 MB |     432 GB |     432 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| GPU reserved memory   |    9806 MB |    9806 MB |  370436 MB |  360630 MB |\n",
            "|       from large pool |    9746 MB |    9746 MB |  363414 MB |  353668 MB |\n",
            "|       from small pool |      60 MB |     104 MB |    7022 MB |    6962 MB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Non-releasable memory |    1031 MB |    2105 MB |   12809 GB |   12808 GB |\n",
            "|       from large pool |    1020 MB |    2093 MB |   12337 GB |   12336 GB |\n",
            "|       from small pool |      10 MB |      17 MB |     472 GB |     472 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Allocations           |     593    |     595    |    5154 K  |    5154 K  |\n",
            "|       from large pool |     160    |     164    |    1795 K  |    1795 K  |\n",
            "|       from small pool |     433    |     516    |    3358 K  |    3358 K  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Active allocs         |     593    |     595    |    5154 K  |    5154 K  |\n",
            "|       from large pool |     160    |     164    |    1795 K  |    1795 K  |\n",
            "|       from small pool |     433    |     516    |    3358 K  |    3358 K  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| GPU reserved segments |      56    |      78    |    4141    |    4085    |\n",
            "|       from large pool |      26    |      26    |     630    |     604    |\n",
            "|       from small pool |      30    |      52    |    3511    |    3481    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Non-releasable allocs |      40    |      51    |    2325 K  |    2325 K  |\n",
            "|       from large pool |      17    |      17    |    1129 K  |    1128 K  |\n",
            "|       from small pool |      23    |      43    |    1196 K  |    1196 K  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Oversize allocations  |       0    |       0    |       0    |       0    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Oversize GPU segments |       0    |       0    |       0    |       0    |\n",
            "|===========================================================================|\n",
            "\n",
            "2022-03-10 05:40:09 | WARNING | fairseq.trainer | attempting to recover from OOM in forward/backward pass\n",
            "epoch 007:  61% 432/712 [01:30<00:46,  6.07it/s, loss=2.653, ppl=6.29, wps=13525.9, ups=4.91, wpb=2755.8, bsz=126.9, num_updates=4600, lr=0.0005, gnorm=1.028, train_wall=19, gb_free=9, wall=980]2022-03-10 05:40:25 | WARNING | fairseq.trainer | OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 2.96 GiB (GPU 0; 11.17 GiB total capacity; 10.42 GiB already allocated; 141.81 MiB free; 10.51 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
            "2022-03-10 05:40:25 | WARNING | fairseq.trainer | |===========================================================================|\n",
            "|                  PyTorch CUDA memory summary, device ID 0                 |\n",
            "|---------------------------------------------------------------------------|\n",
            "|            CUDA OOMs: 85           |        cudaMalloc retries: 172       |\n",
            "|===========================================================================|\n",
            "|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Allocated memory      |   10619 MB |   10668 MB |   12694 GB |   12684 GB |\n",
            "|       from large pool |   10568 MB |   10617 MB |   12255 GB |   12244 GB |\n",
            "|       from small pool |      51 MB |      60 MB |     439 GB |     439 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Active memory         |   10619 MB |   10668 MB |   12694 GB |   12684 GB |\n",
            "|       from large pool |   10568 MB |   10617 MB |   12255 GB |   12244 GB |\n",
            "|       from small pool |      51 MB |      60 MB |     439 GB |     439 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| GPU reserved memory   |   10766 MB |   10818 MB |  373618 MB |  362852 MB |\n",
            "|       from large pool |   10708 MB |   10708 MB |  366418 MB |  355710 MB |\n",
            "|       from small pool |      58 MB |     110 MB |    7200 MB |    7142 MB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Non-releasable memory |  150424 KB |    2531 MB |   13044 GB |   13044 GB |\n",
            "|       from large pool |  143333 KB |    2522 MB |   12564 GB |   12564 GB |\n",
            "|       from small pool |    7091 KB |      21 MB |     479 GB |     479 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Allocations           |     593    |     595    |    5246 K  |    5245 K  |\n",
            "|       from large pool |     162    |     163    |    1829 K  |    1828 K  |\n",
            "|       from small pool |     431    |     516    |    3417 K  |    3417 K  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Active allocs         |     593    |     595    |    5246 K  |    5245 K  |\n",
            "|       from large pool |     162    |     163    |    1829 K  |    1828 K  |\n",
            "|       from small pool |     431    |     516    |    3417 K  |    3417 K  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| GPU reserved segments |      57    |      83    |    4234    |    4177    |\n",
            "|       from large pool |      28    |      28    |     634    |     606    |\n",
            "|       from small pool |      29    |      55    |    3600    |    3571    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Non-releasable allocs |      42    |      51    |    2366 K  |    2366 K  |\n",
            "|       from large pool |      21    |      21    |    1149 K  |    1149 K  |\n",
            "|       from small pool |      21    |      43    |    1217 K  |    1217 K  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Oversize allocations  |       0    |       0    |       0    |       0    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Oversize GPU segments |       0    |       0    |       0    |       0    |\n",
            "|===========================================================================|\n",
            "\n",
            "2022-03-10 05:40:25 | WARNING | fairseq.trainer | attempting to recover from OOM in forward/backward pass\n",
            "epoch 007:  62% 438/712 [01:32<00:44,  6.23it/s, loss=2.653, ppl=6.29, wps=13525.9, ups=4.91, wpb=2755.8, bsz=126.9, num_updates=4600, lr=0.0005, gnorm=1.028, train_wall=19, gb_free=9, wall=980]2022-03-10 05:40:26 | WARNING | fairseq.trainer | OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 2.51 GiB (GPU 0; 11.17 GiB total capacity; 10.39 GiB already allocated; 141.81 MiB free; 10.51 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
            "2022-03-10 05:40:26 | WARNING | fairseq.trainer | |===========================================================================|\n",
            "|                  PyTorch CUDA memory summary, device ID 0                 |\n",
            "|---------------------------------------------------------------------------|\n",
            "|            CUDA OOMs: 86           |        cudaMalloc retries: 173       |\n",
            "|===========================================================================|\n",
            "|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Allocated memory      |   10596 MB |   10639 MB |   12716 GB |   12705 GB |\n",
            "|       from large pool |   10545 MB |   10587 MB |   12276 GB |   12265 GB |\n",
            "|       from small pool |      51 MB |      60 MB |     439 GB |     439 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Active memory         |   10596 MB |   10639 MB |   12716 GB |   12705 GB |\n",
            "|       from large pool |   10545 MB |   10587 MB |   12276 GB |   12265 GB |\n",
            "|       from small pool |      51 MB |      60 MB |     439 GB |     439 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| GPU reserved memory   |   10766 MB |   10846 MB |  373698 MB |  362932 MB |\n",
            "|       from large pool |   10708 MB |   10708 MB |  366418 MB |  355710 MB |\n",
            "|       from small pool |      58 MB |     138 MB |    7280 MB |    7222 MB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Non-releasable memory |  173084 KB |    2268 MB |   13070 GB |   13070 GB |\n",
            "|       from large pool |  165972 KB |    2259 MB |   12590 GB |   12589 GB |\n",
            "|       from small pool |    7112 KB |      16 MB |     480 GB |     480 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Allocations           |     593    |     595    |    5252 K  |    5251 K  |\n",
            "|       from large pool |     162    |     163    |    1830 K  |    1830 K  |\n",
            "|       from small pool |     431    |     516    |    3421 K  |    3421 K  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Active allocs         |     593    |     595    |    5252 K  |    5251 K  |\n",
            "|       from large pool |     162    |     163    |    1830 K  |    1830 K  |\n",
            "|       from small pool |     431    |     516    |    3421 K  |    3421 K  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| GPU reserved segments |      57    |      97    |    4274    |    4217    |\n",
            "|       from large pool |      28    |      28    |     634    |     606    |\n",
            "|       from small pool |      29    |      69    |    3640    |    3611    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Non-releasable allocs |      53    |      53    |    2369 K  |    2369 K  |\n",
            "|       from large pool |      29    |      29    |    1150 K  |    1150 K  |\n",
            "|       from small pool |      24    |      34    |    1219 K  |    1219 K  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Oversize allocations  |       0    |       0    |       0    |       0    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Oversize GPU segments |       0    |       0    |       0    |       0    |\n",
            "|===========================================================================|\n",
            "\n",
            "2022-03-10 05:40:26 | WARNING | fairseq.trainer | attempting to recover from OOM in forward/backward pass\n",
            "epoch 007:  64% 455/712 [01:36<01:06,  3.84it/s, loss=2.653, ppl=6.29, wps=13525.9, ups=4.91, wpb=2755.8, bsz=126.9, num_updates=4600, lr=0.0005, gnorm=1.028, train_wall=19, gb_free=9, wall=980]2022-03-10 05:40:31 | WARNING | fairseq.trainer | OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 1.45 GiB (GPU 0; 11.17 GiB total capacity; 7.96 GiB already allocated; 1.32 GiB free; 9.33 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
            "2022-03-10 05:40:31 | WARNING | fairseq.trainer | |===========================================================================|\n",
            "|                  PyTorch CUDA memory summary, device ID 0                 |\n",
            "|---------------------------------------------------------------------------|\n",
            "|            CUDA OOMs: 87           |        cudaMalloc retries: 174       |\n",
            "|===========================================================================|\n",
            "|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Allocated memory      |    6668 MB |    8151 MB |   12778 GB |   12772 GB |\n",
            "|       from large pool |    6620 MB |    8103 MB |   12337 GB |   12331 GB |\n",
            "|       from small pool |      48 MB |      61 MB |     441 GB |     441 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Active memory         |    6668 MB |    8151 MB |   12778 GB |   12772 GB |\n",
            "|       from large pool |    6620 MB |    8103 MB |   12337 GB |   12331 GB |\n",
            "|       from small pool |      48 MB |      61 MB |     441 GB |     441 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| GPU reserved memory   |    9558 MB |   10804 MB |  373736 MB |  364178 MB |\n",
            "|       from large pool |    9500 MB |   10708 MB |  366418 MB |  356918 MB |\n",
            "|       from small pool |      58 MB |      96 MB |    7318 MB |    7260 MB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Non-releasable memory |     977 MB |    1406 MB |   13143 GB |   13142 GB |\n",
            "|       from large pool |     967 MB |    1396 MB |   12661 GB |   12660 GB |\n",
            "|       from small pool |       9 MB |      12 MB |     481 GB |     481 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Allocations           |     592    |     595    |    5270 K  |    5270 K  |\n",
            "|       from large pool |     160    |     164    |    1837 K  |    1837 K  |\n",
            "|       from small pool |     432    |     516    |    3432 K  |    3432 K  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Active allocs         |     592    |     595    |    5270 K  |    5270 K  |\n",
            "|       from large pool |     160    |     164    |    1837 K  |    1837 K  |\n",
            "|       from small pool |     432    |     516    |    3432 K  |    3432 K  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| GPU reserved segments |      56    |      76    |    4293    |    4237    |\n",
            "|       from large pool |      27    |      28    |     634    |     607    |\n",
            "|       from small pool |      29    |      48    |    3659    |    3630    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Non-releasable allocs |      40    |      41    |    2377 K  |    2377 K  |\n",
            "|       from large pool |      19    |      20    |    1154 K  |    1154 K  |\n",
            "|       from small pool |      21    |      27    |    1222 K  |    1222 K  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Oversize allocations  |       0    |       0    |       0    |       0    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Oversize GPU segments |       0    |       0    |       0    |       0    |\n",
            "|===========================================================================|\n",
            "\n",
            "2022-03-10 05:40:31 | WARNING | fairseq.trainer | attempting to recover from OOM in forward/backward pass\n",
            "epoch 007:  87% 621/712 [02:09<00:15,  5.71it/s, loss=2.634, ppl=6.21, wps=14432, ups=5.17, wpb=2791.9, bsz=128, num_updates=4800, lr=0.0005, gnorm=0.957, train_wall=19, gb_free=10.3, wall=1020]2022-03-10 05:41:03 | WARNING | fairseq.trainer | OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 1.76 GiB (GPU 0; 11.17 GiB total capacity; 8.04 GiB already allocated; 1.64 GiB free; 9.01 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
            "2022-03-10 05:41:03 | WARNING | fairseq.trainer | |===========================================================================|\n",
            "|                  PyTorch CUDA memory summary, device ID 0                 |\n",
            "|---------------------------------------------------------------------------|\n",
            "|            CUDA OOMs: 88           |        cudaMalloc retries: 176       |\n",
            "|===========================================================================|\n",
            "|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Allocated memory      |    8231 MB |    8295 MB |   13197 GB |   13189 GB |\n",
            "|       from large pool |    8182 MB |    8246 MB |   12740 GB |   12732 GB |\n",
            "|       from small pool |      48 MB |      60 MB |     456 GB |     456 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Active memory         |    8231 MB |    8295 MB |   13197 GB |   13189 GB |\n",
            "|       from large pool |    8182 MB |    8246 MB |   12740 GB |   12732 GB |\n",
            "|       from small pool |      48 MB |      60 MB |     456 GB |     456 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| GPU reserved memory   |    9224 MB |   10874 MB |  379684 MB |  370460 MB |\n",
            "|       from large pool |    9164 MB |   10740 MB |  372218 MB |  363054 MB |\n",
            "|       from small pool |      60 MB |     134 MB |    7466 MB |    7406 MB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Non-releasable memory |     992 MB |    1596 MB |   13578 GB |   13577 GB |\n",
            "|       from large pool |     981 MB |    1585 MB |   13080 GB |   13079 GB |\n",
            "|       from small pool |      11 MB |      23 MB |     498 GB |     498 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Allocations           |     593    |     595    |    5457 K  |    5456 K  |\n",
            "|       from large pool |     160    |     164    |    1902 K  |    1902 K  |\n",
            "|       from small pool |     433    |     516    |    3554 K  |    3553 K  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Active allocs         |     593    |     595    |    5457 K  |    5456 K  |\n",
            "|       from large pool |     160    |     164    |    1902 K  |    1902 K  |\n",
            "|       from small pool |     433    |     516    |    3554 K  |    3553 K  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| GPU reserved segments |      57    |      95    |    4371    |    4314    |\n",
            "|       from large pool |      27    |      28    |     638    |     611    |\n",
            "|       from small pool |      30    |      67    |    3733    |    3703    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Non-releasable allocs |      42    |      56    |    2459 K  |    2459 K  |\n",
            "|       from large pool |      16    |      16    |    1194 K  |    1194 K  |\n",
            "|       from small pool |      26    |      49    |    1265 K  |    1265 K  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Oversize allocations  |       0    |       0    |       0    |       0    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Oversize GPU segments |       0    |       0    |       0    |       0    |\n",
            "|===========================================================================|\n",
            "\n",
            "2022-03-10 05:41:03 | WARNING | fairseq.trainer | attempting to recover from OOM in forward/backward pass\n",
            "epoch 007: 100% 711/712 [02:26<00:00,  5.22it/s, loss=2.634, ppl=6.21, wps=14432, ups=5.17, wpb=2791.9, bsz=128, num_updates=4800, lr=0.0005, gnorm=0.957, train_wall=19, gb_free=10.3, wall=1020]2022-03-10 05:41:20 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "\n",
            "epoch 007 | valid on 'valid' subset:   0% 0/8 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 007 | valid on 'valid' subset:  12% 1/8 [00:00<00:01,  5.66it/s]\u001b[A\n",
            "epoch 007 | valid on 'valid' subset:  38% 3/8 [00:00<00:00,  9.86it/s]\u001b[A\n",
            "epoch 007 | valid on 'valid' subset:  62% 5/8 [00:00<00:00, 10.71it/s]\u001b[A\n",
            "epoch 007 | valid on 'valid' subset:  88% 7/8 [00:00<00:00,  9.79it/s]\u001b[A\n",
            "                                                                      \u001b[A2022-03-10 05:41:21 | INFO | valid | epoch 007 | valid on 'valid' subset | loss 3.733 | ppl 13.29 | wps 20260.2 | wpb 2800.1 | bsz 125 | num_updates 4896 | best_loss 3.691\n",
            "2022-03-10 05:41:21 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 7 @ 4896 updates\n",
            "2022-03-10 05:41:21 | INFO | fairseq.trainer | Saving checkpoint to /content/checkpoints/checkpoint7.pt\n",
            "2022-03-10 05:41:21 | INFO | fairseq.trainer | Finished saving checkpoint to /content/checkpoints/checkpoint7.pt\n",
            "2022-03-10 05:41:22 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/checkpoint7.pt (epoch 7 @ 4896 updates, score 3.733) (writing took 1.190730691999761 seconds)\n",
            "2022-03-10 05:41:22 | INFO | fairseq_cli.train | end of epoch 7 (average epoch stats below)\n",
            "2022-03-10 05:41:22 | INFO | train | epoch 007 | loss 2.643 | ppl 6.25 | wps 13056.3 | ups 4.72 | wpb 2767.8 | bsz 127.7 | num_updates 4896 | lr 0.0005 | gnorm 0.989 | train_wall 136 | gb_free 10.2 | wall 1040\n",
            "epoch 008:   0% 0/712 [00:00<?, ?it/s]2022-03-10 05:41:22 | INFO | fairseq.data.iterators | grouped total_num_itrs = 712\n",
            "2022-03-10 05:41:22 | INFO | fairseq.trainer | begin training epoch 8\n",
            "2022-03-10 05:41:22 | INFO | fairseq_cli.train | Start iterating over samples\n",
            "epoch 008:   7% 52/712 [00:08<01:35,  6.95it/s, loss=2.604, ppl=6.08, wps=12691.5, ups=4.64, wpb=2732.9, bsz=128, num_updates=4900, lr=0.0005, gnorm=0.972, train_wall=18, gb_free=10.2, wall=1041]2022-03-10 05:41:31 | WARNING | fairseq.trainer | OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 1.76 GiB (GPU 0; 11.17 GiB total capacity; 9.04 GiB already allocated; 205.81 MiB free; 10.45 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
            "2022-03-10 05:41:31 | WARNING | fairseq.trainer | |===========================================================================|\n",
            "|                  PyTorch CUDA memory summary, device ID 0                 |\n",
            "|---------------------------------------------------------------------------|\n",
            "|            CUDA OOMs: 89           |        cudaMalloc retries: 178       |\n",
            "|===========================================================================|\n",
            "|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Allocated memory      |    9258 MB |    9328 MB |   13540 GB |   13531 GB |\n",
            "|       from large pool |    9209 MB |    9278 MB |   13068 GB |   13059 GB |\n",
            "|       from small pool |      49 MB |      60 MB |     471 GB |     471 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Active memory         |    9258 MB |    9328 MB |   13540 GB |   13531 GB |\n",
            "|       from large pool |    9209 MB |    9278 MB |   13068 GB |   13059 GB |\n",
            "|       from small pool |      49 MB |      60 MB |     471 GB |     471 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| GPU reserved memory   |   10702 MB |   10778 MB |  382282 MB |  371580 MB |\n",
            "|       from large pool |   10640 MB |   10640 MB |  374658 MB |  364018 MB |\n",
            "|       from small pool |      62 MB |     138 MB |    7624 MB |    7562 MB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Non-releasable memory |    1443 MB |    1568 MB |   13936 GB |   13934 GB |\n",
            "|       from large pool |    1430 MB |    1555 MB |   13421 GB |   13420 GB |\n",
            "|       from small pool |      12 MB |      27 MB |     514 GB |     514 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Allocations           |     593    |     595    |    5618 K  |    5618 K  |\n",
            "|       from large pool |     160    |     164    |    1957 K  |    1957 K  |\n",
            "|       from small pool |     433    |     516    |    3660 K  |    3660 K  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Active allocs         |     593    |     595    |    5618 K  |    5618 K  |\n",
            "|       from large pool |     160    |     164    |    1957 K  |    1957 K  |\n",
            "|       from small pool |     433    |     516    |    3660 K  |    3660 K  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| GPU reserved segments |      59    |      97    |    4452    |    4393    |\n",
            "|       from large pool |      28    |      28    |     640    |     612    |\n",
            "|       from small pool |      31    |      69    |    3812    |    3781    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Non-releasable allocs |      52    |      55    |    2531 K  |    2531 K  |\n",
            "|       from large pool |      28    |      28    |    1227 K  |    1227 K  |\n",
            "|       from small pool |      24    |      48    |    1304 K  |    1304 K  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Oversize allocations  |       0    |       0    |       0    |       0    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Oversize GPU segments |       0    |       0    |       0    |       0    |\n",
            "|===========================================================================|\n",
            "\n",
            "2022-03-10 05:41:31 | WARNING | fairseq.trainer | attempting to recover from OOM in forward/backward pass\n",
            "epoch 008:   9% 61/712 [00:10<01:57,  5.56it/s, loss=2.604, ppl=6.08, wps=12691.5, ups=4.64, wpb=2732.9, bsz=128, num_updates=4900, lr=0.0005, gnorm=0.972, train_wall=18, gb_free=10.2, wall=1041]2022-03-10 05:41:33 | WARNING | fairseq.trainer | OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 1.76 GiB (GPU 0; 11.17 GiB total capacity; 8.04 GiB already allocated; 1.74 GiB free; 8.91 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
            "2022-03-10 05:41:33 | WARNING | fairseq.trainer | |===========================================================================|\n",
            "|                  PyTorch CUDA memory summary, device ID 0                 |\n",
            "|---------------------------------------------------------------------------|\n",
            "|            CUDA OOMs: 90           |        cudaMalloc retries: 179       |\n",
            "|===========================================================================|\n",
            "|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Allocated memory      |    8231 MB |    8295 MB |   13572 GB |   13564 GB |\n",
            "|       from large pool |    8182 MB |    8246 MB |   13100 GB |   13092 GB |\n",
            "|       from small pool |      48 MB |      61 MB |     472 GB |     472 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Active memory         |    8231 MB |    8295 MB |   13572 GB |   13564 GB |\n",
            "|       from large pool |    8182 MB |    8246 MB |   13100 GB |   13092 GB |\n",
            "|       from small pool |      48 MB |      61 MB |     472 GB |     472 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| GPU reserved memory   |    9122 MB |   10736 MB |  382316 MB |  373194 MB |\n",
            "|       from large pool |    9064 MB |   10640 MB |  374658 MB |  365594 MB |\n",
            "|       from small pool |      58 MB |      96 MB |    7658 MB |    7600 MB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Non-releasable memory |     890 MB |    1494 MB |   13969 GB |   13969 GB |\n",
            "|       from large pool |     881 MB |    1485 MB |   13454 GB |   13453 GB |\n",
            "|       from small pool |       9 MB |      23 MB |     515 GB |     515 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Allocations           |     593    |     595    |    5628 K  |    5627 K  |\n",
            "|       from large pool |     160    |     164    |    1960 K  |    1960 K  |\n",
            "|       from small pool |     433    |     516    |    3667 K  |    3666 K  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Active allocs         |     593    |     595    |    5628 K  |    5627 K  |\n",
            "|       from large pool |     160    |     164    |    1960 K  |    1960 K  |\n",
            "|       from small pool |     433    |     516    |    3667 K  |    3666 K  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| GPU reserved segments |      56    |      76    |    4469    |    4413    |\n",
            "|       from large pool |      27    |      28    |     640    |     613    |\n",
            "|       from small pool |      29    |      48    |    3829    |    3800    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Non-releasable allocs |      41    |      51    |    2535 K  |    2535 K  |\n",
            "|       from large pool |      16    |      16    |    1229 K  |    1229 K  |\n",
            "|       from small pool |      25    |      45    |    1306 K  |    1306 K  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Oversize allocations  |       0    |       0    |       0    |       0    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Oversize GPU segments |       0    |       0    |       0    |       0    |\n",
            "|===========================================================================|\n",
            "\n",
            "2022-03-10 05:41:33 | WARNING | fairseq.trainer | attempting to recover from OOM in forward/backward pass\n",
            "epoch 008:  45% 318/712 [01:01<00:53,  7.37it/s, loss=2.415, ppl=5.33, wps=13921.6, ups=5.24, wpb=2657.3, bsz=128, num_updates=5200, lr=0.0005, gnorm=1.004, train_wall=19, gb_free=10.4, wall=1100]2022-03-10 05:42:24 | WARNING | fairseq.trainer | OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 2.51 GiB (GPU 0; 11.17 GiB total capacity; 10.39 GiB already allocated; 107.81 MiB free; 10.55 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
            "2022-03-10 05:42:24 | WARNING | fairseq.trainer | |===========================================================================|\n",
            "|                  PyTorch CUDA memory summary, device ID 0                 |\n",
            "|---------------------------------------------------------------------------|\n",
            "|            CUDA OOMs: 91           |        cudaMalloc retries: 182       |\n",
            "|===========================================================================|\n",
            "|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Allocated memory      |   10596 MB |   10638 MB |   14236 GB |   14225 GB |\n",
            "|       from large pool |   10545 MB |   10587 MB |   13739 GB |   13728 GB |\n",
            "|       from small pool |      51 MB |      60 MB |     497 GB |     497 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Active memory         |   10596 MB |   10638 MB |   14236 GB |   14225 GB |\n",
            "|       from large pool |   10545 MB |   10587 MB |   13739 GB |   13728 GB |\n",
            "|       from small pool |      51 MB |      60 MB |     497 GB |     497 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| GPU reserved memory   |   10800 MB |   10870 MB |  388236 MB |  377436 MB |\n",
            "|       from large pool |   10740 MB |   10740 MB |  380358 MB |  369618 MB |\n",
            "|       from small pool |      60 MB |     130 MB |    7878 MB |    7818 MB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Non-releasable memory |  208188 KB |    2302 MB |   14669 GB |   14669 GB |\n",
            "|       from large pool |  199028 KB |    2291 MB |   14126 GB |   14126 GB |\n",
            "|       from small pool |    9160 KB |      27 MB |     542 GB |     542 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Allocations           |     593    |     595    |    5917 K  |    5916 K  |\n",
            "|       from large pool |     162    |     163    |    2060 K  |    2060 K  |\n",
            "|       from small pool |     431    |     516    |    3856 K  |    3856 K  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Active allocs         |     593    |     595    |    5917 K  |    5916 K  |\n",
            "|       from large pool |     162    |     163    |    2060 K  |    2060 K  |\n",
            "|       from small pool |     431    |     516    |    3856 K  |    3856 K  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| GPU reserved segments |      58    |      93    |    4583    |    4525    |\n",
            "|       from large pool |      28    |      28    |     644    |     616    |\n",
            "|       from small pool |      30    |      65    |    3939    |    3909    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Non-releasable allocs |      54    |      54    |    2664 K  |    2664 K  |\n",
            "|       from large pool |      30    |      30    |    1289 K  |    1289 K  |\n",
            "|       from small pool |      24    |      46    |    1374 K  |    1374 K  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Oversize allocations  |       0    |       0    |       0    |       0    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Oversize GPU segments |       0    |       0    |       0    |       0    |\n",
            "|===========================================================================|\n",
            "\n",
            "2022-03-10 05:42:24 | WARNING | fairseq.trainer | attempting to recover from OOM in forward/backward pass\n",
            "epoch 008:  69% 492/712 [01:36<00:35,  6.13it/s, loss=2.467, ppl=5.53, wps=14388.8, ups=5.06, wpb=2842.4, bsz=128, num_updates=5300, lr=0.0005, gnorm=0.985, train_wall=19, gb_free=3.7, wall=1120]2022-03-10 05:42:59 | WARNING | fairseq.trainer | OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 1.83 GiB (GPU 0; 11.17 GiB total capacity; 8.20 GiB already allocated; 1.58 GiB free; 9.08 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
            "2022-03-10 05:42:59 | WARNING | fairseq.trainer | |===========================================================================|\n",
            "|                  PyTorch CUDA memory summary, device ID 0                 |\n",
            "|---------------------------------------------------------------------------|\n",
            "|            CUDA OOMs: 92           |        cudaMalloc retries: 184       |\n",
            "|===========================================================================|\n",
            "|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Allocated memory      |    8396 MB |    8461 MB |   14691 GB |   14683 GB |\n",
            "|       from large pool |    8338 MB |    8403 MB |   14178 GB |   14170 GB |\n",
            "|       from small pool |      57 MB |      60 MB |     512 GB |     512 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Active memory         |    8396 MB |    8461 MB |   14691 GB |   14683 GB |\n",
            "|       from large pool |    8338 MB |    8403 MB |   14178 GB |   14170 GB |\n",
            "|       from small pool |      57 MB |      60 MB |     512 GB |     512 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| GPU reserved memory   |    9294 MB |   10878 MB |  390188 MB |  380894 MB |\n",
            "|       from large pool |    9232 MB |   10740 MB |  382232 MB |  373000 MB |\n",
            "|       from small pool |      62 MB |     138 MB |    7956 MB |    7894 MB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Non-releasable memory |     897 MB |    1585 MB |   15161 GB |   15160 GB |\n",
            "|       from large pool |     893 MB |    1580 MB |   14601 GB |   14600 GB |\n",
            "|       from small pool |       4 MB |      27 MB |     559 GB |     559 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Allocations           |     593    |     595    |    6112 K  |    6112 K  |\n",
            "|       from large pool |     150    |     154    |    2129 K  |    2129 K  |\n",
            "|       from small pool |     443    |     516    |    3983 K  |    3982 K  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Active allocs         |     593    |     595    |    6112 K  |    6112 K  |\n",
            "|       from large pool |     150    |     154    |    2129 K  |    2129 K  |\n",
            "|       from small pool |     443    |     516    |    3983 K  |    3982 K  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| GPU reserved segments |      58    |      97    |    4623    |    4565    |\n",
            "|       from large pool |      27    |      28    |     645    |     618    |\n",
            "|       from small pool |      31    |      69    |    3978    |    3947    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Non-releasable allocs |      47    |      55    |    2750 K  |    2750 K  |\n",
            "|       from large pool |      22    |      22    |    1331 K  |    1331 K  |\n",
            "|       from small pool |      25    |      49    |    1418 K  |    1418 K  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Oversize allocations  |       0    |       0    |       0    |       0    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Oversize GPU segments |       0    |       0    |       0    |       0    |\n",
            "|===========================================================================|\n",
            "\n",
            "2022-03-10 05:42:59 | WARNING | fairseq.trainer | attempting to recover from OOM in forward/backward pass\n",
            "epoch 008:  72% 510/712 [01:41<00:58,  3.45it/s, loss=2.586, ppl=6.01, wps=13240.3, ups=4.59, wpb=2884.3, bsz=128, num_updates=5400, lr=0.0005, gnorm=1.007, train_wall=21, gb_free=9.7, wall=1142]2022-03-10 05:43:04 | WARNING | fairseq.trainer | OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 1.87 GiB (GPU 0; 11.17 GiB total capacity; 8.57 GiB already allocated; 1.63 GiB free; 9.02 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
            "epoch 008:  72% 511/712 [01:42<01:26,  2.31it/s, loss=2.586, ppl=6.01, wps=13240.3, ups=4.59, wpb=2884.3, bsz=128, num_updates=5400, lr=0.0005, gnorm=1.007, train_wall=21, gb_free=9.7, wall=1142]2022-03-10 05:43:04 | WARNING | fairseq.trainer | |===========================================================================|\n",
            "|                  PyTorch CUDA memory summary, device ID 0                 |\n",
            "|---------------------------------------------------------------------------|\n",
            "|            CUDA OOMs: 93           |        cudaMalloc retries: 186       |\n",
            "|===========================================================================|\n",
            "|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Allocated memory      |    8775 MB |    8842 MB |   14769 GB |   14761 GB |\n",
            "|       from large pool |    8726 MB |    8793 MB |   14255 GB |   14247 GB |\n",
            "|       from small pool |      49 MB |      61 MB |     514 GB |     513 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Active memory         |    8775 MB |    8842 MB |   14769 GB |   14761 GB |\n",
            "|       from large pool |    8726 MB |    8793 MB |   14255 GB |   14247 GB |\n",
            "|       from small pool |      49 MB |      61 MB |     514 GB |     513 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| GPU reserved memory   |    9238 MB |   10818 MB |  393624 MB |  384386 MB |\n",
            "|       from large pool |    9178 MB |   10716 MB |  385628 MB |  376450 MB |\n",
            "|       from small pool |      60 MB |     102 MB |    7996 MB |    7936 MB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Non-releasable memory |  473849 KB |    1536 MB |   15238 GB |   15238 GB |\n",
            "|       from large pool |  462692 KB |    1524 MB |   14677 GB |   14677 GB |\n",
            "|       from small pool |   11157 KB |      17 MB |     561 GB |     561 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Allocations           |     593    |     595    |    6132 K  |    6131 K  |\n",
            "|       from large pool |     160    |     164    |    2137 K  |    2136 K  |\n",
            "|       from small pool |     433    |     516    |    3995 K  |    3994 K  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Active allocs         |     593    |     595    |    6132 K  |    6131 K  |\n",
            "|       from large pool |     160    |     164    |    2137 K  |    2136 K  |\n",
            "|       from small pool |     433    |     516    |    3995 K  |    3994 K  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| GPU reserved segments |      57    |      79    |    4645    |    4588    |\n",
            "|       from large pool |      27    |      28    |     647    |     620    |\n",
            "|       from small pool |      30    |      51    |    3998    |    3968    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Non-releasable allocs |      38    |      40    |    2758 K  |    2758 K  |\n",
            "|       from large pool |      17    |      17    |    1336 K  |    1336 K  |\n",
            "|       from small pool |      21    |      34    |    1422 K  |    1422 K  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Oversize allocations  |       0    |       0    |       0    |       0    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Oversize GPU segments |       0    |       0    |       0    |       0    |\n",
            "|===========================================================================|\n",
            "\n",
            "2022-03-10 05:43:04 | WARNING | fairseq.trainer | attempting to recover from OOM in forward/backward pass\n",
            "epoch 008:  74% 530/712 [01:46<00:44,  4.09it/s, loss=2.586, ppl=6.01, wps=13240.3, ups=4.59, wpb=2884.3, bsz=128, num_updates=5400, lr=0.0005, gnorm=1.007, train_wall=21, gb_free=9.7, wall=1142]2022-03-10 05:43:09 | WARNING | fairseq.trainer | OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 1.70 GiB (GPU 0; 11.17 GiB total capacity; 9.73 GiB already allocated; 309.81 MiB free; 10.35 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
            "2022-03-10 05:43:09 | WARNING | fairseq.trainer | |===========================================================================|\n",
            "|                  PyTorch CUDA memory summary, device ID 0                 |\n",
            "|---------------------------------------------------------------------------|\n",
            "|            CUDA OOMs: 94           |        cudaMalloc retries: 188       |\n",
            "|===========================================================================|\n",
            "|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Allocated memory      |    8218 MB |    9961 MB |   14837 GB |   14829 GB |\n",
            "|       from large pool |    8169 MB |    9912 MB |   14321 GB |   14313 GB |\n",
            "|       from small pool |      49 MB |      60 MB |     515 GB |     515 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Active memory         |    8218 MB |    9961 MB |   14837 GB |   14829 GB |\n",
            "|       from large pool |    8169 MB |    9912 MB |   14321 GB |   14313 GB |\n",
            "|       from small pool |      49 MB |      60 MB |     515 GB |     515 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| GPU reserved memory   |   10598 MB |   10598 MB |  396532 MB |  385934 MB |\n",
            "|       from large pool |   10540 MB |   10540 MB |  388474 MB |  377934 MB |\n",
            "|       from small pool |      58 MB |     122 MB |    8058 MB |    8000 MB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Non-releasable memory |  651025 KB |    1039 MB |   15306 GB |   15306 GB |\n",
            "|       from large pool |  641824 KB |    1030 MB |   14743 GB |   14742 GB |\n",
            "|       from small pool |    9201 KB |      17 MB |     563 GB |     563 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Allocations           |     592    |     595    |    6154 K  |    6153 K  |\n",
            "|       from large pool |     160    |     164    |    2144 K  |    2144 K  |\n",
            "|       from small pool |     432    |     516    |    4009 K  |    4009 K  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Active allocs         |     592    |     595    |    6154 K  |    6153 K  |\n",
            "|       from large pool |     160    |     164    |    2144 K  |    2144 K  |\n",
            "|       from small pool |     432    |     516    |    4009 K  |    4009 K  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| GPU reserved segments |      57    |      89    |    4678    |    4621    |\n",
            "|       from large pool |      28    |      28    |     649    |     621    |\n",
            "|       from small pool |      29    |      61    |    4029    |    4000    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Non-releasable allocs |      41    |      51    |    2768 K  |    2768 K  |\n",
            "|       from large pool |      20    |      21    |    1340 K  |    1340 K  |\n",
            "|       from small pool |      21    |      42    |    1427 K  |    1427 K  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Oversize allocations  |       0    |       0    |       0    |       0    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Oversize GPU segments |       0    |       0    |       0    |       0    |\n",
            "|===========================================================================|\n",
            "\n",
            "2022-03-10 05:43:09 | WARNING | fairseq.trainer | attempting to recover from OOM in forward/backward pass\n",
            "epoch 008:  77% 551/712 [01:51<00:25,  6.20it/s, loss=2.586, ppl=6.01, wps=13240.3, ups=4.59, wpb=2884.3, bsz=128, num_updates=5400, lr=0.0005, gnorm=1.007, train_wall=21, gb_free=9.7, wall=1142]2022-03-10 05:43:14 | WARNING | fairseq.trainer | OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 2.95 GiB (GPU 0; 11.17 GiB total capacity; 10.51 GiB already allocated; 59.81 MiB free; 10.59 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
            "2022-03-10 05:43:14 | WARNING | fairseq.trainer | |===========================================================================|\n",
            "|                  PyTorch CUDA memory summary, device ID 0                 |\n",
            "|---------------------------------------------------------------------------|\n",
            "|            CUDA OOMs: 95           |        cudaMalloc retries: 191       |\n",
            "|===========================================================================|\n",
            "|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Allocated memory      |   10715 MB |   10765 MB |   14902 GB |   14892 GB |\n",
            "|       from large pool |   10664 MB |   10713 MB |   14384 GB |   14374 GB |\n",
            "|       from small pool |      51 MB |      61 MB |     517 GB |     517 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Active memory         |   10715 MB |   10765 MB |   14902 GB |   14892 GB |\n",
            "|       from large pool |   10664 MB |   10713 MB |   14384 GB |   14374 GB |\n",
            "|       from small pool |      51 MB |      61 MB |     517 GB |     517 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| GPU reserved memory   |   10848 MB |   10902 MB |  399722 MB |  388874 MB |\n",
            "|       from large pool |   10790 MB |   10790 MB |  391570 MB |  380780 MB |\n",
            "|       from small pool |      58 MB |     128 MB |    8152 MB |    8094 MB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Non-releasable memory |  135532 KB |    2020 MB |   15372 GB |   15372 GB |\n",
            "|       from large pool |  128472 KB |    2012 MB |   14807 GB |   14806 GB |\n",
            "|       from small pool |    7060 KB |      15 MB |     565 GB |     565 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Allocations           |     593    |     595    |    6176 K  |    6176 K  |\n",
            "|       from large pool |     162    |     163    |    2152 K  |    2152 K  |\n",
            "|       from small pool |     431    |     516    |    4024 K  |    4023 K  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Active allocs         |     593    |     595    |    6176 K  |    6176 K  |\n",
            "|       from large pool |     162    |     163    |    2152 K  |    2152 K  |\n",
            "|       from small pool |     431    |     516    |    4024 K  |    4023 K  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| GPU reserved segments |      65    |      99    |    4735    |    4670    |\n",
            "|       from large pool |      36    |      36    |     659    |     623    |\n",
            "|       from small pool |      29    |      64    |    4076    |    4047    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Non-releasable allocs |      48    |      48    |    2778 K  |    2778 K  |\n",
            "|       from large pool |      26    |      26    |    1345 K  |    1345 K  |\n",
            "|       from small pool |      22    |      33    |    1432 K  |    1432 K  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Oversize allocations  |       0    |       0    |       0    |       0    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Oversize GPU segments |       0    |       0    |       0    |       0    |\n",
            "|===========================================================================|\n",
            "\n",
            "2022-03-10 05:43:14 | WARNING | fairseq.trainer | attempting to recover from OOM in forward/backward pass\n",
            "epoch 008:  78% 554/712 [01:52<00:39,  4.03it/s, loss=2.586, ppl=6.01, wps=13240.3, ups=4.59, wpb=2884.3, bsz=128, num_updates=5400, lr=0.0005, gnorm=1.007, train_wall=21, gb_free=9.7, wall=1142]2022-03-10 05:43:15 | WARNING | fairseq.trainer | OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 1.49 GiB (GPU 0; 11.17 GiB total capacity; 8.10 GiB already allocated; 1.10 GiB free; 9.55 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
            "2022-03-10 05:43:15 | WARNING | fairseq.trainer | |===========================================================================|\n",
            "|                  PyTorch CUDA memory summary, device ID 0                 |\n",
            "|---------------------------------------------------------------------------|\n",
            "|            CUDA OOMs: 96           |        cudaMalloc retries: 193       |\n",
            "|===========================================================================|\n",
            "|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Allocated memory      |    6767 MB |    8296 MB |   14917 GB |   14910 GB |\n",
            "|       from large pool |    6719 MB |    8248 MB |   14399 GB |   14392 GB |\n",
            "|       from small pool |      48 MB |      61 MB |     517 GB |     517 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Active memory         |    6767 MB |    8296 MB |   14917 GB |   14910 GB |\n",
            "|       from large pool |    6719 MB |    8248 MB |   14399 GB |   14392 GB |\n",
            "|       from small pool |      48 MB |      61 MB |     517 GB |     517 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| GPU reserved memory   |    9778 MB |   10856 MB |  401260 MB |  391482 MB |\n",
            "|       from large pool |    9720 MB |   10790 MB |  393100 MB |  383380 MB |\n",
            "|       from small pool |      58 MB |      66 MB |    8160 MB |    8102 MB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Non-releasable memory |    1480 MB |    1481 MB |   15391 GB |   15390 GB |\n",
            "|       from large pool |    1470 MB |    1471 MB |   14826 GB |   14824 GB |\n",
            "|       from small pool |       9 MB |      12 MB |     565 GB |     565 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Allocations           |     592    |     595    |    6179 K  |    6178 K  |\n",
            "|       from large pool |     160    |     164    |    2153 K  |    2153 K  |\n",
            "|       from small pool |     432    |     516    |    4025 K  |    4025 K  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Active allocs         |     592    |     595    |    6179 K  |    6178 K  |\n",
            "|       from large pool |     160    |     164    |    2153 K  |    2153 K  |\n",
            "|       from small pool |     432    |     516    |    4025 K  |    4025 K  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| GPU reserved segments |      64    |      69    |    4740    |    4676    |\n",
            "|       from large pool |      35    |      36    |     660    |     625    |\n",
            "|       from small pool |      29    |      33    |    4080    |    4051    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Non-releasable allocs |      33    |      36    |    2779 K  |    2779 K  |\n",
            "|       from large pool |      12    |      13    |    1346 K  |    1346 K  |\n",
            "|       from small pool |      21    |      29    |    1433 K  |    1433 K  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Oversize allocations  |       0    |       0    |       0    |       0    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Oversize GPU segments |       0    |       0    |       0    |       0    |\n",
            "|===========================================================================|\n",
            "\n",
            "2022-03-10 05:43:15 | WARNING | fairseq.trainer | attempting to recover from OOM in forward/backward pass\n",
            "epoch 008:  79% 566/712 [01:55<00:26,  5.51it/s, loss=2.586, ppl=6.01, wps=13240.3, ups=4.59, wpb=2884.3, bsz=128, num_updates=5400, lr=0.0005, gnorm=1.007, train_wall=21, gb_free=9.7, wall=1142]2022-03-10 05:43:18 | WARNING | fairseq.trainer | OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 1.52 GiB (GPU 0; 11.17 GiB total capacity; 8.52 GiB already allocated; 1.08 GiB free; 9.57 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
            "2022-03-10 05:43:18 | WARNING | fairseq.trainer | |===========================================================================|\n",
            "|                  PyTorch CUDA memory summary, device ID 0                 |\n",
            "|---------------------------------------------------------------------------|\n",
            "|            CUDA OOMs: 97           |        cudaMalloc retries: 195       |\n",
            "|===========================================================================|\n",
            "|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Allocated memory      |    7168 MB |    8720 MB |   14955 GB |   14948 GB |\n",
            "|       from large pool |    7119 MB |    8671 MB |   14436 GB |   14429 GB |\n",
            "|       from small pool |      48 MB |      60 MB |     519 GB |     519 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Active memory         |    7168 MB |    8720 MB |   14955 GB |   14948 GB |\n",
            "|       from large pool |    7119 MB |    8671 MB |   14436 GB |   14429 GB |\n",
            "|       from small pool |      48 MB |      60 MB |     519 GB |     519 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| GPU reserved memory   |    9800 MB |   10338 MB |  404902 MB |  395102 MB |\n",
            "|       from large pool |    9742 MB |   10226 MB |  396688 MB |  386946 MB |\n",
            "|       from small pool |      58 MB |     112 MB |    8214 MB |    8156 MB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Non-releasable memory |    1079 MB |    1079 MB |   15425 GB |   15424 GB |\n",
            "|       from large pool |    1070 MB |    1070 MB |   14858 GB |   14857 GB |\n",
            "|       from small pool |       9 MB |      18 MB |     566 GB |     566 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Allocations           |     592    |     595    |    6192 K  |    6191 K  |\n",
            "|       from large pool |     160    |     164    |    2157 K  |    2157 K  |\n",
            "|       from small pool |     432    |     516    |    4034 K  |    4033 K  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Active allocs         |     592    |     595    |    6192 K  |    6191 K  |\n",
            "|       from large pool |     160    |     164    |    2157 K  |    2157 K  |\n",
            "|       from small pool |     432    |     516    |    4034 K  |    4033 K  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| GPU reserved segments |      64    |      92    |    4770    |    4706    |\n",
            "|       from large pool |      35    |      36    |     663    |     628    |\n",
            "|       from small pool |      29    |      56    |    4107    |    4078    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Non-releasable allocs |      43    |      43    |    2785 K  |    2785 K  |\n",
            "|       from large pool |      20    |      20    |    1349 K  |    1349 K  |\n",
            "|       from small pool |      23    |      35    |    1436 K  |    1436 K  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Oversize allocations  |       0    |       0    |       0    |       0    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Oversize GPU segments |       0    |       0    |       0    |       0    |\n",
            "|===========================================================================|\n",
            "\n",
            "2022-03-10 05:43:18 | WARNING | fairseq.trainer | attempting to recover from OOM in forward/backward pass\n",
            "epoch 008:  81% 574/712 [01:56<00:20,  6.76it/s, loss=2.586, ppl=6.01, wps=13240.3, ups=4.59, wpb=2884.3, bsz=128, num_updates=5400, lr=0.0005, gnorm=1.007, train_wall=21, gb_free=9.7, wall=1142]2022-03-10 05:43:19 | WARNING | fairseq.trainer | OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 2.96 GiB (GPU 0; 11.17 GiB total capacity; 10.42 GiB already allocated; 103.81 MiB free; 10.55 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
            "2022-03-10 05:43:19 | WARNING | fairseq.trainer | |===========================================================================|\n",
            "|                  PyTorch CUDA memory summary, device ID 0                 |\n",
            "|---------------------------------------------------------------------------|\n",
            "|            CUDA OOMs: 98           |        cudaMalloc retries: 196       |\n",
            "|===========================================================================|\n",
            "|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Allocated memory      |   10621 MB |   10671 MB |   14979 GB |   14969 GB |\n",
            "|       from large pool |   10570 MB |   10620 MB |   14460 GB |   14449 GB |\n",
            "|       from small pool |      51 MB |      60 MB |     519 GB |     519 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Active memory         |   10621 MB |   10671 MB |   14979 GB |   14969 GB |\n",
            "|       from large pool |   10570 MB |   10620 MB |   14460 GB |   14449 GB |\n",
            "|       from small pool |      51 MB |      60 MB |     519 GB |     519 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| GPU reserved memory   |   10804 MB |   10842 MB |  407496 MB |  396692 MB |\n",
            "|       from large pool |   10746 MB |   10746 MB |  399244 MB |  388498 MB |\n",
            "|       from small pool |      58 MB |      96 MB |    8252 MB |    8194 MB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Non-releasable memory |  186672 KB |    1975 MB |   15444 GB |   15444 GB |\n",
            "|       from large pool |  179581 KB |    1966 MB |   14876 GB |   14876 GB |\n",
            "|       from small pool |    7091 KB |      18 MB |     567 GB |     567 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Allocations           |     593    |     595    |    6200 K  |    6199 K  |\n",
            "|       from large pool |     162    |     163    |    2160 K  |    2160 K  |\n",
            "|       from small pool |     431    |     516    |    4039 K  |    4038 K  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Active allocs         |     593    |     595    |    6200 K  |    6199 K  |\n",
            "|       from large pool |     162    |     163    |    2160 K  |    2160 K  |\n",
            "|       from small pool |     431    |     516    |    4039 K  |    4038 K  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| GPU reserved segments |      69    |      88    |    4795    |    4726    |\n",
            "|       from large pool |      40    |      40    |     669    |     629    |\n",
            "|       from small pool |      29    |      48    |    4126    |    4097    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Non-releasable allocs |      52    |      52    |    2788 K  |    2788 K  |\n",
            "|       from large pool |      27    |      27    |    1350 K  |    1350 K  |\n",
            "|       from small pool |      25    |      41    |    1438 K  |    1438 K  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Oversize allocations  |       0    |       0    |       0    |       0    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Oversize GPU segments |       0    |       0    |       0    |       0    |\n",
            "|===========================================================================|\n",
            "\n",
            "2022-03-10 05:43:19 | WARNING | fairseq.trainer | attempting to recover from OOM in forward/backward pass\n",
            "epoch 008: 100% 711/712 [02:24<00:00,  5.06it/s, loss=2.54, ppl=5.81, wps=10797.2, ups=4.02, wpb=2688.9, bsz=126.9, num_updates=5500, lr=0.0005, gnorm=1.087, train_wall=20, gb_free=9.8, wall=1166]2022-03-10 05:43:47 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "\n",
            "epoch 008 | valid on 'valid' subset:   0% 0/8 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 008 | valid on 'valid' subset:  12% 1/8 [00:00<00:01,  5.75it/s]\u001b[A\n",
            "epoch 008 | valid on 'valid' subset:  38% 3/8 [00:00<00:00,  9.89it/s]\u001b[A\n",
            "epoch 008 | valid on 'valid' subset:  62% 5/8 [00:00<00:00, 10.78it/s]\u001b[A\n",
            "epoch 008 | valid on 'valid' subset:  88% 7/8 [00:00<00:00,  9.77it/s]\u001b[A\n",
            "                                                                      \u001b[A2022-03-10 05:43:48 | INFO | valid | epoch 008 | valid on 'valid' subset | loss 3.802 | ppl 13.95 | wps 20372.2 | wpb 2800.1 | bsz 125 | num_updates 5598 | best_loss 3.691\n",
            "2022-03-10 05:43:48 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 8 @ 5598 updates\n",
            "2022-03-10 05:43:48 | INFO | fairseq.trainer | Saving checkpoint to /content/checkpoints/checkpoint8.pt\n",
            "2022-03-10 05:43:49 | INFO | fairseq.trainer | Finished saving checkpoint to /content/checkpoints/checkpoint8.pt\n",
            "2022-03-10 05:43:50 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/checkpoint8.pt (epoch 8 @ 5598 updates, score 3.802) (writing took 1.3994062639999356 seconds)\n",
            "2022-03-10 05:43:50 | INFO | fairseq_cli.train | end of epoch 8 (average epoch stats below)\n",
            "2022-03-10 05:43:50 | INFO | train | epoch 008 | loss 2.486 | ppl 5.6 | wps 13190.7 | ups 4.75 | wpb 2774.5 | bsz 127.7 | num_updates 5598 | lr 0.0005 | gnorm 1.018 | train_wall 135 | gb_free 10.3 | wall 1188\n",
            "epoch 009:   0% 0/712 [00:00<?, ?it/s]2022-03-10 05:43:50 | INFO | fairseq.data.iterators | grouped total_num_itrs = 712\n",
            "2022-03-10 05:43:50 | INFO | fairseq.trainer | begin training epoch 9\n",
            "2022-03-10 05:43:50 | INFO | fairseq_cli.train | Start iterating over samples\n",
            "epoch 009:   4% 30/712 [00:04<01:39,  6.84it/s, loss=2.559, ppl=5.89, wps=12485.3, ups=4.49, wpb=2781.7, bsz=128, num_updates=5600, lr=0.0005, gnorm=1.032, train_wall=19, gb_free=9.7, wall=1189]2022-03-10 05:43:55 | WARNING | fairseq.trainer | OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 2.51 GiB (GPU 0; 11.17 GiB total capacity; 10.39 GiB already allocated; 131.81 MiB free; 10.52 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
            "2022-03-10 05:43:55 | WARNING | fairseq.trainer | |===========================================================================|\n",
            "|                  PyTorch CUDA memory summary, device ID 0                 |\n",
            "|---------------------------------------------------------------------------|\n",
            "|            CUDA OOMs: 99           |        cudaMalloc retries: 198       |\n",
            "|===========================================================================|\n",
            "|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Allocated memory      |   10599 MB |   10641 MB |   15423 GB |   15412 GB |\n",
            "|       from large pool |   10548 MB |   10590 MB |   14887 GB |   14877 GB |\n",
            "|       from small pool |      51 MB |      60 MB |     535 GB |     535 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Active memory         |   10599 MB |   10641 MB |   15423 GB |   15412 GB |\n",
            "|       from large pool |   10548 MB |   10590 MB |   14887 GB |   14877 GB |\n",
            "|       from small pool |      51 MB |      60 MB |     535 GB |     535 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| GPU reserved memory   |   10776 MB |   10878 MB |  411598 MB |  400822 MB |\n",
            "|       from large pool |   10716 MB |   10748 MB |  403226 MB |  392510 MB |\n",
            "|       from small pool |      60 MB |     130 MB |    8372 MB |    8312 MB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Non-releasable memory |  180284 KB |    2116 MB |   15902 GB |   15902 GB |\n",
            "|       from large pool |  171124 KB |    2106 MB |   15318 GB |   15318 GB |\n",
            "|       from small pool |    9160 KB |      16 MB |     584 GB |     584 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Allocations           |     593    |     595    |    6390 K  |    6390 K  |\n",
            "|       from large pool |     162    |     163    |    2227 K  |    2227 K  |\n",
            "|       from small pool |     431    |     516    |    4163 K  |    4162 K  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Active allocs         |     593    |     595    |    6390 K  |    6390 K  |\n",
            "|       from large pool |     162    |     163    |    2227 K  |    2227 K  |\n",
            "|       from small pool |     431    |     516    |    4163 K  |    4162 K  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| GPU reserved segments |      71    |     108    |    4864    |    4793    |\n",
            "|       from large pool |      41    |      43    |     678    |     637    |\n",
            "|       from small pool |      30    |      65    |    4186    |    4156    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Non-releasable allocs |      59    |      59    |    2873 K  |    2873 K  |\n",
            "|       from large pool |      35    |      35    |    1392 K  |    1392 K  |\n",
            "|       from small pool |      24    |      36    |    1481 K  |    1480 K  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Oversize allocations  |       0    |       0    |       0    |       0    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Oversize GPU segments |       0    |       0    |       0    |       0    |\n",
            "|===========================================================================|\n",
            "\n",
            "2022-03-10 05:43:55 | WARNING | fairseq.trainer | attempting to recover from OOM in forward/backward pass\n",
            "epoch 009:  34% 239/712 [00:45<01:24,  5.58it/s, loss=2.257, ppl=4.78, wps=13535.1, ups=5.03, wpb=2689.7, bsz=126.9, num_updates=5800, lr=0.0005, gnorm=1.001, train_wall=19, gb_free=10, wall=1228]2022-03-10 05:44:36 | WARNING | fairseq.trainer | OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 1.52 GiB (GPU 0; 11.17 GiB total capacity; 8.52 GiB already allocated; 329.81 MiB free; 10.33 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
            "2022-03-10 05:44:36 | WARNING | fairseq.trainer | |===========================================================================|\n",
            "|                  PyTorch CUDA memory summary, device ID 0                 |\n",
            "|---------------------------------------------------------------------------|\n",
            "|            CUDA OOMs: 100          |        cudaMalloc retries: 201       |\n",
            "|===========================================================================|\n",
            "|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Allocated memory      |    7167 MB |    8719 MB |   15965 GB |   15958 GB |\n",
            "|       from large pool |    7118 MB |    8670 MB |   15410 GB |   15403 GB |\n",
            "|       from small pool |      48 MB |      61 MB |     554 GB |     554 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Active memory         |    7167 MB |    8719 MB |   15965 GB |   15958 GB |\n",
            "|       from large pool |    7118 MB |    8670 MB |   15410 GB |   15403 GB |\n",
            "|       from small pool |      48 MB |      61 MB |     554 GB |     554 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| GPU reserved memory   |   10578 MB |   10578 MB |  416088 MB |  405510 MB |\n",
            "|       from large pool |   10520 MB |   10520 MB |  407562 MB |  397042 MB |\n",
            "|       from small pool |      58 MB |     138 MB |    8526 MB |    8468 MB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Non-releasable memory |    1858 MB |    1883 MB |   16453 GB |   16451 GB |\n",
            "|       from large pool |    1849 MB |    1873 MB |   15848 GB |   15846 GB |\n",
            "|       from small pool |       9 MB |      12 MB |     605 GB |     605 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Allocations           |     592    |     595    |    6626 K  |    6625 K  |\n",
            "|       from large pool |     160    |     164    |    2310 K  |    2310 K  |\n",
            "|       from small pool |     432    |     516    |    4315 K  |    4315 K  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Active allocs         |     592    |     595    |    6626 K  |    6625 K  |\n",
            "|       from large pool |     160    |     164    |    2310 K  |    2310 K  |\n",
            "|       from small pool |     432    |     516    |    4315 K  |    4315 K  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| GPU reserved segments |      68    |     109    |    4944    |    4876    |\n",
            "|       from large pool |      39    |      40    |     681    |     642    |\n",
            "|       from small pool |      29    |      69    |    4263    |    4234    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Non-releasable allocs |      44    |      45    |    2978 K  |    2978 K  |\n",
            "|       from large pool |      21    |      22    |    1443 K  |    1443 K  |\n",
            "|       from small pool |      23    |      29    |    1535 K  |    1535 K  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Oversize allocations  |       0    |       0    |       0    |       0    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Oversize GPU segments |       0    |       0    |       0    |       0    |\n",
            "|===========================================================================|\n",
            "\n",
            "2022-03-10 05:44:36 | WARNING | fairseq.trainer | attempting to recover from OOM in forward/backward pass\n",
            "epoch 009:  53% 375/712 [01:11<00:47,  7.08it/s, loss=2.15, ppl=4.44, wps=13731.1, ups=5.36, wpb=2560.7, bsz=128, num_updates=5900, lr=0.0005, gnorm=1.022, train_wall=17, gb_free=10.4, wall=1246]2022-03-10 05:45:02 | WARNING | fairseq.trainer | OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 2.96 GiB (GPU 0; 11.17 GiB total capacity; 10.43 GiB already allocated; 59.81 MiB free; 10.59 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
            "epoch 009:  53% 376/712 [01:12<01:33,  3.61it/s, loss=2.15, ppl=4.44, wps=13731.1, ups=5.36, wpb=2560.7, bsz=128, num_updates=5900, lr=0.0005, gnorm=1.022, train_wall=17, gb_free=10.4, wall=1246]2022-03-10 05:45:02 | WARNING | fairseq.trainer | |===========================================================================|\n",
            "|                  PyTorch CUDA memory summary, device ID 0                 |\n",
            "|---------------------------------------------------------------------------|\n",
            "|            CUDA OOMs: 101          |        cudaMalloc retries: 204       |\n",
            "|===========================================================================|\n",
            "|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Allocated memory      |   10625 MB |   10675 MB |   16295 GB |   16285 GB |\n",
            "|       from large pool |   10574 MB |   10624 MB |   15727 GB |   15717 GB |\n",
            "|       from small pool |      51 MB |      60 MB |     567 GB |     567 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Active memory         |   10625 MB |   10675 MB |   16295 GB |   16285 GB |\n",
            "|       from large pool |   10574 MB |   10624 MB |   15727 GB |   15717 GB |\n",
            "|       from small pool |      51 MB |      60 MB |     567 GB |     567 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| GPU reserved memory   |   10848 MB |   10872 MB |  419714 MB |  408866 MB |\n",
            "|       from large pool |   10784 MB |   10784 MB |  411028 MB |  400244 MB |\n",
            "|       from small pool |      64 MB |     138 MB |    8686 MB |    8622 MB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Non-releasable memory |  227376 KB |    1619 MB |   16806 GB |   16806 GB |\n",
            "|       from large pool |  214141 KB |    1606 MB |   16186 GB |   16186 GB |\n",
            "|       from small pool |   13235 KB |      19 MB |     619 GB |     619 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Allocations           |     593    |     595    |    6778 K  |    6777 K  |\n",
            "|       from large pool |     162    |     163    |    2362 K  |    2362 K  |\n",
            "|       from small pool |     431    |     516    |    4415 K  |    4415 K  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Active allocs         |     593    |     595    |    6778 K  |    6777 K  |\n",
            "|       from large pool |     162    |     163    |    2362 K  |    2362 K  |\n",
            "|       from small pool |     431    |     516    |    4415 K  |    4415 K  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| GPU reserved segments |      75    |     111    |    5031    |    4956    |\n",
            "|       from large pool |      43    |      43    |     688    |     645    |\n",
            "|       from small pool |      32    |      69    |    4343    |    4311    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Non-releasable allocs |      53    |      54    |    3046 K  |    3046 K  |\n",
            "|       from large pool |      27    |      28    |    1475 K  |    1475 K  |\n",
            "|       from small pool |      26    |      44    |    1570 K  |    1570 K  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Oversize allocations  |       0    |       0    |       0    |       0    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Oversize GPU segments |       0    |       0    |       0    |       0    |\n",
            "|===========================================================================|\n",
            "\n",
            "2022-03-10 05:45:02 | WARNING | fairseq.trainer | attempting to recover from OOM in forward/backward pass\n",
            "epoch 009:  53% 378/712 [01:12<01:25,  3.89it/s, loss=2.15, ppl=4.44, wps=13731.1, ups=5.36, wpb=2560.7, bsz=128, num_updates=5900, lr=0.0005, gnorm=1.022, train_wall=17, gb_free=10.4, wall=1246]2022-03-10 05:45:03 | WARNING | fairseq.trainer | OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 1.87 GiB (GPU 0; 11.17 GiB total capacity; 8.57 GiB already allocated; 1007.81 MiB free; 9.67 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
            "2022-03-10 05:45:03 | WARNING | fairseq.trainer | |===========================================================================|\n",
            "|                  PyTorch CUDA memory summary, device ID 0                 |\n",
            "|---------------------------------------------------------------------------|\n",
            "|            CUDA OOMs: 102          |        cudaMalloc retries: 205       |\n",
            "|===========================================================================|\n",
            "|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Allocated memory      |    8773 MB |    8840 MB |   16312 GB |   16303 GB |\n",
            "|       from large pool |    8724 MB |    8791 MB |   15744 GB |   15735 GB |\n",
            "|       from small pool |      49 MB |      61 MB |     567 GB |     567 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Active memory         |    8773 MB |    8840 MB |   16312 GB |   16303 GB |\n",
            "|       from large pool |    8724 MB |    8791 MB |   15744 GB |   15735 GB |\n",
            "|       from small pool |      49 MB |      61 MB |     567 GB |     567 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| GPU reserved memory   |    9900 MB |   10858 MB |  419724 MB |  409824 MB |\n",
            "|       from large pool |    9840 MB |   10784 MB |  411028 MB |  401188 MB |\n",
            "|       from small pool |      60 MB |      74 MB |    8696 MB |    8636 MB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Non-releasable memory |    1126 MB |    1444 MB |   16825 GB |   16824 GB |\n",
            "|       from large pool |    1115 MB |    1432 MB |   16205 GB |   16204 GB |\n",
            "|       from small pool |      10 MB |      14 MB |     620 GB |     619 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Allocations           |     593    |     595    |    6781 K  |    6780 K  |\n",
            "|       from large pool |     160    |     164    |    2363 K  |    2363 K  |\n",
            "|       from small pool |     433    |     516    |    4417 K  |    4416 K  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Active allocs         |     593    |     595    |    6781 K  |    6780 K  |\n",
            "|       from large pool |     160    |     164    |    2363 K  |    2363 K  |\n",
            "|       from small pool |     433    |     516    |    4417 K  |    4416 K  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| GPU reserved segments |      69    |      80    |    5036    |    4967    |\n",
            "|       from large pool |      39    |      43    |     688    |     649    |\n",
            "|       from small pool |      30    |      37    |    4348    |    4318    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Non-releasable allocs |      51    |      54    |    3047 K  |    3047 K  |\n",
            "|       from large pool |      29    |      29    |    1475 K  |    1475 K  |\n",
            "|       from small pool |      22    |      28    |    1571 K  |    1571 K  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Oversize allocations  |       0    |       0    |       0    |       0    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Oversize GPU segments |       0    |       0    |       0    |       0    |\n",
            "|===========================================================================|\n",
            "\n",
            "2022-03-10 05:45:03 | WARNING | fairseq.trainer | attempting to recover from OOM in forward/backward pass\n",
            "epoch 009:  55% 393/712 [01:15<00:54,  5.85it/s, loss=2.15, ppl=4.44, wps=13731.1, ups=5.36, wpb=2560.7, bsz=128, num_updates=5900, lr=0.0005, gnorm=1.022, train_wall=17, gb_free=10.4, wall=1246]2022-03-10 05:45:05 | WARNING | fairseq.trainer | OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 1.76 GiB (GPU 0; 11.17 GiB total capacity; 9.04 GiB already allocated; 1009.81 MiB free; 9.67 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
            "2022-03-10 05:45:05 | WARNING | fairseq.trainer | |===========================================================================|\n",
            "|                  PyTorch CUDA memory summary, device ID 0                 |\n",
            "|---------------------------------------------------------------------------|\n",
            "|            CUDA OOMs: 103          |        cudaMalloc retries: 206       |\n",
            "|===========================================================================|\n",
            "|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Allocated memory      |    9261 MB |    9331 MB |   16349 GB |   16340 GB |\n",
            "|       from large pool |    9212 MB |    9281 MB |   15780 GB |   15771 GB |\n",
            "|       from small pool |      49 MB |      60 MB |     569 GB |     569 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Active memory         |    9261 MB |    9331 MB |   16349 GB |   16340 GB |\n",
            "|       from large pool |    9212 MB |    9281 MB |   15780 GB |   15771 GB |\n",
            "|       from small pool |      49 MB |      60 MB |     569 GB |     569 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| GPU reserved memory   |    9898 MB |    9944 MB |  419768 MB |  409870 MB |\n",
            "|       from large pool |    9840 MB |    9840 MB |  411028 MB |  401188 MB |\n",
            "|       from small pool |      58 MB |     104 MB |    8740 MB |    8682 MB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Non-releasable memory |  651828 KB |    1426 MB |   16863 GB |   16862 GB |\n",
            "|       from large pool |  643016 KB |    1416 MB |   16241 GB |   16240 GB |\n",
            "|       from small pool |    8812 KB |      19 MB |     621 GB |     621 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Allocations           |     593    |     595    |    6797 K  |    6796 K  |\n",
            "|       from large pool |     160    |     164    |    2369 K  |    2369 K  |\n",
            "|       from small pool |     433    |     516    |    4427 K  |    4427 K  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Active allocs         |     593    |     595    |    6797 K  |    6796 K  |\n",
            "|       from large pool |     160    |     164    |    2369 K  |    2369 K  |\n",
            "|       from small pool |     433    |     516    |    4427 K  |    4427 K  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| GPU reserved segments |      68    |      91    |    5058    |    4990    |\n",
            "|       from large pool |      39    |      39    |     688    |     649    |\n",
            "|       from small pool |      29    |      52    |    4370    |    4341    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Non-releasable allocs |      53    |      53    |    3054 K  |    3054 K  |\n",
            "|       from large pool |      31    |      31    |    1479 K  |    1479 K  |\n",
            "|       from small pool |      22    |      44    |    1574 K  |    1574 K  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Oversize allocations  |       0    |       0    |       0    |       0    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Oversize GPU segments |       0    |       0    |       0    |       0    |\n",
            "|===========================================================================|\n",
            "\n",
            "2022-03-10 05:45:05 | WARNING | fairseq.trainer | attempting to recover from OOM in forward/backward pass\n",
            "epoch 009:  62% 443/712 [01:26<01:18,  3.45it/s, loss=2.392, ppl=5.25, wps=13902.8, ups=4.83, wpb=2876.4, bsz=126.7, num_updates=6000, lr=0.0005, gnorm=1.08, train_wall=19, gb_free=10.4, wall=1267]2022-03-10 05:45:16 | WARNING | fairseq.trainer | OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 1.49 GiB (GPU 0; 11.17 GiB total capacity; 8.10 GiB already allocated; 811.81 MiB free; 9.86 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
            "2022-03-10 05:45:16 | WARNING | fairseq.trainer | |===========================================================================|\n",
            "|                  PyTorch CUDA memory summary, device ID 0                 |\n",
            "|---------------------------------------------------------------------------|\n",
            "|            CUDA OOMs: 104          |        cudaMalloc retries: 209       |\n",
            "|===========================================================================|\n",
            "|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Allocated memory      |    6769 MB |    8298 MB |   16496 GB |   16489 GB |\n",
            "|       from large pool |    6721 MB |    8249 MB |   15922 GB |   15915 GB |\n",
            "|       from small pool |      48 MB |      60 MB |     574 GB |     574 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Active memory         |    6769 MB |    8298 MB |   16496 GB |   16489 GB |\n",
            "|       from large pool |    6721 MB |    8249 MB |   15922 GB |   15915 GB |\n",
            "|       from small pool |      48 MB |      60 MB |     574 GB |     574 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| GPU reserved memory   |   10096 MB |   10096 MB |  422744 MB |  412648 MB |\n",
            "|       from large pool |   10038 MB |   10038 MB |  413858 MB |  403820 MB |\n",
            "|       from small pool |      58 MB |     134 MB |    8886 MB |    8828 MB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Non-releasable memory |    1796 MB |    1797 MB |   17012 GB |   17011 GB |\n",
            "|       from large pool |    1786 MB |    1788 MB |   16386 GB |   16384 GB |\n",
            "|       from small pool |       9 MB |      18 MB |     626 GB |     626 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Allocations           |     592    |     595    |    6852 K  |    6851 K  |\n",
            "|       from large pool |     160    |     164    |    2388 K  |    2388 K  |\n",
            "|       from small pool |     432    |     516    |    4464 K  |    4463 K  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Active allocs         |     592    |     595    |    6852 K  |    6851 K  |\n",
            "|       from large pool |     160    |     164    |    2388 K  |    2388 K  |\n",
            "|       from small pool |     432    |     516    |    4464 K  |    4463 K  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| GPU reserved segments |      68    |     106    |    5133    |    5065    |\n",
            "|       from large pool |      39    |      39    |     690    |     651    |\n",
            "|       from small pool |      29    |      67    |    4443    |    4414    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Non-releasable allocs |      39    |      53    |    3078 K  |    3078 K  |\n",
            "|       from large pool |      16    |      17    |    1490 K  |    1490 K  |\n",
            "|       from small pool |      23    |      49    |    1588 K  |    1587 K  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Oversize allocations  |       0    |       0    |       0    |       0    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Oversize GPU segments |       0    |       0    |       0    |       0    |\n",
            "|===========================================================================|\n",
            "\n",
            "2022-03-10 05:45:16 | WARNING | fairseq.trainer | attempting to recover from OOM in forward/backward pass\n",
            "epoch 009:  65% 464/712 [01:30<00:42,  5.79it/s, loss=2.392, ppl=5.25, wps=13902.8, ups=4.83, wpb=2876.4, bsz=126.7, num_updates=6000, lr=0.0005, gnorm=1.08, train_wall=19, gb_free=10.4, wall=1267]2022-03-10 05:45:20 | WARNING | fairseq.trainer | OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 1.44 GiB (GPU 0; 11.17 GiB total capacity; 7.98 GiB already allocated; 865.81 MiB free; 9.81 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
            "2022-03-10 05:45:20 | WARNING | fairseq.trainer | |===========================================================================|\n",
            "|                  PyTorch CUDA memory summary, device ID 0                 |\n",
            "|---------------------------------------------------------------------------|\n",
            "|            CUDA OOMs: 105          |        cudaMalloc retries: 210       |\n",
            "|===========================================================================|\n",
            "|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Allocated memory      |    6697 MB |    8173 MB |   16551 GB |   16544 GB |\n",
            "|       from large pool |    6648 MB |    8124 MB |   15975 GB |   15969 GB |\n",
            "|       from small pool |      48 MB |      60 MB |     575 GB |     575 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Active memory         |    6697 MB |    8173 MB |   16551 GB |   16544 GB |\n",
            "|       from large pool |    6648 MB |    8124 MB |   15975 GB |   15969 GB |\n",
            "|       from small pool |      48 MB |      60 MB |     575 GB |     575 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| GPU reserved memory   |   10042 MB |   10090 MB |  424268 MB |  414226 MB |\n",
            "|       from large pool |    9984 MB |    9984 MB |  415334 MB |  405350 MB |\n",
            "|       from small pool |      58 MB |     106 MB |    8934 MB |    8876 MB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Non-releasable memory |    1868 MB |    1868 MB |   17065 GB |   17063 GB |\n",
            "|       from large pool |    1859 MB |    1859 MB |   16437 GB |   16435 GB |\n",
            "|       from small pool |       9 MB |      17 MB |     628 GB |     628 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Allocations           |     592    |     595    |    6875 K  |    6874 K  |\n",
            "|       from large pool |     160    |     164    |    2396 K  |    2396 K  |\n",
            "|       from small pool |     432    |     516    |    4478 K  |    4478 K  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Active allocs         |     592    |     595    |    6875 K  |    6874 K  |\n",
            "|       from large pool |     160    |     164    |    2396 K  |    2396 K  |\n",
            "|       from small pool |     432    |     516    |    4478 K  |    4478 K  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| GPU reserved segments |      68    |      92    |    5158    |    5090    |\n",
            "|       from large pool |      39    |      39    |     691    |     652    |\n",
            "|       from small pool |      29    |      53    |    4467    |    4438    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Non-releasable allocs |      52    |      52    |    3089 K  |    3089 K  |\n",
            "|       from large pool |      28    |      28    |    1496 K  |    1496 K  |\n",
            "|       from small pool |      24    |      39    |    1593 K  |    1592 K  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Oversize allocations  |       0    |       0    |       0    |       0    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Oversize GPU segments |       0    |       0    |       0    |       0    |\n",
            "|===========================================================================|\n",
            "\n",
            "2022-03-10 05:45:20 | WARNING | fairseq.trainer | attempting to recover from OOM in forward/backward pass\n",
            "epoch 009:  71% 502/712 [01:36<00:26,  7.84it/s, loss=2.392, ppl=5.25, wps=13902.8, ups=4.83, wpb=2876.4, bsz=126.7, num_updates=6000, lr=0.0005, gnorm=1.08, train_wall=19, gb_free=10.4, wall=1267]2022-03-10 05:45:27 | WARNING | fairseq.trainer | OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 1.54 GiB (GPU 0; 11.17 GiB total capacity; 8.23 GiB already allocated; 811.81 MiB free; 9.86 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
            "2022-03-10 05:45:27 | WARNING | fairseq.trainer | |===========================================================================|\n",
            "|                  PyTorch CUDA memory summary, device ID 0                 |\n",
            "|---------------------------------------------------------------------------|\n",
            "|            CUDA OOMs: 106          |        cudaMalloc retries: 211       |\n",
            "|===========================================================================|\n",
            "|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Allocated memory      |    6852 MB |    8427 MB |   16630 GB |   16624 GB |\n",
            "|       from large pool |    6794 MB |    8369 MB |   16051 GB |   16044 GB |\n",
            "|       from small pool |      58 MB |      60 MB |     579 GB |     579 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Active memory         |    6852 MB |    8427 MB |   16630 GB |   16624 GB |\n",
            "|       from large pool |    6794 MB |    8369 MB |   16051 GB |   16044 GB |\n",
            "|       from small pool |      58 MB |      60 MB |     579 GB |     579 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| GPU reserved memory   |   10096 MB |   10212 MB |  425914 MB |  415818 MB |\n",
            "|       from large pool |   10034 MB |   10084 MB |  416910 MB |  406876 MB |\n",
            "|       from small pool |      62 MB |     128 MB |    9004 MB |    8942 MB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Non-releasable memory |    1667 MB |    1691 MB |   17147 GB |   17145 GB |\n",
            "|       from large pool |    1663 MB |    1687 MB |   16514 GB |   16512 GB |\n",
            "|       from small pool |       3 MB |      33 MB |     632 GB |     632 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Allocations           |     592    |     595    |    6917 K  |    6916 K  |\n",
            "|       from large pool |     150    |     154    |    2410 K  |    2410 K  |\n",
            "|       from small pool |     442    |     516    |    4506 K  |    4506 K  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Active allocs         |     592    |     595    |    6917 K  |    6916 K  |\n",
            "|       from large pool |     150    |     154    |    2410 K  |    2410 K  |\n",
            "|       from small pool |     442    |     516    |    4506 K  |    4506 K  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| GPU reserved segments |      69    |     103    |    5194    |    5125    |\n",
            "|       from large pool |      38    |      39    |     692    |     654    |\n",
            "|       from small pool |      31    |      64    |    4502    |    4471    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Non-releasable allocs |      49    |      52    |    3108 K  |    3108 K  |\n",
            "|       from large pool |      26    |      27    |    1504 K  |    1504 K  |\n",
            "|       from small pool |      23    |      50    |    1603 K  |    1603 K  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Oversize allocations  |       0    |       0    |       0    |       0    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Oversize GPU segments |       0    |       0    |       0    |       0    |\n",
            "|===========================================================================|\n",
            "\n",
            "2022-03-10 05:45:27 | WARNING | fairseq.trainer | attempting to recover from OOM in forward/backward pass\n",
            "epoch 009:  72% 512/712 [01:39<01:14,  2.67it/s, loss=2.281, ppl=4.86, wps=13261.1, ups=5.03, wpb=2635, bsz=128, num_updates=6100, lr=0.0005, gnorm=1.07, train_wall=18, gb_free=10.4, wall=1287]2022-03-10 05:45:30 | WARNING | fairseq.trainer | OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 1.44 GiB (GPU 0; 11.17 GiB total capacity; 7.69 GiB already allocated; 915.81 MiB free; 9.76 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
            "2022-03-10 05:45:30 | WARNING | fairseq.trainer | |===========================================================================|\n",
            "|                  PyTorch CUDA memory summary, device ID 0                 |\n",
            "|---------------------------------------------------------------------------|\n",
            "|            CUDA OOMs: 107          |        cudaMalloc retries: 213       |\n",
            "|===========================================================================|\n",
            "|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Allocated memory      |    6400 MB |    7876 MB |   16676 GB |   16669 GB |\n",
            "|       from large pool |    6351 MB |    7827 MB |   16096 GB |   16089 GB |\n",
            "|       from small pool |      48 MB |      61 MB |     580 GB |     579 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Active memory         |    6400 MB |    7876 MB |   16676 GB |   16669 GB |\n",
            "|       from large pool |    6351 MB |    7827 MB |   16096 GB |   16089 GB |\n",
            "|       from small pool |      48 MB |      61 MB |     580 GB |     579 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| GPU reserved memory   |    9992 MB |    9992 MB |  428714 MB |  418722 MB |\n",
            "|       from large pool |    9934 MB |    9934 MB |  419686 MB |  409752 MB |\n",
            "|       from small pool |      58 MB |      86 MB |    9028 MB |    8970 MB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Non-releasable memory |    2115 MB |    2115 MB |   17188 GB |   17186 GB |\n",
            "|       from large pool |    2106 MB |    2106 MB |   16554 GB |   16552 GB |\n",
            "|       from small pool |       9 MB |      12 MB |     633 GB |     633 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Allocations           |     592    |     595    |    6927 K  |    6927 K  |\n",
            "|       from large pool |     160    |     164    |    2414 K  |    2414 K  |\n",
            "|       from small pool |     432    |     516    |    4512 K  |    4512 K  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Active allocs         |     592    |     595    |    6927 K  |    6927 K  |\n",
            "|       from large pool |     160    |     164    |    2414 K  |    2414 K  |\n",
            "|       from small pool |     432    |     516    |    4512 K  |    4512 K  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| GPU reserved segments |      67    |      81    |    5208    |    5141    |\n",
            "|       from large pool |      38    |      38    |     694    |     656    |\n",
            "|       from small pool |      29    |      43    |    4514    |    4485    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Non-releasable allocs |      47    |      47    |    3112 K  |    3112 K  |\n",
            "|       from large pool |      25    |      25    |    1507 K  |    1507 K  |\n",
            "|       from small pool |      22    |      26    |    1605 K  |    1605 K  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Oversize allocations  |       0    |       0    |       0    |       0    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Oversize GPU segments |       0    |       0    |       0    |       0    |\n",
            "|===========================================================================|\n",
            "\n",
            "2022-03-10 05:45:30 | WARNING | fairseq.trainer | attempting to recover from OOM in forward/backward pass\n",
            "epoch 009:  76% 541/712 [01:47<00:41,  4.08it/s, loss=2.281, ppl=4.86, wps=13261.1, ups=5.03, wpb=2635, bsz=128, num_updates=6100, lr=0.0005, gnorm=1.07, train_wall=18, gb_free=10.4, wall=1287]2022-03-10 05:45:37 | WARNING | fairseq.trainer | OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 1.76 GiB (GPU 0; 11.17 GiB total capacity; 8.04 GiB already allocated; 1.13 GiB free; 9.52 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
            "2022-03-10 05:45:37 | WARNING | fairseq.trainer | |===========================================================================|\n",
            "|                  PyTorch CUDA memory summary, device ID 0                 |\n",
            "|---------------------------------------------------------------------------|\n",
            "|            CUDA OOMs: 108          |        cudaMalloc retries: 214       |\n",
            "|===========================================================================|\n",
            "|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Allocated memory      |    8232 MB |    8296 MB |   16775 GB |   16767 GB |\n",
            "|       from large pool |    8183 MB |    8247 MB |   16193 GB |   16185 GB |\n",
            "|       from small pool |      48 MB |      61 MB |     582 GB |     582 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Active memory         |    8232 MB |    8296 MB |   16775 GB |   16767 GB |\n",
            "|       from large pool |    8183 MB |    8247 MB |   16193 GB |   16185 GB |\n",
            "|       from small pool |      48 MB |      61 MB |     582 GB |     582 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| GPU reserved memory   |    9748 MB |    9804 MB |  430002 MB |  420254 MB |\n",
            "|       from large pool |    9690 MB |    9690 MB |  420918 MB |  411228 MB |\n",
            "|       from small pool |      58 MB |     114 MB |    9084 MB |    9026 MB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Non-releasable memory |    1515 MB |    1714 MB |   17283 GB |   17282 GB |\n",
            "|       from large pool |    1506 MB |    1705 MB |   16647 GB |   16646 GB |\n",
            "|       from small pool |       9 MB |      12 MB |     636 GB |     636 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Allocations           |     593    |     595    |    6959 K  |    6959 K  |\n",
            "|       from large pool |     160    |     164    |    2426 K  |    2425 K  |\n",
            "|       from small pool |     433    |     516    |    4533 K  |    4533 K  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Active allocs         |     593    |     595    |    6959 K  |    6959 K  |\n",
            "|       from large pool |     160    |     164    |    2426 K  |    2425 K  |\n",
            "|       from small pool |     433    |     516    |    4533 K  |    4533 K  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| GPU reserved segments |      67    |      95    |    5237    |    5170    |\n",
            "|       from large pool |      38    |      38    |     695    |     657    |\n",
            "|       from small pool |      29    |      57    |    4542    |    4513    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Non-releasable allocs |      48    |      49    |    3126 K  |    3126 K  |\n",
            "|       from large pool |      27    |      27    |    1514 K  |    1514 K  |\n",
            "|       from small pool |      21    |      29    |    1612 K  |    1612 K  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Oversize allocations  |       0    |       0    |       0    |       0    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Oversize GPU segments |       0    |       0    |       0    |       0    |\n",
            "|===========================================================================|\n",
            "\n",
            "2022-03-10 05:45:37 | WARNING | fairseq.trainer | attempting to recover from OOM in forward/backward pass\n",
            "epoch 009:  78% 552/712 [01:49<00:33,  4.84it/s, loss=2.281, ppl=4.86, wps=13261.1, ups=5.03, wpb=2635, bsz=128, num_updates=6100, lr=0.0005, gnorm=1.07, train_wall=18, gb_free=10.4, wall=1287]2022-03-10 05:45:39 | WARNING | fairseq.trainer | OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 1.70 GiB (GPU 0; 11.17 GiB total capacity; 8.03 GiB already allocated; 1.13 GiB free; 9.52 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
            "epoch 009:  78% 553/712 [01:49<00:45,  3.51it/s, loss=2.281, ppl=4.86, wps=13261.1, ups=5.03, wpb=2635, bsz=128, num_updates=6100, lr=0.0005, gnorm=1.07, train_wall=18, gb_free=10.4, wall=1287]2022-03-10 05:45:39 | WARNING | fairseq.trainer | |===========================================================================|\n",
            "|                  PyTorch CUDA memory summary, device ID 0                 |\n",
            "|---------------------------------------------------------------------------|\n",
            "|            CUDA OOMs: 109          |        cudaMalloc retries: 215       |\n",
            "|===========================================================================|\n",
            "|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Allocated memory      |    8218 MB |    8281 MB |   16807 GB |   16799 GB |\n",
            "|       from large pool |    8169 MB |    8232 MB |   16223 GB |   16215 GB |\n",
            "|       from small pool |      49 MB |      61 MB |     583 GB |     583 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Active memory         |    8218 MB |    8281 MB |   16807 GB |   16799 GB |\n",
            "|       from large pool |    8169 MB |    8232 MB |   16223 GB |   16215 GB |\n",
            "|       from small pool |      49 MB |      61 MB |     583 GB |     583 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| GPU reserved memory   |    9748 MB |    9796 MB |  430050 MB |  420302 MB |\n",
            "|       from large pool |    9690 MB |    9690 MB |  420918 MB |  411228 MB |\n",
            "|       from small pool |      58 MB |     106 MB |    9132 MB |    9074 MB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Non-releasable memory |    1529 MB |    1651 MB |   17316 GB |   17315 GB |\n",
            "|       from large pool |    1521 MB |    1642 MB |   16679 GB |   16678 GB |\n",
            "|       from small pool |       8 MB |      12 MB |     637 GB |     637 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Allocations           |     593    |     595    |    6971 K  |    6970 K  |\n",
            "|       from large pool |     160    |     164    |    2430 K  |    2429 K  |\n",
            "|       from small pool |     433    |     516    |    4541 K  |    4540 K  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Active allocs         |     593    |     595    |    6971 K  |    6970 K  |\n",
            "|       from large pool |     160    |     164    |    2430 K  |    2429 K  |\n",
            "|       from small pool |     433    |     516    |    4541 K  |    4540 K  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| GPU reserved segments |      67    |      91    |    5261    |    5194    |\n",
            "|       from large pool |      38    |      38    |     695    |     657    |\n",
            "|       from small pool |      29    |      53    |    4566    |    4537    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Non-releasable allocs |      46    |      47    |    3131 K  |    3131 K  |\n",
            "|       from large pool |      24    |      24    |    1516 K  |    1516 K  |\n",
            "|       from small pool |      22    |      31    |    1615 K  |    1615 K  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Oversize allocations  |       0    |       0    |       0    |       0    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Oversize GPU segments |       0    |       0    |       0    |       0    |\n",
            "|===========================================================================|\n",
            "\n",
            "2022-03-10 05:45:39 | WARNING | fairseq.trainer | attempting to recover from OOM in forward/backward pass\n",
            "epoch 009:  81% 575/712 [01:53<00:25,  5.42it/s, loss=2.281, ppl=4.86, wps=13261.1, ups=5.03, wpb=2635, bsz=128, num_updates=6100, lr=0.0005, gnorm=1.07, train_wall=18, gb_free=10.4, wall=1287]2022-03-10 05:45:43 | WARNING | fairseq.trainer | OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 2.95 GiB (GPU 0; 11.17 GiB total capacity; 10.51 GiB already allocated; 43.81 MiB free; 10.61 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
            "2022-03-10 05:45:43 | WARNING | fairseq.trainer | |===========================================================================|\n",
            "|                  PyTorch CUDA memory summary, device ID 0                 |\n",
            "|---------------------------------------------------------------------------|\n",
            "|            CUDA OOMs: 110          |        cudaMalloc retries: 217       |\n",
            "|===========================================================================|\n",
            "|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Allocated memory      |   10716 MB |   10766 MB |   16862 GB |   16852 GB |\n",
            "|       from large pool |   10665 MB |   10715 MB |   16277 GB |   16266 GB |\n",
            "|       from small pool |      51 MB |      60 MB |     585 GB |     585 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Active memory         |   10716 MB |   10766 MB |   16862 GB |   16852 GB |\n",
            "|       from large pool |   10665 MB |   10715 MB |   16277 GB |   16266 GB |\n",
            "|       from small pool |      51 MB |      60 MB |     585 GB |     585 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| GPU reserved memory   |   10864 MB |   10880 MB |  431232 MB |  420368 MB |\n",
            "|       from large pool |   10804 MB |   10804 MB |  422032 MB |  411228 MB |\n",
            "|       from small pool |      60 MB |     126 MB |    9200 MB |    9140 MB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Non-releasable memory |  150545 KB |    1509 MB |   17369 GB |   17369 GB |\n",
            "|       from large pool |  141437 KB |    1498 MB |   16730 GB |   16729 GB |\n",
            "|       from small pool |    9108 KB |      21 MB |     639 GB |     639 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Allocations           |     593    |     595    |    6996 K  |    6995 K  |\n",
            "|       from large pool |     162    |     163    |    2439 K  |    2438 K  |\n",
            "|       from small pool |     431    |     516    |    4557 K  |    4556 K  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Active allocs         |     593    |     595    |    6996 K  |    6995 K  |\n",
            "|       from large pool |     162    |     163    |    2439 K  |    2438 K  |\n",
            "|       from small pool |     431    |     516    |    4557 K  |    4556 K  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| GPU reserved segments |      75    |     107    |    5302    |    5227    |\n",
            "|       from large pool |      45    |      45    |     702    |     657    |\n",
            "|       from small pool |      30    |      63    |    4600    |    4570    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Non-releasable allocs |      58    |      58    |    3142 K  |    3142 K  |\n",
            "|       from large pool |      34    |      34    |    1522 K  |    1522 K  |\n",
            "|       from small pool |      24    |      41    |    1620 K  |    1620 K  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Oversize allocations  |       0    |       0    |       0    |       0    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Oversize GPU segments |       0    |       0    |       0    |       0    |\n",
            "|===========================================================================|\n",
            "\n",
            "2022-03-10 05:45:43 | WARNING | fairseq.trainer | attempting to recover from OOM in forward/backward pass\n",
            "epoch 009:  92% 658/712 [02:13<00:24,  2.18it/s, loss=2.398, ppl=5.27, wps=11234.4, ups=4.11, wpb=2733.8, bsz=128, num_updates=6200, lr=0.0005, gnorm=1.076, train_wall=22, gb_free=10.4, wall=1311]2022-03-10 05:46:04 | WARNING | fairseq.trainer | OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 1.83 GiB (GPU 0; 11.17 GiB total capacity; 10.03 GiB already allocated; 179.81 MiB free; 10.48 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
            "2022-03-10 05:46:04 | WARNING | fairseq.trainer | |===========================================================================|\n",
            "|                  PyTorch CUDA memory summary, device ID 0                 |\n",
            "|---------------------------------------------------------------------------|\n",
            "|            CUDA OOMs: 111          |        cudaMalloc retries: 222       |\n",
            "|===========================================================================|\n",
            "|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Allocated memory      |    8398 MB |   10270 MB |   17123 GB |   17115 GB |\n",
            "|       from large pool |    8340 MB |   10213 MB |   16530 GB |   16521 GB |\n",
            "|       from small pool |      57 MB |      60 MB |     593 GB |     593 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Active memory         |    8398 MB |   10270 MB |   17123 GB |   17115 GB |\n",
            "|       from large pool |    8340 MB |   10213 MB |   16530 GB |   16521 GB |\n",
            "|       from small pool |      57 MB |      60 MB |     593 GB |     593 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| GPU reserved memory   |   10728 MB |   10728 MB |  455304 MB |  444576 MB |\n",
            "|       from large pool |   10666 MB |   10666 MB |  445938 MB |  435272 MB |\n",
            "|       from small pool |      62 MB |      98 MB |    9366 MB |    9304 MB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Non-releasable memory |  466942 KB |    1458 MB |   17604 GB |   17604 GB |\n",
            "|       from large pool |  462618 KB |    1453 MB |   16956 GB |   16956 GB |\n",
            "|       from small pool |    4324 KB |      24 MB |     647 GB |     647 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Allocations           |     592    |     595    |    7089 K  |    7088 K  |\n",
            "|       from large pool |     150    |     154    |    2471 K  |    2471 K  |\n",
            "|       from small pool |     442    |     516    |    4617 K  |    4617 K  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Active allocs         |     592    |     595    |    7089 K  |    7088 K  |\n",
            "|       from large pool |     150    |     154    |    2471 K  |    2471 K  |\n",
            "|       from small pool |     442    |     516    |    4617 K  |    4617 K  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| GPU reserved segments |      71    |      91    |    5428    |    5357    |\n",
            "|       from large pool |      40    |      42    |     745    |     705    |\n",
            "|       from small pool |      31    |      49    |    4683    |    4652    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Non-releasable allocs |      53    |      54    |    3184 K  |    3184 K  |\n",
            "|       from large pool |      30    |      31    |    1542 K  |    1542 K  |\n",
            "|       from small pool |      23    |      42    |    1641 K  |    1641 K  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Oversize allocations  |       0    |       0    |       0    |       0    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Oversize GPU segments |       0    |       0    |       0    |       0    |\n",
            "|===========================================================================|\n",
            "\n",
            "2022-03-10 05:46:04 | WARNING | fairseq.trainer | attempting to recover from OOM in forward/backward pass\n",
            "epoch 009: 100% 711/712 [02:24<00:00,  6.05it/s, loss=2.398, ppl=5.27, wps=11234.4, ups=4.11, wpb=2733.8, bsz=128, num_updates=6200, lr=0.0005, gnorm=1.076, train_wall=22, gb_free=10.4, wall=1311]2022-03-10 05:46:15 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "\n",
            "epoch 009 | valid on 'valid' subset:   0% 0/8 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 009 | valid on 'valid' subset:  12% 1/8 [00:00<00:01,  5.27it/s]\u001b[A\n",
            "epoch 009 | valid on 'valid' subset:  38% 3/8 [00:00<00:00,  9.50it/s]\u001b[A\n",
            "epoch 009 | valid on 'valid' subset:  62% 5/8 [00:00<00:00, 10.48it/s]\u001b[A\n",
            "epoch 009 | valid on 'valid' subset:  88% 7/8 [00:00<00:00,  9.58it/s]\u001b[A\n",
            "epoch 009 | valid on 'valid' subset: 100% 8/8 [00:01<00:00,  5.42it/s]\u001b[A\n",
            "                                                                      \u001b[A2022-03-10 05:46:16 | INFO | valid | epoch 009 | valid on 'valid' subset | loss 3.843 | ppl 14.35 | wps 20008.8 | wpb 2800.1 | bsz 125 | num_updates 6297 | best_loss 3.691\n",
            "2022-03-10 05:46:16 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 9 @ 6297 updates\n",
            "2022-03-10 05:46:16 | INFO | fairseq.trainer | Saving checkpoint to /content/checkpoints/checkpoint9.pt\n",
            "2022-03-10 05:46:17 | INFO | fairseq.trainer | Finished saving checkpoint to /content/checkpoints/checkpoint9.pt\n",
            "2022-03-10 05:46:17 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/checkpoint9.pt (epoch 9 @ 6297 updates, score 3.843) (writing took 1.2019140379998134 seconds)\n",
            "2022-03-10 05:46:17 | INFO | fairseq_cli.train | end of epoch 9 (average epoch stats below)\n",
            "2022-03-10 05:46:17 | INFO | train | epoch 009 | loss 2.318 | ppl 4.99 | wps 13064.8 | ups 4.74 | wpb 2757.7 | bsz 127.7 | num_updates 6297 | lr 0.0005 | gnorm 1.031 | train_wall 134 | gb_free 10.2 | wall 1336\n",
            "2022-03-10 05:46:17 | INFO | fairseq.data.iterators | grouped total_num_itrs = 712\n",
            "epoch 010:   0% 0/712 [00:00<?, ?it/s]2022-03-10 05:46:17 | INFO | fairseq.trainer | begin training epoch 10\n",
            "2022-03-10 05:46:17 | INFO | fairseq_cli.train | Start iterating over samples\n",
            "epoch 010:   2% 12/712 [00:03<02:23,  4.89it/s, loss=2.529, ppl=5.77, wps=11907, ups=3.93, wpb=3025.9, bsz=128, num_updates=6300, lr=0.0005, gnorm=1.039, train_wall=21, gb_free=10.3, wall=1337]2022-03-10 05:46:21 | WARNING | fairseq.trainer | OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 1.76 GiB (GPU 0; 11.17 GiB total capacity; 8.04 GiB already allocated; 925.81 MiB free; 9.75 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
            "2022-03-10 05:46:21 | WARNING | fairseq.trainer | |===========================================================================|\n",
            "|                  PyTorch CUDA memory summary, device ID 0                 |\n",
            "|---------------------------------------------------------------------------|\n",
            "|            CUDA OOMs: 112          |        cudaMalloc retries: 223       |\n",
            "|===========================================================================|\n",
            "|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Allocated memory      |    8231 MB |    8294 MB |   17331 GB |   17323 GB |\n",
            "|       from large pool |    8182 MB |    8245 MB |   16731 GB |   16723 GB |\n",
            "|       from small pool |      48 MB |      60 MB |     599 GB |     599 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Active memory         |    8231 MB |    8294 MB |   17331 GB |   17323 GB |\n",
            "|       from large pool |    8182 MB |    8245 MB |   16731 GB |   16723 GB |\n",
            "|       from small pool |      48 MB |      60 MB |     599 GB |     599 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| GPU reserved memory   |    9982 MB |   10046 MB |  456496 MB |  446514 MB |\n",
            "|       from large pool |    9924 MB |    9924 MB |  447070 MB |  437146 MB |\n",
            "|       from small pool |      58 MB |     122 MB |    9426 MB |    9368 MB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Non-releasable memory |    1750 MB |    1750 MB |   17833 GB |   17831 GB |\n",
            "|       from large pool |    1741 MB |    1741 MB |   17178 GB |   17177 GB |\n",
            "|       from small pool |       9 MB |      18 MB |     654 GB |     654 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Allocations           |     593    |     595    |    7164 K  |    7164 K  |\n",
            "|       from large pool |     160    |     164    |    2498 K  |    2498 K  |\n",
            "|       from small pool |     433    |     516    |    4666 K  |    4665 K  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Active allocs         |     593    |     595    |    7164 K  |    7164 K  |\n",
            "|       from large pool |     160    |     164    |    2498 K  |    2498 K  |\n",
            "|       from small pool |     433    |     516    |    4666 K  |    4665 K  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| GPU reserved segments |      69    |     101    |    5459    |    5390    |\n",
            "|       from large pool |      40    |      40    |     746    |     706    |\n",
            "|       from small pool |      29    |      61    |    4713    |    4684    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Non-releasable allocs |      47    |      53    |    3219 K  |    3219 K  |\n",
            "|       from large pool |      25    |      25    |    1560 K  |    1560 K  |\n",
            "|       from small pool |      22    |      49    |    1659 K  |    1659 K  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Oversize allocations  |       0    |       0    |       0    |       0    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Oversize GPU segments |       0    |       0    |       0    |       0    |\n",
            "|===========================================================================|\n",
            "\n",
            "2022-03-10 05:46:21 | WARNING | fairseq.trainer | attempting to recover from OOM in forward/backward pass\n",
            "epoch 010:   6% 46/712 [00:09<01:39,  6.68it/s, loss=2.529, ppl=5.77, wps=11907, ups=3.93, wpb=3025.9, bsz=128, num_updates=6300, lr=0.0005, gnorm=1.039, train_wall=21, gb_free=10.3, wall=1337]2022-03-10 05:46:27 | WARNING | fairseq.trainer | OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 1.70 GiB (GPU 0; 11.17 GiB total capacity; 8.03 GiB already allocated; 923.81 MiB free; 9.75 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
            "2022-03-10 05:46:27 | WARNING | fairseq.trainer | |===========================================================================|\n",
            "|                  PyTorch CUDA memory summary, device ID 0                 |\n",
            "|---------------------------------------------------------------------------|\n",
            "|            CUDA OOMs: 113          |        cudaMalloc retries: 224       |\n",
            "|===========================================================================|\n",
            "|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Allocated memory      |    8218 MB |    8282 MB |   17408 GB |   17400 GB |\n",
            "|       from large pool |    8169 MB |    8233 MB |   16805 GB |   16797 GB |\n",
            "|       from small pool |      49 MB |      60 MB |     602 GB |     602 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Active memory         |    8218 MB |    8282 MB |   17408 GB |   17400 GB |\n",
            "|       from large pool |    8169 MB |    8233 MB |   16805 GB |   16797 GB |\n",
            "|       from small pool |      49 MB |      60 MB |     602 GB |     602 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| GPU reserved memory   |    9984 MB |   10058 MB |  456572 MB |  446588 MB |\n",
            "|       from large pool |    9924 MB |    9924 MB |  447070 MB |  437146 MB |\n",
            "|       from small pool |      60 MB |     134 MB |    9502 MB |    9442 MB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Non-releasable memory |    1765 MB |    1765 MB |   17916 GB |   17914 GB |\n",
            "|       from large pool |    1754 MB |    1754 MB |   17258 GB |   17256 GB |\n",
            "|       from small pool |      10 MB |      20 MB |     657 GB |     657 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Allocations           |     593    |     595    |    7202 K  |    7201 K  |\n",
            "|       from large pool |     160    |     164    |    2511 K  |    2511 K  |\n",
            "|       from small pool |     433    |     516    |    4690 K  |    4690 K  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Active allocs         |     593    |     595    |    7202 K  |    7201 K  |\n",
            "|       from large pool |     160    |     164    |    2511 K  |    2511 K  |\n",
            "|       from small pool |     433    |     516    |    4690 K  |    4690 K  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| GPU reserved segments |      70    |     107    |    5497    |    5427    |\n",
            "|       from large pool |      40    |      40    |     746    |     706    |\n",
            "|       from small pool |      30    |      67    |    4751    |    4721    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Non-releasable allocs |      55    |      55    |    3237 K  |    3237 K  |\n",
            "|       from large pool |      30    |      30    |    1569 K  |    1568 K  |\n",
            "|       from small pool |      25    |      42    |    1668 K  |    1668 K  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Oversize allocations  |       0    |       0    |       0    |       0    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Oversize GPU segments |       0    |       0    |       0    |       0    |\n",
            "|===========================================================================|\n",
            "\n",
            "2022-03-10 05:46:27 | WARNING | fairseq.trainer | attempting to recover from OOM in forward/backward pass\n",
            "epoch 010:   7% 53/712 [00:11<02:49,  3.89it/s, loss=2.529, ppl=5.77, wps=11907, ups=3.93, wpb=3025.9, bsz=128, num_updates=6300, lr=0.0005, gnorm=1.039, train_wall=21, gb_free=10.3, wall=1337]2022-03-10 05:46:29 | WARNING | fairseq.trainer | OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 1.52 GiB (GPU 0; 11.17 GiB total capacity; 8.52 GiB already allocated; 749.81 MiB free; 9.92 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
            "2022-03-10 05:46:29 | WARNING | fairseq.trainer | |===========================================================================|\n",
            "|                  PyTorch CUDA memory summary, device ID 0                 |\n",
            "|---------------------------------------------------------------------------|\n",
            "|            CUDA OOMs: 114          |        cudaMalloc retries: 227       |\n",
            "|===========================================================================|\n",
            "|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Allocated memory      |    7169 MB |    8721 MB |   17440 GB |   17433 GB |\n",
            "|       from large pool |    7120 MB |    8672 MB |   16836 GB |   16830 GB |\n",
            "|       from small pool |      48 MB |      60 MB |     603 GB |     603 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Active memory         |    7169 MB |    8721 MB |   17440 GB |   17433 GB |\n",
            "|       from large pool |    7120 MB |    8672 MB |   16836 GB |   16830 GB |\n",
            "|       from small pool |      48 MB |      60 MB |     603 GB |     603 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| GPU reserved memory   |   10158 MB |   10158 MB |  459426 MB |  449268 MB |\n",
            "|       from large pool |   10100 MB |   10100 MB |  449862 MB |  439762 MB |\n",
            "|       from small pool |      58 MB |     110 MB |    9564 MB |    9506 MB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Non-releasable memory |    1436 MB |    1436 MB |   17946 GB |   17945 GB |\n",
            "|       from large pool |    1427 MB |    1427 MB |   17288 GB |   17286 GB |\n",
            "|       from small pool |       9 MB |      22 MB |     658 GB |     658 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Allocations           |     592    |     595    |    7209 K  |    7209 K  |\n",
            "|       from large pool |     160    |     164    |    2514 K  |    2514 K  |\n",
            "|       from small pool |     432    |     516    |    4694 K  |    4694 K  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Active allocs         |     592    |     595    |    7209 K  |    7209 K  |\n",
            "|       from large pool |     160    |     164    |    2514 K  |    2514 K  |\n",
            "|       from small pool |     432    |     516    |    4694 K  |    4694 K  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| GPU reserved segments |      69    |      95    |    5530    |    5461    |\n",
            "|       from large pool |      40    |      40    |     748    |     708    |\n",
            "|       from small pool |      29    |      55    |    4782    |    4753    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Non-releasable allocs |      51    |      51    |    3240 K  |    3240 K  |\n",
            "|       from large pool |      25    |      25    |    1570 K  |    1570 K  |\n",
            "|       from small pool |      26    |      44    |    1669 K  |    1669 K  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Oversize allocations  |       0    |       0    |       0    |       0    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Oversize GPU segments |       0    |       0    |       0    |       0    |\n",
            "|===========================================================================|\n",
            "\n",
            "2022-03-10 05:46:29 | WARNING | fairseq.trainer | attempting to recover from OOM in forward/backward pass\n",
            "epoch 010:  12% 84/712 [00:17<01:38,  6.41it/s, loss=2.529, ppl=5.77, wps=11907, ups=3.93, wpb=3025.9, bsz=128, num_updates=6300, lr=0.0005, gnorm=1.039, train_wall=21, gb_free=10.3, wall=1337]2022-03-10 05:46:35 | WARNING | fairseq.trainer | OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 1.54 GiB (GPU 0; 11.17 GiB total capacity; 8.23 GiB already allocated; 721.81 MiB free; 9.95 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
            "2022-03-10 05:46:35 | WARNING | fairseq.trainer | |===========================================================================|\n",
            "|                  PyTorch CUDA memory summary, device ID 0                 |\n",
            "|---------------------------------------------------------------------------|\n",
            "|            CUDA OOMs: 115          |        cudaMalloc retries: 228       |\n",
            "|===========================================================================|\n",
            "|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Allocated memory      |    6852 MB |    8427 MB |   17511 GB |   17505 GB |\n",
            "|       from large pool |    6794 MB |    8369 MB |   16905 GB |   16898 GB |\n",
            "|       from small pool |      58 MB |      60 MB |     606 GB |     606 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Active memory         |    6852 MB |    8427 MB |   17511 GB |   17505 GB |\n",
            "|       from large pool |    6794 MB |    8369 MB |   16905 GB |   16898 GB |\n",
            "|       from small pool |      58 MB |      60 MB |     606 GB |     606 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| GPU reserved memory   |   10186 MB |   10256 MB |  461076 MB |  450890 MB |\n",
            "|       from large pool |   10124 MB |   10124 MB |  451438 MB |  441314 MB |\n",
            "|       from small pool |      62 MB |     132 MB |    9638 MB |    9576 MB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Non-releasable memory |    1757 MB |    1758 MB |   18017 GB |   18015 GB |\n",
            "|       from large pool |    1753 MB |    1754 MB |   17355 GB |   17354 GB |\n",
            "|       from small pool |       3 MB |      18 MB |     661 GB |     661 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Allocations           |     592    |     595    |    7243 K  |    7243 K  |\n",
            "|       from large pool |     150    |     154    |    2526 K  |    2526 K  |\n",
            "|       from small pool |     442    |     516    |    4717 K  |    4716 K  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Active allocs         |     592    |     595    |    7243 K  |    7243 K  |\n",
            "|       from large pool |     150    |     154    |    2526 K  |    2526 K  |\n",
            "|       from small pool |     442    |     516    |    4717 K  |    4716 K  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| GPU reserved segments |      71    |     106    |    5568    |    5497    |\n",
            "|       from large pool |      40    |      40    |     749    |     709    |\n",
            "|       from small pool |      31    |      66    |    4819    |    4788    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Non-releasable allocs |      56    |      57    |    3256 K  |    3256 K  |\n",
            "|       from large pool |      29    |      30    |    1578 K  |    1578 K  |\n",
            "|       from small pool |      27    |      52    |    1677 K  |    1677 K  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Oversize allocations  |       0    |       0    |       0    |       0    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Oversize GPU segments |       0    |       0    |       0    |       0    |\n",
            "|===========================================================================|\n",
            "\n",
            "2022-03-10 05:46:35 | WARNING | fairseq.trainer | attempting to recover from OOM in forward/backward pass\n",
            "epoch 010:  25% 178/712 [00:35<01:35,  5.59it/s, loss=1.967, ppl=3.91, wps=13301.2, ups=4.78, wpb=2784.4, bsz=128, num_updates=6400, lr=0.0005, gnorm=0.946, train_wall=18, gb_free=8.5, wall=1358]2022-03-10 05:46:53 | WARNING | fairseq.trainer | OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 1.76 GiB (GPU 0; 11.17 GiB total capacity; 9.04 GiB already allocated; 1.14 GiB free; 9.51 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
            "2022-03-10 05:46:53 | WARNING | fairseq.trainer | |===========================================================================|\n",
            "|                  PyTorch CUDA memory summary, device ID 0                 |\n",
            "|---------------------------------------------------------------------------|\n",
            "|            CUDA OOMs: 116          |        cudaMalloc retries: 229       |\n",
            "|===========================================================================|\n",
            "|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Allocated memory      |    9260 MB |    9329 MB |   17749 GB |   17740 GB |\n",
            "|       from large pool |    9210 MB |    9280 MB |   17135 GB |   17126 GB |\n",
            "|       from small pool |      49 MB |      60 MB |     614 GB |     613 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Active memory         |    9260 MB |    9329 MB |   17749 GB |   17740 GB |\n",
            "|       from large pool |    9210 MB |    9280 MB |   17135 GB |   17126 GB |\n",
            "|       from small pool |      49 MB |      60 MB |     614 GB |     613 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| GPU reserved memory   |    9742 MB |    9808 MB |  462274 MB |  452532 MB |\n",
            "|       from large pool |    9680 MB |    9680 MB |  452570 MB |  442890 MB |\n",
            "|       from small pool |      62 MB |     128 MB |    9704 MB |    9642 MB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Non-releasable memory |  493524 KB |    1222 MB |   18266 GB |   18266 GB |\n",
            "|       from large pool |  480616 KB |    1209 MB |   17596 GB |   17596 GB |\n",
            "|       from small pool |   12908 KB |      20 MB |     670 GB |     670 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Allocations           |     593    |     595    |    7348 K  |    7348 K  |\n",
            "|       from large pool |     160    |     164    |    2564 K  |    2563 K  |\n",
            "|       from small pool |     433    |     516    |    4784 K  |    4784 K  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Active allocs         |     593    |     595    |    7348 K  |    7348 K  |\n",
            "|       from large pool |     160    |     164    |    2564 K  |    2563 K  |\n",
            "|       from small pool |     433    |     516    |    4784 K  |    4784 K  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| GPU reserved segments |      71    |     104    |    5602    |    5531    |\n",
            "|       from large pool |      40    |      40    |     750    |     710    |\n",
            "|       from small pool |      31    |      64    |    4852    |    4821    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Non-releasable allocs |      57    |      57    |    3304 K  |    3304 K  |\n",
            "|       from large pool |      31    |      31    |    1602 K  |    1602 K  |\n",
            "|       from small pool |      26    |      44    |    1702 K  |    1702 K  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Oversize allocations  |       0    |       0    |       0    |       0    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Oversize GPU segments |       0    |       0    |       0    |       0    |\n",
            "|===========================================================================|\n",
            "\n",
            "2022-03-10 05:46:53 | WARNING | fairseq.trainer | attempting to recover from OOM in forward/backward pass\n",
            "epoch 010:  34% 240/712 [00:47<01:36,  4.88it/s, loss=2.075, ppl=4.21, wps=14113.2, ups=5.23, wpb=2698.7, bsz=126.9, num_updates=6500, lr=0.0005, gnorm=1.018, train_wall=18, gb_free=10.3, wall=1377]2022-03-10 05:47:05 | WARNING | fairseq.trainer | OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 2.95 GiB (GPU 0; 11.17 GiB total capacity; 10.51 GiB already allocated; 13.81 MiB free; 10.64 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
            "2022-03-10 05:47:05 | WARNING | fairseq.trainer | |===========================================================================|\n",
            "|                  PyTorch CUDA memory summary, device ID 0                 |\n",
            "|---------------------------------------------------------------------------|\n",
            "|            CUDA OOMs: 117          |        cudaMalloc retries: 232       |\n",
            "|===========================================================================|\n",
            "|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Allocated memory      |   10717 MB |   10767 MB |   17921 GB |   17911 GB |\n",
            "|       from large pool |   10666 MB |   10715 MB |   17301 GB |   17291 GB |\n",
            "|       from small pool |      51 MB |      60 MB |     620 GB |     619 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Active memory         |   10717 MB |   10767 MB |   17921 GB |   17911 GB |\n",
            "|       from large pool |   10666 MB |   10715 MB |   17301 GB |   17291 GB |\n",
            "|       from small pool |      51 MB |      60 MB |     620 GB |     619 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| GPU reserved memory   |   10894 MB |   10902 MB |  464708 MB |  453814 MB |\n",
            "|       from large pool |   10836 MB |   10836 MB |  454858 MB |  444022 MB |\n",
            "|       from small pool |      58 MB |     132 MB |    9850 MB |    9792 MB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Non-releasable memory |  180556 KB |    2220 MB |   18449 GB |   18449 GB |\n",
            "|       from large pool |  173496 KB |    2211 MB |   17772 GB |   17772 GB |\n",
            "|       from small pool |    7060 KB |      33 MB |     676 GB |     676 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Allocations           |     593    |     595    |    7417 K  |    7417 K  |\n",
            "|       from large pool |     162    |     163    |    2588 K  |    2587 K  |\n",
            "|       from small pool |     431    |     516    |    4829 K  |    4829 K  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Active allocs         |     593    |     595    |    7417 K  |    7417 K  |\n",
            "|       from large pool |     162    |     163    |    2588 K  |    2587 K  |\n",
            "|       from small pool |     431    |     516    |    4829 K  |    4829 K  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| GPU reserved segments |      76    |     111    |    5683    |    5607    |\n",
            "|       from large pool |      47    |      47    |     758    |     711    |\n",
            "|       from small pool |      29    |      66    |    4925    |    4896    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Non-releasable allocs |      57    |      57    |    3335 K  |    3335 K  |\n",
            "|       from large pool |      33    |      33    |    1618 K  |    1618 K  |\n",
            "|       from small pool |      24    |      52    |    1717 K  |    1717 K  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Oversize allocations  |       0    |       0    |       0    |       0    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Oversize GPU segments |       0    |       0    |       0    |       0    |\n",
            "|===========================================================================|\n",
            "\n",
            "2022-03-10 05:47:05 | WARNING | fairseq.trainer | attempting to recover from OOM in forward/backward pass\n",
            "epoch 010:  34% 242/712 [00:48<02:07,  3.69it/s, loss=2.075, ppl=4.21, wps=14113.2, ups=5.23, wpb=2698.7, bsz=126.9, num_updates=6500, lr=0.0005, gnorm=1.018, train_wall=18, gb_free=10.3, wall=1377]2022-03-10 05:47:06 | WARNING | fairseq.trainer | OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 2.96 GiB (GPU 0; 11.17 GiB total capacity; 10.43 GiB already allocated; 13.81 MiB free; 10.64 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
            "2022-03-10 05:47:06 | WARNING | fairseq.trainer | |===========================================================================|\n",
            "|                  PyTorch CUDA memory summary, device ID 0                 |\n",
            "|---------------------------------------------------------------------------|\n",
            "|            CUDA OOMs: 118          |        cudaMalloc retries: 233       |\n",
            "|===========================================================================|\n",
            "|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Allocated memory      |   10626 MB |   10676 MB |   17938 GB |   17927 GB |\n",
            "|       from large pool |   10575 MB |   10625 MB |   17318 GB |   17307 GB |\n",
            "|       from small pool |      51 MB |      60 MB |     620 GB |     620 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Active memory         |   10626 MB |   10676 MB |   17938 GB |   17927 GB |\n",
            "|       from large pool |   10575 MB |   10625 MB |   17318 GB |   17307 GB |\n",
            "|       from small pool |      51 MB |      60 MB |     620 GB |     620 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| GPU reserved memory   |   10894 MB |   10902 MB |  464716 MB |  453822 MB |\n",
            "|       from large pool |   10836 MB |   10836 MB |  454858 MB |  444022 MB |\n",
            "|       from small pool |      58 MB |      66 MB |    9858 MB |    9800 MB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Non-releasable memory |  273792 KB |    2529 MB |   18467 GB |   18467 GB |\n",
            "|       from large pool |  266701 KB |    2520 MB |   17790 GB |   17790 GB |\n",
            "|       from small pool |    7091 KB |      15 MB |     676 GB |     676 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Allocations           |     593    |     595    |    7419 K  |    7418 K  |\n",
            "|       from large pool |     162    |     163    |    2588 K  |    2588 K  |\n",
            "|       from small pool |     431    |     516    |    4830 K  |    4830 K  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Active allocs         |     593    |     595    |    7419 K  |    7418 K  |\n",
            "|       from large pool |     162    |     163    |    2588 K  |    2588 K  |\n",
            "|       from small pool |     431    |     516    |    4830 K  |    4830 K  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| GPU reserved segments |      76    |      80    |    5687    |    5611    |\n",
            "|       from large pool |      47    |      47    |     758    |     711    |\n",
            "|       from small pool |      29    |      33    |    4929    |    4900    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Non-releasable allocs |      46    |      49    |    3336 K  |    3336 K  |\n",
            "|       from large pool |      24    |      27    |    1618 K  |    1618 K  |\n",
            "|       from small pool |      22    |      29    |    1717 K  |    1717 K  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Oversize allocations  |       0    |       0    |       0    |       0    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Oversize GPU segments |       0    |       0    |       0    |       0    |\n",
            "|===========================================================================|\n",
            "\n",
            "2022-03-10 05:47:06 | WARNING | fairseq.trainer | attempting to recover from OOM in forward/backward pass\n",
            "epoch 010:  69% 492/712 [01:36<00:32,  6.80it/s, loss=2.199, ppl=4.59, wps=13779.4, ups=5.03, wpb=2740.4, bsz=126.7, num_updates=6700, lr=0.0005, gnorm=1.113, train_wall=19, gb_free=10.6, wall=1418]2022-03-10 05:47:54 | WARNING | fairseq.trainer | OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 1.87 GiB (GPU 0; 11.17 GiB total capacity; 8.57 GiB already allocated; 1.28 GiB free; 9.38 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
            "epoch 010:  69% 493/712 [01:37<01:11,  3.06it/s, loss=2.199, ppl=4.59, wps=13779.4, ups=5.03, wpb=2740.4, bsz=126.7, num_updates=6700, lr=0.0005, gnorm=1.113, train_wall=19, gb_free=10.6, wall=1418]2022-03-10 05:47:54 | WARNING | fairseq.trainer | |===========================================================================|\n",
            "|                  PyTorch CUDA memory summary, device ID 0                 |\n",
            "|---------------------------------------------------------------------------|\n",
            "|            CUDA OOMs: 119          |        cudaMalloc retries: 238       |\n",
            "|===========================================================================|\n",
            "|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Allocated memory      |    8777 MB |    8844 MB |   18538 GB |   18529 GB |\n",
            "|       from large pool |    8727 MB |    8795 MB |   17895 GB |   17886 GB |\n",
            "|       from small pool |      49 MB |      60 MB |     643 GB |     643 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Active memory         |    8777 MB |    8844 MB |   18538 GB |   18529 GB |\n",
            "|       from large pool |    8727 MB |    8795 MB |   17895 GB |   17886 GB |\n",
            "|       from small pool |      49 MB |      60 MB |     643 GB |     643 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| GPU reserved memory   |    9602 MB |   10286 MB |  486304 MB |  476702 MB |\n",
            "|       from large pool |    9544 MB |   10148 MB |  476222 MB |  466678 MB |\n",
            "|       from small pool |      58 MB |     138 MB |   10082 MB |   10024 MB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Non-releasable memory |     824 MB |    1358 MB |   19072 GB |   19071 GB |\n",
            "|       from large pool |     816 MB |    1349 MB |   18370 GB |   18369 GB |\n",
            "|       from small pool |       8 MB |      13 MB |     702 GB |     702 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Allocations           |     593    |     595    |    7700 K  |    7699 K  |\n",
            "|       from large pool |     160    |     164    |    2686 K  |    2686 K  |\n",
            "|       from small pool |     433    |     516    |    5013 K  |    5012 K  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Active allocs         |     593    |     595    |    7700 K  |    7699 K  |\n",
            "|       from large pool |     160    |     164    |    2686 K  |    2686 K  |\n",
            "|       from small pool |     433    |     516    |    5013 K  |    5012 K  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| GPU reserved segments |      61    |     102    |    5828    |    5767    |\n",
            "|       from large pool |      32    |      33    |     787    |     755    |\n",
            "|       from small pool |      29    |      69    |    5041    |    5012    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Non-releasable allocs |      41    |      50    |    3466 K  |    3466 K  |\n",
            "|       from large pool |      19    |      19    |    1682 K  |    1682 K  |\n",
            "|       from small pool |      22    |      43    |    1784 K  |    1784 K  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Oversize allocations  |       0    |       0    |       0    |       0    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Oversize GPU segments |       0    |       0    |       0    |       0    |\n",
            "|===========================================================================|\n",
            "\n",
            "2022-03-10 05:47:54 | WARNING | fairseq.trainer | attempting to recover from OOM in forward/backward pass\n",
            "epoch 010:  81% 575/712 [01:57<00:53,  2.56it/s, loss=2.203, ppl=4.6, wps=13989.7, ups=5.07, wpb=2758.5, bsz=128, num_updates=6800, lr=0.0005, gnorm=1.056, train_wall=18, gb_free=4.1, wall=1437]2022-03-10 05:48:15 | WARNING | fairseq.trainer | OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 1.83 GiB (GPU 0; 11.17 GiB total capacity; 8.20 GiB already allocated; 1.56 GiB free; 9.09 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
            "2022-03-10 05:48:15 | WARNING | fairseq.trainer | |===========================================================================|\n",
            "|                  PyTorch CUDA memory summary, device ID 0                 |\n",
            "|---------------------------------------------------------------------------|\n",
            "|            CUDA OOMs: 120          |        cudaMalloc retries: 242       |\n",
            "|===========================================================================|\n",
            "|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Allocated memory      |    8399 MB |    8464 MB |   18818 GB |   18810 GB |\n",
            "|       from large pool |    8341 MB |    8406 MB |   18167 GB |   18159 GB |\n",
            "|       from small pool |      57 MB |      61 MB |     651 GB |     651 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Active memory         |    8399 MB |    8464 MB |   18818 GB |   18810 GB |\n",
            "|       from large pool |    8341 MB |    8406 MB |   18167 GB |   18159 GB |\n",
            "|       from small pool |      57 MB |      61 MB |     651 GB |     651 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| GPU reserved memory   |    9308 MB |   10822 MB |  494378 MB |  485070 MB |\n",
            "|       from large pool |    9246 MB |   10730 MB |  484102 MB |  474856 MB |\n",
            "|       from small pool |      62 MB |      92 MB |   10276 MB |   10214 MB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Non-releasable memory |     908 MB |    1558 MB |   19359 GB |   19358 GB |\n",
            "|       from large pool |     904 MB |    1554 MB |   18648 GB |   18647 GB |\n",
            "|       from small pool |       4 MB |      12 MB |     711 GB |     711 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Allocations           |     593    |     595    |    7793 K  |    7792 K  |\n",
            "|       from large pool |     150    |     154    |    2718 K  |    2718 K  |\n",
            "|       from small pool |     443    |     516    |    5074 K  |    5074 K  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Active allocs         |     593    |     595    |    7793 K  |    7792 K  |\n",
            "|       from large pool |     150    |     154    |    2718 K  |    2718 K  |\n",
            "|       from small pool |     443    |     516    |    5074 K  |    5074 K  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| GPU reserved segments |      62    |      78    |    5931    |    5869    |\n",
            "|       from large pool |      31    |      32    |     793    |     762    |\n",
            "|       from small pool |      31    |      46    |    5138    |    5107    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Non-releasable allocs |      46    |      46    |    3509 K  |    3509 K  |\n",
            "|       from large pool |      21    |      21    |    1702 K  |    1702 K  |\n",
            "|       from small pool |      25    |      28    |    1806 K  |    1806 K  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Oversize allocations  |       0    |       0    |       0    |       0    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Oversize GPU segments |       0    |       0    |       0    |       0    |\n",
            "|===========================================================================|\n",
            "\n",
            "2022-03-10 05:48:15 | WARNING | fairseq.trainer | attempting to recover from OOM in forward/backward pass\n",
            "epoch 010:  97% 690/712 [02:21<00:04,  4.97it/s, loss=2.423, ppl=5.36, wps=12628.1, ups=4.24, wpb=2976.6, bsz=128, num_updates=6900, lr=0.0005, gnorm=1.057, train_wall=22, gb_free=10.3, wall=1461]2022-03-10 05:48:40 | WARNING | fairseq.trainer | OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 2.51 GiB (GPU 0; 11.17 GiB total capacity; 10.40 GiB already allocated; 173.81 MiB free; 10.48 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
            "2022-03-10 05:48:40 | WARNING | fairseq.trainer | |===========================================================================|\n",
            "|                  PyTorch CUDA memory summary, device ID 0                 |\n",
            "|---------------------------------------------------------------------------|\n",
            "|            CUDA OOMs: 121          |        cudaMalloc retries: 245       |\n",
            "|===========================================================================|\n",
            "|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Allocated memory      |   10605 MB |   10647 MB |   19131 GB |   19121 GB |\n",
            "|       from large pool |   10554 MB |   10596 MB |   18468 GB |   18458 GB |\n",
            "|       from small pool |      51 MB |      60 MB |     663 GB |     663 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Active memory         |   10605 MB |   10647 MB |   19131 GB |   19121 GB |\n",
            "|       from large pool |   10554 MB |   10596 MB |   18468 GB |   18458 GB |\n",
            "|       from small pool |      51 MB |      60 MB |     663 GB |     663 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| GPU reserved memory   |   10734 MB |   10788 MB |  509498 MB |  498764 MB |\n",
            "|       from large pool |   10676 MB |   10676 MB |  499092 MB |  488416 MB |\n",
            "|       from small pool |      58 MB |     112 MB |   10406 MB |   10348 MB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Non-releasable memory |  131804 KB |    1811 MB |   19668 GB |   19667 GB |\n",
            "|       from large pool |  124692 KB |    1801 MB |   18944 GB |   18944 GB |\n",
            "|       from small pool |    7112 KB |      15 MB |     723 GB |     723 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Allocations           |     593    |     595    |    7921 K  |    7921 K  |\n",
            "|       from large pool |     162    |     163    |    2761 K  |    2761 K  |\n",
            "|       from small pool |     431    |     516    |    5160 K  |    5159 K  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Active allocs         |     593    |     595    |    7921 K  |    7921 K  |\n",
            "|       from large pool |     162    |     163    |    2761 K  |    2761 K  |\n",
            "|       from small pool |     431    |     516    |    5160 K  |    5159 K  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| GPU reserved segments |      86    |     113    |    6048    |    5962    |\n",
            "|       from large pool |      57    |      57    |     845    |     788    |\n",
            "|       from small pool |      29    |      56    |    5203    |    5174    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Non-releasable allocs |      54    |      54    |    3567 K  |    3567 K  |\n",
            "|       from large pool |      30    |      30    |    1730 K  |    1730 K  |\n",
            "|       from small pool |      24    |      42    |    1837 K  |    1837 K  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Oversize allocations  |       0    |       0    |       0    |       0    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Oversize GPU segments |       0    |       0    |       0    |       0    |\n",
            "|===========================================================================|\n",
            "\n",
            "2022-03-10 05:48:40 | WARNING | fairseq.trainer | attempting to recover from OOM in forward/backward pass\n",
            "epoch 010: 100% 711/712 [02:27<00:00,  7.13it/s, loss=2.423, ppl=5.36, wps=12628.1, ups=4.24, wpb=2976.6, bsz=128, num_updates=6900, lr=0.0005, gnorm=1.057, train_wall=22, gb_free=10.3, wall=1461]2022-03-10 05:48:45 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "\n",
            "epoch 010 | valid on 'valid' subset:   0% 0/8 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 010 | valid on 'valid' subset:  12% 1/8 [00:00<00:01,  6.11it/s]\u001b[A\n",
            "epoch 010 | valid on 'valid' subset:  38% 3/8 [00:00<00:00,  9.89it/s]\u001b[A\n",
            "epoch 010 | valid on 'valid' subset:  62% 5/8 [00:00<00:00, 10.34it/s]\u001b[A\n",
            "epoch 010 | valid on 'valid' subset:  88% 7/8 [00:00<00:00,  9.59it/s]\u001b[A\n",
            "epoch 010 | valid on 'valid' subset: 100% 8/8 [00:02<00:00,  2.65it/s]\u001b[A\n",
            "                                                                      \u001b[A2022-03-10 05:48:47 | INFO | valid | epoch 010 | valid on 'valid' subset | loss 3.935 | ppl 15.29 | wps 11148.4 | wpb 2800.1 | bsz 125 | num_updates 6999 | best_loss 3.691\n",
            "2022-03-10 05:48:47 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 10 @ 6999 updates\n",
            "2022-03-10 05:48:47 | INFO | fairseq.trainer | Saving checkpoint to /content/checkpoints/checkpoint10.pt\n",
            "2022-03-10 05:48:48 | INFO | fairseq.trainer | Finished saving checkpoint to /content/checkpoints/checkpoint10.pt\n",
            "2022-03-10 05:48:48 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/checkpoint10.pt (epoch 10 @ 6999 updates, score 3.935) (writing took 1.1032255779996376 seconds)\n",
            "2022-03-10 05:48:48 | INFO | fairseq_cli.train | end of epoch 10 (average epoch stats below)\n",
            "2022-03-10 05:48:48 | INFO | train | epoch 010 | loss 2.2 | ppl 4.59 | wps 12896.3 | ups 4.65 | wpb 2775.2 | bsz 127.7 | num_updates 6999 | lr 0.0005 | gnorm 1.046 | train_wall 139 | gb_free 3.2 | wall 1487\n",
            "2022-03-10 05:48:48 | INFO | fairseq_cli.train | done training in 1486.7 seconds\n",
            "\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Waiting for W&B process to finish... \u001b[32m(success).\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                                                                                \n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run history:\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:              train/bsz ▁▁▁▁▁▁▁▁▁▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:          train/gb_free █▇███████▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:            train/gnorm █▁▂▃▄▄▅▆▇▇\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:             train/loss █▅▄▃▃▂▂▂▁▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:               train/lr ▁▁▁▁▁▁▁▁▁▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:              train/ppl █▃▂▂▁▁▁▁▁▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:       train/train_wall ▁▅▄▂▅▅▅▅▄█\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:              train/ups █▂▅▅▃▂▄▅▅▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:             train/wall ▁▂▃▃▄▅▆▆▇█\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:              train/wpb ▃▇▆▁▃▅▆█▄█\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:              train/wps █▃▅▃▁▂▄▇▄▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:        train_inner/bsz █▂███▂██████▂█▂▁███▂█▁████▂████▂█▂▁██▂▁█\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:    train_inner/gb_free █▇█▇▃▇█▄▆▇█▃▆▆███▁▇█▇█▁███▇███▃▇▇▇██████\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:      train_inner/gnorm █▂▂▁▁▂▁▁▁▁▁▁▁▁▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▃▂▂▃▃▂▂▃▂\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:       train_inner/loss █▆▆▅▅▄▄▄▄▃▃▃▃▃▃▃▃▂▂▂▂▂▂▂▂▂▂▂▂▂▁▂▂▁▁▁▂▁▁▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:         train_inner/lr ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:        train_inner/ppl █▄▃▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: train_inner/train_wall ▆▁▆█▆▄▇▅▄▅▅▆▅▇▆▄▄▅▄▇▅▃▆█▄▅▅▅▄▆▅▆▅▅▅█▇▄▅█\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:        train_inner/ups ▄█▅▃▁▅▃▅▄▅▅▄▃▃▄▅▃▄▆▃▃▅▄▂▄▅▅▅▄▄▅▂▃▅▄▂▂▆▅▃\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:       train_inner/wall ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:        train_inner/wpb ▅▁▄█▇▄▄▆▅▄▄▅▄█▅▃▃▄▃▆▄▃▄▆▄▄▅▅▄▇▅▄▅▄▆▄▇▄▄▇\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:        train_inner/wps ▅█▆▅▁▆▄█▅▆▅▆▃▆▅▅▂▄▆▅▃▆▄▃▄▇▆█▅██▁▄▆▇▂▃▇▇▄\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:        valid/best_loss █▄▂▁▁▁▁▁▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:              valid/bsz ▁▁▁▁▁▁▁▁▁▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:             valid/loss █▄▂▁▁▁▁▂▂▂\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:              valid/ppl █▃▂▁▁▁▁▁▂▂\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:              valid/wpb ▁▁▁▁▁▁▁▁▁▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:              valid/wps █████████▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run summary:\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:              train/bsz 127.7\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:          train/gb_free 3.2\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:            train/gnorm 1.046\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:             train/loss 2.2\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:               train/lr 0.0005\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:              train/ppl 4.59\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:       train/train_wall 139.0\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:              train/ups 4.65\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:             train/wall 1487.0\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:              train/wpb 2775.2\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:              train/wps 12896.3\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:        train_inner/bsz 128.0\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:    train_inner/gb_free 10.3\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:      train_inner/gnorm 1.057\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:       train_inner/loss 2.423\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:         train_inner/lr 0.0005\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:        train_inner/ppl 5.36\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: train_inner/train_wall 22.0\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:        train_inner/ups 4.24\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:       train_inner/wall 1461.0\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:        train_inner/wpb 2976.6\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:        train_inner/wps 12628.1\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:        valid/best_loss 3.691\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:              valid/bsz 125.0\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:             valid/loss 3.935\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:              valid/ppl 15.29\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:              valid/wpb 2800.1\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:              valid/wps 11148.4\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Synced \u001b[33mcheckpoints\u001b[0m: \u001b[34m\u001b[4mhttps://wandb.ai/rahul2023_usa/Transformer%20from%20scratch%20-%2010%20March%20-%20kn%20to%20ml/runs/3t1nyi40\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Find logs at: \u001b[35m\u001b[1m./wandb/run-20220310_052402-3t1nyi40/logs\u001b[0m\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!fairseq-generate /content/nmt-kn-ml/tokenized.kn-ml \\\n",
        "  --path /content/checkpoints/checkpoint_best.pt \\\n",
        "  --beam 5 \\\n",
        "  --remove-bpe \\\n",
        " | grep ^H | LC_ALL=C sort -V | cut -f3- > test_ml_generated_scratch_transformer.txt "
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OB50GvgVOthe",
        "outputId": "df4a080c-dc96-454d-8681-12e99eef4c95"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2022-03-10 05:53:16 | INFO | fairseq.tasks.text_to_speech | Please install tensorboardX: pip install tensorboardX\n",
            "2022-03-10 05:53:19 | INFO | fairseq_cli.generate | {'_name': None, 'common': {'_name': None, 'no_progress_bar': False, 'log_interval': 100, 'log_format': None, 'log_file': None, 'tensorboard_logdir': None, 'wandb_project': None, 'azureml_logging': False, 'seed': 1, 'cpu': False, 'tpu': False, 'bf16': False, 'memory_efficient_bf16': False, 'fp16': False, 'memory_efficient_fp16': False, 'fp16_no_flatten_grads': False, 'fp16_init_scale': 128, 'fp16_scale_window': None, 'fp16_scale_tolerance': 0.0, 'on_cpu_convert_precision': False, 'min_loss_scale': 0.0001, 'threshold_loss_scale': None, 'amp': False, 'amp_batch_retries': 2, 'amp_init_scale': 128, 'amp_scale_window': None, 'user_dir': None, 'empty_cache_freq': 0, 'all_gather_list_size': 16384, 'model_parallel_size': 1, 'quantization_config_path': None, 'profile': False, 'reset_logging': False, 'suppress_crashes': False, 'use_plasma_view': False, 'plasma_path': '/tmp/plasma'}, 'common_eval': {'_name': None, 'path': '/content/checkpoints/checkpoint_best.pt', 'post_process': 'subword_nmt', 'quiet': False, 'model_overrides': '{}', 'results_path': None}, 'distributed_training': {'_name': None, 'distributed_world_size': 1, 'distributed_num_procs': 1, 'distributed_rank': 0, 'distributed_backend': 'nccl', 'distributed_init_method': None, 'distributed_port': -1, 'device_id': 0, 'distributed_no_spawn': False, 'ddp_backend': 'pytorch_ddp', 'ddp_comm_hook': 'none', 'bucket_cap_mb': 25, 'fix_batches_to_gpus': False, 'find_unused_parameters': False, 'gradient_as_bucket_view': False, 'fast_stat_sync': False, 'heartbeat_timeout': -1, 'broadcast_buffers': False, 'slowmo_momentum': None, 'slowmo_base_algorithm': 'localsgd', 'localsgd_frequency': 3, 'nprocs_per_node': 1, 'pipeline_model_parallel': False, 'pipeline_balance': None, 'pipeline_devices': None, 'pipeline_chunks': 0, 'pipeline_encoder_balance': None, 'pipeline_encoder_devices': None, 'pipeline_decoder_balance': None, 'pipeline_decoder_devices': None, 'pipeline_checkpoint': 'never', 'zero_sharding': 'none', 'fp16': False, 'memory_efficient_fp16': False, 'tpu': False, 'no_reshard_after_forward': False, 'fp32_reduce_scatter': False, 'cpu_offload': False, 'use_sharded_state': False, 'not_fsdp_flatten_parameters': False}, 'dataset': {'_name': None, 'num_workers': 1, 'skip_invalid_size_inputs_valid_test': False, 'max_tokens': 12000, 'batch_size': None, 'required_batch_size_multiple': 8, 'required_seq_len_multiple': 1, 'dataset_impl': None, 'data_buffer_size': 10, 'train_subset': 'train', 'valid_subset': 'valid', 'combine_valid_subsets': None, 'ignore_unused_valid_subsets': False, 'validate_interval': 1, 'validate_interval_updates': 0, 'validate_after_updates': 0, 'fixed_validation_seed': None, 'disable_validation': False, 'max_tokens_valid': None, 'batch_size_valid': None, 'max_valid_steps': None, 'curriculum': 0, 'gen_subset': 'test', 'num_shards': 1, 'shard_id': 0, 'grouped_shuffling': False, 'update_epoch_batch_itr': False, 'update_ordered_indices_seed': False}, 'optimization': {'_name': None, 'max_epoch': 0, 'max_update': 0, 'stop_time_hours': 0.0, 'clip_norm': 0.0, 'sentence_avg': False, 'update_freq': [1], 'lr': [0.25], 'stop_min_lr': -1.0, 'use_bmuf': False, 'skip_remainder_batch': False}, 'checkpoint': {'_name': None, 'save_dir': 'checkpoints', 'restore_file': 'checkpoint_last.pt', 'continue_once': None, 'finetune_from_model': None, 'reset_dataloader': False, 'reset_lr_scheduler': False, 'reset_meters': False, 'reset_optimizer': False, 'optimizer_overrides': '{}', 'save_interval': 1, 'save_interval_updates': 0, 'keep_interval_updates': -1, 'keep_interval_updates_pattern': -1, 'keep_last_epochs': -1, 'keep_best_checkpoints': -1, 'no_save': False, 'no_epoch_checkpoints': False, 'no_last_checkpoints': False, 'no_save_optimizer_state': False, 'best_checkpoint_metric': 'loss', 'maximize_best_checkpoint_metric': False, 'patience': -1, 'checkpoint_suffix': '', 'checkpoint_shard_count': 1, 'load_checkpoint_on_all_dp_ranks': False, 'write_checkpoints_asynchronously': False, 'model_parallel_size': 1}, 'bmuf': {'_name': None, 'block_lr': 1.0, 'block_momentum': 0.875, 'global_sync_iter': 50, 'warmup_iterations': 500, 'use_nbm': False, 'average_sync': False, 'distributed_world_size': 1}, 'generation': {'_name': None, 'beam': 5, 'nbest': 1, 'max_len_a': 0.0, 'max_len_b': 200, 'min_len': 1, 'match_source_len': False, 'unnormalized': False, 'no_early_stop': False, 'no_beamable_mm': False, 'lenpen': 1.0, 'unkpen': 0.0, 'replace_unk': None, 'sacrebleu': False, 'score_reference': False, 'prefix_size': 0, 'no_repeat_ngram_size': 0, 'sampling': False, 'sampling_topk': -1, 'sampling_topp': -1.0, 'constraints': None, 'temperature': 1.0, 'diverse_beam_groups': -1, 'diverse_beam_strength': 0.5, 'diversity_rate': -1.0, 'print_alignment': None, 'print_step': False, 'lm_path': None, 'lm_weight': 0.0, 'iter_decode_eos_penalty': 0.0, 'iter_decode_max_iter': 10, 'iter_decode_force_max_iter': False, 'iter_decode_with_beam': 1, 'iter_decode_with_external_reranker': False, 'retain_iter_history': False, 'retain_dropout': False, 'retain_dropout_modules': None, 'decoding_format': None, 'no_seed_provided': False}, 'eval_lm': {'_name': None, 'output_word_probs': False, 'output_word_stats': False, 'context_window': 0, 'softmax_batch': 9223372036854775807}, 'interactive': {'_name': None, 'buffer_size': 0, 'input': '-'}, 'model': {'_name': 'wav2vec2', 'extractor_mode': 'default', 'encoder_layers': 12, 'encoder_embed_dim': 768, 'encoder_ffn_embed_dim': 3072, 'encoder_attention_heads': 12, 'activation_fn': 'gelu', 'layer_type': 'transformer', 'dropout': 0.1, 'attention_dropout': 0.1, 'activation_dropout': 0.0, 'encoder_layerdrop': 0.0, 'dropout_input': 0.0, 'dropout_features': 0.0, 'final_dim': 0, 'layer_norm_first': False, 'conv_feature_layers': '[(512, 10, 5)] + [(512, 3, 2)] * 4 + [(512,2,2)] + [(512,2,2)]', 'conv_bias': False, 'logit_temp': 0.1, 'quantize_targets': False, 'quantize_input': False, 'same_quantizer': False, 'target_glu': False, 'feature_grad_mult': 1.0, 'quantizer_depth': 1, 'quantizer_factor': 3, 'latent_vars': 320, 'latent_groups': 2, 'latent_dim': 0, 'mask_length': 10, 'mask_prob': 0.65, 'mask_selection': 'static', 'mask_other': 0.0, 'no_mask_overlap': False, 'mask_min_space': 1, 'require_same_masks': True, 'mask_dropout': 0.0, 'mask_channel_length': 10, 'mask_channel_prob': 0.0, 'mask_channel_before': False, 'mask_channel_selection': 'static', 'mask_channel_other': 0.0, 'no_mask_channel_overlap': False, 'mask_channel_min_space': 1, 'num_negatives': 100, 'negatives_from_everywhere': False, 'cross_sample_negatives': 0, 'codebook_negatives': 0, 'conv_pos': 128, 'conv_pos_groups': 16, 'pos_conv_depth': 1, 'latent_temp': [2.0, 0.5, 0.999995], 'max_positions': 100000, 'checkpoint_activations': False, 'required_seq_len_multiple': 1, 'crop_seq_to_multiple': 1, 'depthwise_conv_kernel_size': 31, 'attn_type': '', 'pos_enc_type': 'abs', 'fp16': False}, 'task': {'_name': 'translation', 'data': '/content/nmt-kn-ml/tokenized.kn-ml', 'source_lang': None, 'target_lang': None, 'load_alignments': False, 'left_pad_source': True, 'left_pad_target': False, 'max_source_positions': 1024, 'max_target_positions': 1024, 'upsample_primary': -1, 'truncate_source': False, 'num_batch_buckets': 0, 'train_subset': 'train', 'dataset_impl': None, 'required_seq_len_multiple': 1, 'eval_bleu': False, 'eval_bleu_args': '{}', 'eval_bleu_detok': 'space', 'eval_bleu_detok_args': '{}', 'eval_tokenized_bleu': False, 'eval_bleu_remove_bpe': None, 'eval_bleu_print_samples': False}, 'criterion': {'_name': 'cross_entropy', 'sentence_avg': True}, 'optimizer': None, 'lr_scheduler': {'_name': 'fixed', 'force_anneal': None, 'lr_shrink': 0.1, 'warmup_updates': 0, 'lr': [0.25]}, 'scoring': {'_name': 'bleu', 'pad': 1, 'eos': 2, 'unk': 3}, 'bpe': None, 'tokenizer': None, 'ema': {'_name': None, 'store_ema': False, 'ema_decay': 0.9999, 'ema_start_update': 0, 'ema_seed_model': None, 'ema_update_freq': 1, 'ema_fp32': False}, 'simul_type': None}\n",
            "2022-03-10 05:53:19 | INFO | fairseq.tasks.translation | [kn] dictionary: 16904 types\n",
            "2022-03-10 05:53:19 | INFO | fairseq.tasks.translation | [ml] dictionary: 15656 types\n",
            "2022-03-10 05:53:19 | INFO | fairseq_cli.generate | loading model(s) from /content/checkpoints/checkpoint_best.pt\n",
            "2022-03-10 05:53:19 | INFO | fairseq.data.data_utils | loaded 1,000 examples from: /content/nmt-kn-ml/tokenized.kn-ml/test.kn-ml.kn\n",
            "2022-03-10 05:53:19 | INFO | fairseq.data.data_utils | loaded 1,000 examples from: /content/nmt-kn-ml/tokenized.kn-ml/test.kn-ml.ml\n",
            "2022-03-10 05:53:19 | INFO | fairseq.tasks.translation | /content/nmt-kn-ml/tokenized.kn-ml test kn-ml 1000 examples\n",
            "2022-03-10 05:53:40 | INFO | fairseq_cli.generate | NOTE: hypothesis and token scores are output in base 2\n",
            "2022-03-10 05:53:40 | INFO | fairseq_cli.generate | Translated 1,000 sentences (21,414 tokens) in 12.2s (81.88 sentences/s, 1753.33 tokens/s)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!fairseq-generate /content/nmt-kn-ml/tokenized.kn-ml \\\n",
        "  --path /content/checkpoints/checkpoint_best.pt \\\n",
        "  --beam 5 \\\n",
        "   --remove-bpe \\\n",
        "  | grep ^T | LC_ALL=C sort -V | cut -f2- > test_ml_actual_scratch_transformer.txt "
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BEABVzMSOxNt",
        "outputId": "07fd9531-0163-4a0d-c98a-c282a9ed62ba"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2022-03-10 05:53:49 | INFO | fairseq.tasks.text_to_speech | Please install tensorboardX: pip install tensorboardX\n",
            "2022-03-10 05:53:52 | INFO | fairseq_cli.generate | {'_name': None, 'common': {'_name': None, 'no_progress_bar': False, 'log_interval': 100, 'log_format': None, 'log_file': None, 'tensorboard_logdir': None, 'wandb_project': None, 'azureml_logging': False, 'seed': 1, 'cpu': False, 'tpu': False, 'bf16': False, 'memory_efficient_bf16': False, 'fp16': False, 'memory_efficient_fp16': False, 'fp16_no_flatten_grads': False, 'fp16_init_scale': 128, 'fp16_scale_window': None, 'fp16_scale_tolerance': 0.0, 'on_cpu_convert_precision': False, 'min_loss_scale': 0.0001, 'threshold_loss_scale': None, 'amp': False, 'amp_batch_retries': 2, 'amp_init_scale': 128, 'amp_scale_window': None, 'user_dir': None, 'empty_cache_freq': 0, 'all_gather_list_size': 16384, 'model_parallel_size': 1, 'quantization_config_path': None, 'profile': False, 'reset_logging': False, 'suppress_crashes': False, 'use_plasma_view': False, 'plasma_path': '/tmp/plasma'}, 'common_eval': {'_name': None, 'path': '/content/checkpoints/checkpoint_best.pt', 'post_process': 'subword_nmt', 'quiet': False, 'model_overrides': '{}', 'results_path': None}, 'distributed_training': {'_name': None, 'distributed_world_size': 1, 'distributed_num_procs': 1, 'distributed_rank': 0, 'distributed_backend': 'nccl', 'distributed_init_method': None, 'distributed_port': -1, 'device_id': 0, 'distributed_no_spawn': False, 'ddp_backend': 'pytorch_ddp', 'ddp_comm_hook': 'none', 'bucket_cap_mb': 25, 'fix_batches_to_gpus': False, 'find_unused_parameters': False, 'gradient_as_bucket_view': False, 'fast_stat_sync': False, 'heartbeat_timeout': -1, 'broadcast_buffers': False, 'slowmo_momentum': None, 'slowmo_base_algorithm': 'localsgd', 'localsgd_frequency': 3, 'nprocs_per_node': 1, 'pipeline_model_parallel': False, 'pipeline_balance': None, 'pipeline_devices': None, 'pipeline_chunks': 0, 'pipeline_encoder_balance': None, 'pipeline_encoder_devices': None, 'pipeline_decoder_balance': None, 'pipeline_decoder_devices': None, 'pipeline_checkpoint': 'never', 'zero_sharding': 'none', 'fp16': False, 'memory_efficient_fp16': False, 'tpu': False, 'no_reshard_after_forward': False, 'fp32_reduce_scatter': False, 'cpu_offload': False, 'use_sharded_state': False, 'not_fsdp_flatten_parameters': False}, 'dataset': {'_name': None, 'num_workers': 1, 'skip_invalid_size_inputs_valid_test': False, 'max_tokens': 12000, 'batch_size': None, 'required_batch_size_multiple': 8, 'required_seq_len_multiple': 1, 'dataset_impl': None, 'data_buffer_size': 10, 'train_subset': 'train', 'valid_subset': 'valid', 'combine_valid_subsets': None, 'ignore_unused_valid_subsets': False, 'validate_interval': 1, 'validate_interval_updates': 0, 'validate_after_updates': 0, 'fixed_validation_seed': None, 'disable_validation': False, 'max_tokens_valid': None, 'batch_size_valid': None, 'max_valid_steps': None, 'curriculum': 0, 'gen_subset': 'test', 'num_shards': 1, 'shard_id': 0, 'grouped_shuffling': False, 'update_epoch_batch_itr': False, 'update_ordered_indices_seed': False}, 'optimization': {'_name': None, 'max_epoch': 0, 'max_update': 0, 'stop_time_hours': 0.0, 'clip_norm': 0.0, 'sentence_avg': False, 'update_freq': [1], 'lr': [0.25], 'stop_min_lr': -1.0, 'use_bmuf': False, 'skip_remainder_batch': False}, 'checkpoint': {'_name': None, 'save_dir': 'checkpoints', 'restore_file': 'checkpoint_last.pt', 'continue_once': None, 'finetune_from_model': None, 'reset_dataloader': False, 'reset_lr_scheduler': False, 'reset_meters': False, 'reset_optimizer': False, 'optimizer_overrides': '{}', 'save_interval': 1, 'save_interval_updates': 0, 'keep_interval_updates': -1, 'keep_interval_updates_pattern': -1, 'keep_last_epochs': -1, 'keep_best_checkpoints': -1, 'no_save': False, 'no_epoch_checkpoints': False, 'no_last_checkpoints': False, 'no_save_optimizer_state': False, 'best_checkpoint_metric': 'loss', 'maximize_best_checkpoint_metric': False, 'patience': -1, 'checkpoint_suffix': '', 'checkpoint_shard_count': 1, 'load_checkpoint_on_all_dp_ranks': False, 'write_checkpoints_asynchronously': False, 'model_parallel_size': 1}, 'bmuf': {'_name': None, 'block_lr': 1.0, 'block_momentum': 0.875, 'global_sync_iter': 50, 'warmup_iterations': 500, 'use_nbm': False, 'average_sync': False, 'distributed_world_size': 1}, 'generation': {'_name': None, 'beam': 5, 'nbest': 1, 'max_len_a': 0.0, 'max_len_b': 200, 'min_len': 1, 'match_source_len': False, 'unnormalized': False, 'no_early_stop': False, 'no_beamable_mm': False, 'lenpen': 1.0, 'unkpen': 0.0, 'replace_unk': None, 'sacrebleu': False, 'score_reference': False, 'prefix_size': 0, 'no_repeat_ngram_size': 0, 'sampling': False, 'sampling_topk': -1, 'sampling_topp': -1.0, 'constraints': None, 'temperature': 1.0, 'diverse_beam_groups': -1, 'diverse_beam_strength': 0.5, 'diversity_rate': -1.0, 'print_alignment': None, 'print_step': False, 'lm_path': None, 'lm_weight': 0.0, 'iter_decode_eos_penalty': 0.0, 'iter_decode_max_iter': 10, 'iter_decode_force_max_iter': False, 'iter_decode_with_beam': 1, 'iter_decode_with_external_reranker': False, 'retain_iter_history': False, 'retain_dropout': False, 'retain_dropout_modules': None, 'decoding_format': None, 'no_seed_provided': False}, 'eval_lm': {'_name': None, 'output_word_probs': False, 'output_word_stats': False, 'context_window': 0, 'softmax_batch': 9223372036854775807}, 'interactive': {'_name': None, 'buffer_size': 0, 'input': '-'}, 'model': {'_name': 'wav2vec2', 'extractor_mode': 'default', 'encoder_layers': 12, 'encoder_embed_dim': 768, 'encoder_ffn_embed_dim': 3072, 'encoder_attention_heads': 12, 'activation_fn': 'gelu', 'layer_type': 'transformer', 'dropout': 0.1, 'attention_dropout': 0.1, 'activation_dropout': 0.0, 'encoder_layerdrop': 0.0, 'dropout_input': 0.0, 'dropout_features': 0.0, 'final_dim': 0, 'layer_norm_first': False, 'conv_feature_layers': '[(512, 10, 5)] + [(512, 3, 2)] * 4 + [(512,2,2)] + [(512,2,2)]', 'conv_bias': False, 'logit_temp': 0.1, 'quantize_targets': False, 'quantize_input': False, 'same_quantizer': False, 'target_glu': False, 'feature_grad_mult': 1.0, 'quantizer_depth': 1, 'quantizer_factor': 3, 'latent_vars': 320, 'latent_groups': 2, 'latent_dim': 0, 'mask_length': 10, 'mask_prob': 0.65, 'mask_selection': 'static', 'mask_other': 0.0, 'no_mask_overlap': False, 'mask_min_space': 1, 'require_same_masks': True, 'mask_dropout': 0.0, 'mask_channel_length': 10, 'mask_channel_prob': 0.0, 'mask_channel_before': False, 'mask_channel_selection': 'static', 'mask_channel_other': 0.0, 'no_mask_channel_overlap': False, 'mask_channel_min_space': 1, 'num_negatives': 100, 'negatives_from_everywhere': False, 'cross_sample_negatives': 0, 'codebook_negatives': 0, 'conv_pos': 128, 'conv_pos_groups': 16, 'pos_conv_depth': 1, 'latent_temp': [2.0, 0.5, 0.999995], 'max_positions': 100000, 'checkpoint_activations': False, 'required_seq_len_multiple': 1, 'crop_seq_to_multiple': 1, 'depthwise_conv_kernel_size': 31, 'attn_type': '', 'pos_enc_type': 'abs', 'fp16': False}, 'task': {'_name': 'translation', 'data': '/content/nmt-kn-ml/tokenized.kn-ml', 'source_lang': None, 'target_lang': None, 'load_alignments': False, 'left_pad_source': True, 'left_pad_target': False, 'max_source_positions': 1024, 'max_target_positions': 1024, 'upsample_primary': -1, 'truncate_source': False, 'num_batch_buckets': 0, 'train_subset': 'train', 'dataset_impl': None, 'required_seq_len_multiple': 1, 'eval_bleu': False, 'eval_bleu_args': '{}', 'eval_bleu_detok': 'space', 'eval_bleu_detok_args': '{}', 'eval_tokenized_bleu': False, 'eval_bleu_remove_bpe': None, 'eval_bleu_print_samples': False}, 'criterion': {'_name': 'cross_entropy', 'sentence_avg': True}, 'optimizer': None, 'lr_scheduler': {'_name': 'fixed', 'force_anneal': None, 'lr_shrink': 0.1, 'warmup_updates': 0, 'lr': [0.25]}, 'scoring': {'_name': 'bleu', 'pad': 1, 'eos': 2, 'unk': 3}, 'bpe': None, 'tokenizer': None, 'ema': {'_name': None, 'store_ema': False, 'ema_decay': 0.9999, 'ema_start_update': 0, 'ema_seed_model': None, 'ema_update_freq': 1, 'ema_fp32': False}, 'simul_type': None}\n",
            "2022-03-10 05:53:52 | INFO | fairseq.tasks.translation | [kn] dictionary: 16904 types\n",
            "2022-03-10 05:53:52 | INFO | fairseq.tasks.translation | [ml] dictionary: 15656 types\n",
            "2022-03-10 05:53:52 | INFO | fairseq_cli.generate | loading model(s) from /content/checkpoints/checkpoint_best.pt\n",
            "2022-03-10 05:53:52 | INFO | fairseq.data.data_utils | loaded 1,000 examples from: /content/nmt-kn-ml/tokenized.kn-ml/test.kn-ml.kn\n",
            "2022-03-10 05:53:52 | INFO | fairseq.data.data_utils | loaded 1,000 examples from: /content/nmt-kn-ml/tokenized.kn-ml/test.kn-ml.ml\n",
            "2022-03-10 05:53:52 | INFO | fairseq.tasks.translation | /content/nmt-kn-ml/tokenized.kn-ml test kn-ml 1000 examples\n",
            "2022-03-10 05:54:13 | INFO | fairseq_cli.generate | NOTE: hypothesis and token scores are output in base 2\n",
            "2022-03-10 05:54:13 | INFO | fairseq_cli.generate | Translated 1,000 sentences (21,414 tokens) in 12.2s (82.03 sentences/s, 1756.50 tokens/s)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!fairseq-score --sys /content/test_ml_generated_scratch_transformer.txt --ref /content/test_ml_actual_scratch_transformer.txt"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2pVTUeknOz2C",
        "outputId": "90950d41-823a-44a9-9025-06b8fd23bfbb"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2022-03-10 05:54:23 | INFO | fairseq.tasks.text_to_speech | Please install tensorboardX: pip install tensorboardX\n",
            "Namespace(ignore_case=False, order=4, ref='/content/test_ml_actual_scratch_transformer.txt', sacrebleu=False, sentence_bleu=False, sys='/content/test_ml_generated_scratch_transformer.txt')\n",
            "BLEU4 = 14.36, 42.7/16.7/9.8/6.1 (BP=1.000, ratio=1.016, syslen=20324, reflen=20001)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from nltk.translate.bleu_score import sentence_bleu\n",
        "f = open(\"/content/test_ml_generated_scratch_transformer.txt\")\n",
        "ml_candidates = [i.strip() for i in f.readlines()]\n",
        "f = open(\"/content/test_ml_actual_scratch_transformer.txt\")\n",
        "ml_gold = [i.strip() for i in f.readlines()]\n",
        "total_four = 0\n",
        "for i in range(len(ml_candidates)):\n",
        "    candidate = ml_candidates[i].split(\" \")\n",
        "    references = ml_gold[i].split(\" \")\n",
        "    reference = [references]\n",
        "    score_cumulative4 = sentence_bleu(reference, candidate, weights=(0.25, 0.25, 0.25, 0.25))\n",
        "    total_four = total_four + score_cumulative4\n",
        "print(total_four/len(ml_candidates))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aPHr8U7yO44N",
        "outputId": "d46e194d-e9c2-4111-d7ff-4b35a23ad865"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/nltk/translate/bleu_score.py:490: UserWarning: \n",
            "Corpus/Sentence contains 0 counts of 4-gram overlaps.\n",
            "BLEU scores might be undesirable; use SmoothingFunction().\n",
            "  warnings.warn(_msg)\n",
            "/usr/local/lib/python3.7/dist-packages/nltk/translate/bleu_score.py:490: UserWarning: \n",
            "Corpus/Sentence contains 0 counts of 2-gram overlaps.\n",
            "BLEU scores might be undesirable; use SmoothingFunction().\n",
            "  warnings.warn(_msg)\n",
            "/usr/local/lib/python3.7/dist-packages/nltk/translate/bleu_score.py:490: UserWarning: \n",
            "Corpus/Sentence contains 0 counts of 3-gram overlaps.\n",
            "BLEU scores might be undesirable; use SmoothingFunction().\n",
            "  warnings.warn(_msg)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.3430951762193223\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vfTkMlunZo3F",
        "outputId": "6c4a15a1-a0f6-49d3-d561-e65866432969"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!cp /content/checkpoints/checkpoint_best.pt /content/drive/MyDrive/Transformer_from_scratch_ml/"
      ],
      "metadata": {
        "id": "N7Npj5mnZ7n3"
      },
      "execution_count": 21,
      "outputs": []
    }
  ]
}