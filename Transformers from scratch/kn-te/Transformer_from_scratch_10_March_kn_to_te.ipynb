{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "XCHlPGNWTP1x"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        " # w and b (wandb) for logging\n",
        "! pip install wandb\n",
        "\n",
        "# sacremos - for tokenizing\n",
        "! pip install sacremos\n",
        "\n",
        "# fairseq - for training and evaluation of the model\n",
        "! git clone https://github.com/pytorch/fairseq\n",
        "%cd fairseq\n",
        "! pip install --editable ./\n",
        "%cd ..\n",
        "\n",
        "%cd /content/fairseq/\n",
        "!python -m pip install --editable .\n",
        "%cd /content\n",
        "\n",
        "! echo $PYTHONPATH\n",
        "\n",
        "import os\n",
        "os.environ['PYTHONPATH'] += \":/content/fairseq/\"\n",
        "\n",
        "! echo $PYTHONPATH"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 104
        },
        "id": "oYalg3fMUML6",
        "outputId": "7887bfe6-04ca-4b5b-d6ce-4ae0ec5f0024"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/javascript": [
              "\n",
              "        window._wandbApiKey = new Promise((resolve, reject) => {\n",
              "            function loadScript(url) {\n",
              "            return new Promise(function(resolve, reject) {\n",
              "                let newScript = document.createElement(\"script\");\n",
              "                newScript.onerror = reject;\n",
              "                newScript.onload = resolve;\n",
              "                document.body.appendChild(newScript);\n",
              "                newScript.src = url;\n",
              "            });\n",
              "            }\n",
              "            loadScript(\"https://cdn.jsdelivr.net/npm/postmate/build/postmate.min.js\").then(() => {\n",
              "            const iframe = document.createElement('iframe')\n",
              "            iframe.style.cssText = \"width:0;height:0;border:none\"\n",
              "            document.body.appendChild(iframe)\n",
              "            const handshake = new Postmate({\n",
              "                container: iframe,\n",
              "                url: 'https://wandb.ai/authorize'\n",
              "            });\n",
              "            const timeout = setTimeout(() => reject(\"Couldn't auto authenticate\"), 5000)\n",
              "            handshake.then(function(child) {\n",
              "                child.on('authorize', data => {\n",
              "                    clearTimeout(timeout)\n",
              "                    resolve(data)\n",
              "                });\n",
              "            });\n",
              "            })\n",
              "        });\n",
              "    "
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: You can find your API key in your browser here: https://wandb.ai/authorize\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "wandb: Paste an API key from your profile and hit enter, or press ctrl+c to quit: ··········\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ],
      "source": [
        "import wandb\n",
        "wandb.login()\n",
        "# login authorization."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!unzip /content/tokenized_kn_te.zip"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "x_u2b5FBcE1L",
        "outputId": "43ff7267-0160-41f1-f6eb-dfdb3050dcf7"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Archive:  /content/tokenized_kn_te.zip\n",
            "   creating: content/nmt-kn-te/tokenized.kn-te/\n",
            "  inflating: content/nmt-kn-te/tokenized.kn-te/valid.kn-te.te.bin  \n",
            "  inflating: content/nmt-kn-te/tokenized.kn-te/test.kn-te.kn.idx  \n",
            "  inflating: content/nmt-kn-te/tokenized.kn-te/dict.te.txt  \n",
            "  inflating: content/nmt-kn-te/tokenized.kn-te/valid.kn-te.kn.idx  \n",
            "  inflating: content/nmt-kn-te/tokenized.kn-te/preprocess.log  \n",
            "  inflating: content/nmt-kn-te/tokenized.kn-te/train.kn-te.kn.idx  \n",
            "  inflating: content/nmt-kn-te/tokenized.kn-te/valid.kn-te.te.idx  \n",
            "  inflating: content/nmt-kn-te/tokenized.kn-te/test.kn-te.te.bin  \n",
            "  inflating: content/nmt-kn-te/tokenized.kn-te/test.kn-te.kn.bin  \n",
            "  inflating: content/nmt-kn-te/tokenized.kn-te/test.kn-te.te.idx  \n",
            "  inflating: content/nmt-kn-te/tokenized.kn-te/train.kn-te.te.idx  \n",
            "  inflating: content/nmt-kn-te/tokenized.kn-te/train.kn-te.kn.bin  \n",
            "  inflating: content/nmt-kn-te/tokenized.kn-te/valid.kn-te.kn.bin  \n",
            "  inflating: content/nmt-kn-te/tokenized.kn-te/train.kn-te.te.bin  \n",
            "  inflating: content/nmt-kn-te/tokenized.kn-te/dict.kn.txt  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "! fairseq-train /content/content/nmt-kn-te/tokenized.kn-te \\\n",
        "--arch transformer \\\n",
        "--dropout 0.1 \\\n",
        "--attention-dropout 0.1 \\\n",
        "--activation-dropout 0.1 \\\n",
        "--encoder-embed-dim 256 \\\n",
        "--encoder-ffn-embed-dim 512 \\\n",
        "--encoder-layers 3 \\\n",
        "--encoder-attention-heads 8 \\\n",
        "--encoder-learned-pos \\\n",
        "--decoder-ffn-embed-dim 512 \\\n",
        "--decoder-layers 3 \\\n",
        "--decoder-attention-heads 8 \\\n",
        "--decoder-learned-pos \\\n",
        "--max-epoch 10 \\\n",
        "--optimizer adam \\\n",
        "--lr 5e-4 \\\n",
        "--batch-size 128 \\\n",
        "--seed 1 \\\n",
        "--wandb-project \"Transformer from scratch - 10 March - kn to te\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FN0ghZTscLfG",
        "outputId": "bdd2b00c-6ab4-43d5-ccfa-3d873a15d9d8"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2022-03-10 06:13:03 | INFO | numexpr.utils | NumExpr defaulting to 2 threads.\n",
            "2022-03-10 06:13:04 | INFO | fairseq.tasks.text_to_speech | Please install tensorboardX: pip install tensorboardX\n",
            "2022-03-10 06:13:07 | INFO | fairseq_cli.train | {'_name': None, 'common': {'_name': None, 'no_progress_bar': False, 'log_interval': 100, 'log_format': None, 'log_file': None, 'tensorboard_logdir': None, 'wandb_project': 'Transformer from scratch - 10 March - kn to te', 'azureml_logging': False, 'seed': 1, 'cpu': False, 'tpu': False, 'bf16': False, 'memory_efficient_bf16': False, 'fp16': False, 'memory_efficient_fp16': False, 'fp16_no_flatten_grads': False, 'fp16_init_scale': 128, 'fp16_scale_window': None, 'fp16_scale_tolerance': 0.0, 'on_cpu_convert_precision': False, 'min_loss_scale': 0.0001, 'threshold_loss_scale': None, 'amp': False, 'amp_batch_retries': 2, 'amp_init_scale': 128, 'amp_scale_window': None, 'user_dir': None, 'empty_cache_freq': 0, 'all_gather_list_size': 16384, 'model_parallel_size': 1, 'quantization_config_path': None, 'profile': False, 'reset_logging': False, 'suppress_crashes': False, 'use_plasma_view': False, 'plasma_path': '/tmp/plasma'}, 'common_eval': {'_name': None, 'path': None, 'post_process': None, 'quiet': False, 'model_overrides': '{}', 'results_path': None}, 'distributed_training': {'_name': None, 'distributed_world_size': 1, 'distributed_num_procs': 1, 'distributed_rank': 0, 'distributed_backend': 'nccl', 'distributed_init_method': None, 'distributed_port': -1, 'device_id': 0, 'distributed_no_spawn': False, 'ddp_backend': 'pytorch_ddp', 'ddp_comm_hook': 'none', 'bucket_cap_mb': 25, 'fix_batches_to_gpus': False, 'find_unused_parameters': False, 'gradient_as_bucket_view': False, 'fast_stat_sync': False, 'heartbeat_timeout': -1, 'broadcast_buffers': False, 'slowmo_momentum': None, 'slowmo_base_algorithm': 'localsgd', 'localsgd_frequency': 3, 'nprocs_per_node': 1, 'pipeline_model_parallel': False, 'pipeline_balance': None, 'pipeline_devices': None, 'pipeline_chunks': 0, 'pipeline_encoder_balance': None, 'pipeline_encoder_devices': None, 'pipeline_decoder_balance': None, 'pipeline_decoder_devices': None, 'pipeline_checkpoint': 'never', 'zero_sharding': 'none', 'fp16': False, 'memory_efficient_fp16': False, 'tpu': False, 'no_reshard_after_forward': False, 'fp32_reduce_scatter': False, 'cpu_offload': False, 'use_sharded_state': False, 'not_fsdp_flatten_parameters': False}, 'dataset': {'_name': None, 'num_workers': 1, 'skip_invalid_size_inputs_valid_test': False, 'max_tokens': None, 'batch_size': 128, 'required_batch_size_multiple': 8, 'required_seq_len_multiple': 1, 'dataset_impl': None, 'data_buffer_size': 10, 'train_subset': 'train', 'valid_subset': 'valid', 'combine_valid_subsets': None, 'ignore_unused_valid_subsets': False, 'validate_interval': 1, 'validate_interval_updates': 0, 'validate_after_updates': 0, 'fixed_validation_seed': None, 'disable_validation': False, 'max_tokens_valid': None, 'batch_size_valid': 128, 'max_valid_steps': None, 'curriculum': 0, 'gen_subset': 'test', 'num_shards': 1, 'shard_id': 0, 'grouped_shuffling': False, 'update_epoch_batch_itr': False, 'update_ordered_indices_seed': False}, 'optimization': {'_name': None, 'max_epoch': 10, 'max_update': 0, 'stop_time_hours': 0.0, 'clip_norm': 0.0, 'sentence_avg': False, 'update_freq': [1], 'lr': [0.0005], 'stop_min_lr': -1.0, 'use_bmuf': False, 'skip_remainder_batch': False}, 'checkpoint': {'_name': None, 'save_dir': 'checkpoints', 'restore_file': 'checkpoint_last.pt', 'continue_once': None, 'finetune_from_model': None, 'reset_dataloader': False, 'reset_lr_scheduler': False, 'reset_meters': False, 'reset_optimizer': False, 'optimizer_overrides': '{}', 'save_interval': 1, 'save_interval_updates': 0, 'keep_interval_updates': -1, 'keep_interval_updates_pattern': -1, 'keep_last_epochs': -1, 'keep_best_checkpoints': -1, 'no_save': False, 'no_epoch_checkpoints': False, 'no_last_checkpoints': False, 'no_save_optimizer_state': False, 'best_checkpoint_metric': 'loss', 'maximize_best_checkpoint_metric': False, 'patience': -1, 'checkpoint_suffix': '', 'checkpoint_shard_count': 1, 'load_checkpoint_on_all_dp_ranks': False, 'write_checkpoints_asynchronously': False, 'model_parallel_size': 1}, 'bmuf': {'_name': None, 'block_lr': 1.0, 'block_momentum': 0.875, 'global_sync_iter': 50, 'warmup_iterations': 500, 'use_nbm': False, 'average_sync': False, 'distributed_world_size': 1}, 'generation': {'_name': None, 'beam': 5, 'nbest': 1, 'max_len_a': 0.0, 'max_len_b': 200, 'min_len': 1, 'match_source_len': False, 'unnormalized': False, 'no_early_stop': False, 'no_beamable_mm': False, 'lenpen': 1.0, 'unkpen': 0.0, 'replace_unk': None, 'sacrebleu': False, 'score_reference': False, 'prefix_size': 0, 'no_repeat_ngram_size': 0, 'sampling': False, 'sampling_topk': -1, 'sampling_topp': -1.0, 'constraints': None, 'temperature': 1.0, 'diverse_beam_groups': -1, 'diverse_beam_strength': 0.5, 'diversity_rate': -1.0, 'print_alignment': None, 'print_step': False, 'lm_path': None, 'lm_weight': 0.0, 'iter_decode_eos_penalty': 0.0, 'iter_decode_max_iter': 10, 'iter_decode_force_max_iter': False, 'iter_decode_with_beam': 1, 'iter_decode_with_external_reranker': False, 'retain_iter_history': False, 'retain_dropout': False, 'retain_dropout_modules': None, 'decoding_format': None, 'no_seed_provided': False}, 'eval_lm': {'_name': None, 'output_word_probs': False, 'output_word_stats': False, 'context_window': 0, 'softmax_batch': 9223372036854775807}, 'interactive': {'_name': None, 'buffer_size': 0, 'input': '-'}, 'model': Namespace(_name='transformer', activation_dropout=0.1, activation_fn='relu', adam_betas=(0.9, 0.999), adam_eps=1e-08, adaptive_input=False, adaptive_softmax_cutoff=None, adaptive_softmax_dropout=0, all_gather_list_size=16384, amp=False, amp_batch_retries=2, amp_init_scale=128, amp_scale_window=None, arch='transformer', attention_dropout=0.1, azureml_logging=False, batch_size=128, batch_size_valid=128, best_checkpoint_metric='loss', bf16=False, bpe=None, broadcast_buffers=False, bucket_cap_mb=25, checkpoint_activations=False, checkpoint_shard_count=1, checkpoint_suffix='', clip_norm=0.0, combine_valid_subsets=None, continue_once=None, cpu=False, cpu_offload=False, criterion='cross_entropy', cross_self_attention=False, curriculum=0, data='/content/content/nmt-kn-te/tokenized.kn-te', data_buffer_size=10, dataset_impl=None, ddp_backend='pytorch_ddp', ddp_comm_hook='none', decoder_attention_heads=8, decoder_embed_dim=256, decoder_embed_path=None, decoder_ffn_embed_dim=512, decoder_input_dim=256, decoder_layerdrop=0, decoder_layers=3, decoder_layers_to_keep=None, decoder_learned_pos=True, decoder_normalize_before=False, decoder_output_dim=256, device_id=0, disable_validation=False, distributed_backend='nccl', distributed_init_method=None, distributed_no_spawn=False, distributed_num_procs=1, distributed_port=-1, distributed_rank=0, distributed_world_size=1, dropout=0.1, ema_decay=0.9999, ema_fp32=False, ema_seed_model=None, ema_start_update=0, ema_update_freq=1, empty_cache_freq=0, encoder_attention_heads=8, encoder_embed_dim=256, encoder_embed_path=None, encoder_ffn_embed_dim=512, encoder_layerdrop=0, encoder_layers=3, encoder_layers_to_keep=None, encoder_learned_pos=True, encoder_normalize_before=False, eos=2, eval_bleu=False, eval_bleu_args='{}', eval_bleu_detok='space', eval_bleu_detok_args='{}', eval_bleu_print_samples=False, eval_bleu_remove_bpe=None, eval_tokenized_bleu=False, fast_stat_sync=False, find_unused_parameters=False, finetune_from_model=None, fix_batches_to_gpus=False, fixed_validation_seed=None, force_anneal=None, fp16=False, fp16_adam_stats=False, fp16_init_scale=128, fp16_no_flatten_grads=False, fp16_scale_tolerance=0.0, fp16_scale_window=None, fp32_reduce_scatter=False, gen_subset='test', gradient_as_bucket_view=False, grouped_shuffling=False, heartbeat_timeout=-1, ignore_unused_valid_subsets=False, keep_best_checkpoints=-1, keep_interval_updates=-1, keep_interval_updates_pattern=-1, keep_last_epochs=-1, layernorm_embedding=False, left_pad_source=True, left_pad_target=False, load_alignments=False, load_checkpoint_on_all_dp_ranks=False, localsgd_frequency=3, log_file=None, log_format=None, log_interval=100, lr=[0.0005], lr_scheduler='fixed', lr_shrink=0.1, max_epoch=10, max_tokens=None, max_tokens_valid=None, max_update=0, max_valid_steps=None, maximize_best_checkpoint_metric=False, memory_efficient_bf16=False, memory_efficient_fp16=False, min_loss_scale=0.0001, model_parallel_size=1, no_cross_attention=False, no_epoch_checkpoints=False, no_last_checkpoints=False, no_progress_bar=False, no_reshard_after_forward=False, no_save=False, no_save_optimizer_state=False, no_scale_embedding=False, no_seed_provided=False, no_token_positional_embeddings=False, not_fsdp_flatten_parameters=False, nprocs_per_node=1, num_batch_buckets=0, num_shards=1, num_workers=1, offload_activations=False, on_cpu_convert_precision=False, optimizer='adam', optimizer_overrides='{}', pad=1, patience=-1, pipeline_balance=None, pipeline_checkpoint='never', pipeline_chunks=0, pipeline_decoder_balance=None, pipeline_decoder_devices=None, pipeline_devices=None, pipeline_encoder_balance=None, pipeline_encoder_devices=None, pipeline_model_parallel=False, plasma_path='/tmp/plasma', profile=False, quant_noise_pq=0, quant_noise_pq_block_size=8, quant_noise_scalar=0, quantization_config_path=None, required_batch_size_multiple=8, required_seq_len_multiple=1, reset_dataloader=False, reset_logging=False, reset_lr_scheduler=False, reset_meters=False, reset_optimizer=False, restore_file='checkpoint_last.pt', save_dir='checkpoints', save_interval=1, save_interval_updates=0, scoring='bleu', seed=1, sentence_avg=False, shard_id=0, share_all_embeddings=False, share_decoder_input_output_embed=False, simul_type=None, skip_invalid_size_inputs_valid_test=False, skip_remainder_batch=False, slowmo_base_algorithm='localsgd', slowmo_momentum=None, source_lang=None, stop_min_lr=-1.0, stop_time_hours=0, store_ema=False, suppress_crashes=False, target_lang=None, task='translation', tensorboard_logdir=None, threshold_loss_scale=None, tie_adaptive_weights=False, tokenizer=None, tpu=False, train_subset='train', truncate_source=False, unk=3, update_epoch_batch_itr=False, update_freq=[1], update_ordered_indices_seed=False, upsample_primary=-1, use_bmuf=False, use_old_adam=False, use_plasma_view=False, use_sharded_state=False, user_dir=None, valid_subset='valid', validate_after_updates=0, validate_interval=1, validate_interval_updates=0, wandb_project='Transformer from scratch - 10 March - kn to te', warmup_updates=0, weight_decay=0.0, write_checkpoints_asynchronously=False, zero_sharding='none'), 'task': {'_name': 'translation', 'data': '/content/content/nmt-kn-te/tokenized.kn-te', 'source_lang': None, 'target_lang': None, 'load_alignments': False, 'left_pad_source': True, 'left_pad_target': False, 'max_source_positions': 1024, 'max_target_positions': 1024, 'upsample_primary': -1, 'truncate_source': False, 'num_batch_buckets': 0, 'train_subset': 'train', 'dataset_impl': None, 'required_seq_len_multiple': 1, 'eval_bleu': False, 'eval_bleu_args': '{}', 'eval_bleu_detok': 'space', 'eval_bleu_detok_args': '{}', 'eval_tokenized_bleu': False, 'eval_bleu_remove_bpe': None, 'eval_bleu_print_samples': False}, 'criterion': {'_name': 'cross_entropy', 'sentence_avg': False}, 'optimizer': {'_name': 'adam', 'adam_betas': [0.9, 0.999], 'adam_eps': 1e-08, 'weight_decay': 0.0, 'use_old_adam': False, 'fp16_adam_stats': False, 'tpu': False, 'lr': [0.0005]}, 'lr_scheduler': {'_name': 'fixed', 'force_anneal': None, 'lr_shrink': 0.1, 'warmup_updates': 0, 'lr': [0.0005]}, 'scoring': {'_name': 'bleu', 'pad': 1, 'eos': 2, 'unk': 3}, 'bpe': None, 'tokenizer': None, 'ema': {'_name': None, 'store_ema': False, 'ema_decay': 0.9999, 'ema_start_update': 0, 'ema_seed_model': None, 'ema_update_freq': 1, 'ema_fp32': False}, 'simul_type': None}\n",
            "2022-03-10 06:13:07 | INFO | fairseq.tasks.translation | [kn] dictionary: 16624 types\n",
            "2022-03-10 06:13:07 | INFO | fairseq.tasks.translation | [te] dictionary: 15992 types\n",
            "2022-03-10 06:13:07 | INFO | fairseq_cli.train | TransformerModel(\n",
            "  (encoder): TransformerEncoderBase(\n",
            "    (dropout_module): FairseqDropout()\n",
            "    (embed_tokens): Embedding(16624, 256, padding_idx=1)\n",
            "    (embed_positions): LearnedPositionalEmbedding(1026, 256, padding_idx=1)\n",
            "    (layers): ModuleList(\n",
            "      (0): TransformerEncoderLayerBase(\n",
            "        (self_attn): MultiheadAttention(\n",
            "          (dropout_module): FairseqDropout()\n",
            "          (k_proj): Linear(in_features=256, out_features=256, bias=True)\n",
            "          (v_proj): Linear(in_features=256, out_features=256, bias=True)\n",
            "          (q_proj): Linear(in_features=256, out_features=256, bias=True)\n",
            "          (out_proj): Linear(in_features=256, out_features=256, bias=True)\n",
            "        )\n",
            "        (self_attn_layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
            "        (dropout_module): FairseqDropout()\n",
            "        (activation_dropout_module): FairseqDropout()\n",
            "        (fc1): Linear(in_features=256, out_features=512, bias=True)\n",
            "        (fc2): Linear(in_features=512, out_features=256, bias=True)\n",
            "        (final_layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
            "      )\n",
            "      (1): TransformerEncoderLayerBase(\n",
            "        (self_attn): MultiheadAttention(\n",
            "          (dropout_module): FairseqDropout()\n",
            "          (k_proj): Linear(in_features=256, out_features=256, bias=True)\n",
            "          (v_proj): Linear(in_features=256, out_features=256, bias=True)\n",
            "          (q_proj): Linear(in_features=256, out_features=256, bias=True)\n",
            "          (out_proj): Linear(in_features=256, out_features=256, bias=True)\n",
            "        )\n",
            "        (self_attn_layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
            "        (dropout_module): FairseqDropout()\n",
            "        (activation_dropout_module): FairseqDropout()\n",
            "        (fc1): Linear(in_features=256, out_features=512, bias=True)\n",
            "        (fc2): Linear(in_features=512, out_features=256, bias=True)\n",
            "        (final_layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
            "      )\n",
            "      (2): TransformerEncoderLayerBase(\n",
            "        (self_attn): MultiheadAttention(\n",
            "          (dropout_module): FairseqDropout()\n",
            "          (k_proj): Linear(in_features=256, out_features=256, bias=True)\n",
            "          (v_proj): Linear(in_features=256, out_features=256, bias=True)\n",
            "          (q_proj): Linear(in_features=256, out_features=256, bias=True)\n",
            "          (out_proj): Linear(in_features=256, out_features=256, bias=True)\n",
            "        )\n",
            "        (self_attn_layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
            "        (dropout_module): FairseqDropout()\n",
            "        (activation_dropout_module): FairseqDropout()\n",
            "        (fc1): Linear(in_features=256, out_features=512, bias=True)\n",
            "        (fc2): Linear(in_features=512, out_features=256, bias=True)\n",
            "        (final_layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
            "      )\n",
            "    )\n",
            "  )\n",
            "  (decoder): TransformerDecoderBase(\n",
            "    (dropout_module): FairseqDropout()\n",
            "    (embed_tokens): Embedding(15992, 256, padding_idx=1)\n",
            "    (embed_positions): LearnedPositionalEmbedding(1026, 256, padding_idx=1)\n",
            "    (layers): ModuleList(\n",
            "      (0): TransformerDecoderLayerBase(\n",
            "        (dropout_module): FairseqDropout()\n",
            "        (self_attn): MultiheadAttention(\n",
            "          (dropout_module): FairseqDropout()\n",
            "          (k_proj): Linear(in_features=256, out_features=256, bias=True)\n",
            "          (v_proj): Linear(in_features=256, out_features=256, bias=True)\n",
            "          (q_proj): Linear(in_features=256, out_features=256, bias=True)\n",
            "          (out_proj): Linear(in_features=256, out_features=256, bias=True)\n",
            "        )\n",
            "        (activation_dropout_module): FairseqDropout()\n",
            "        (self_attn_layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
            "        (encoder_attn): MultiheadAttention(\n",
            "          (dropout_module): FairseqDropout()\n",
            "          (k_proj): Linear(in_features=256, out_features=256, bias=True)\n",
            "          (v_proj): Linear(in_features=256, out_features=256, bias=True)\n",
            "          (q_proj): Linear(in_features=256, out_features=256, bias=True)\n",
            "          (out_proj): Linear(in_features=256, out_features=256, bias=True)\n",
            "        )\n",
            "        (encoder_attn_layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
            "        (fc1): Linear(in_features=256, out_features=512, bias=True)\n",
            "        (fc2): Linear(in_features=512, out_features=256, bias=True)\n",
            "        (final_layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
            "      )\n",
            "      (1): TransformerDecoderLayerBase(\n",
            "        (dropout_module): FairseqDropout()\n",
            "        (self_attn): MultiheadAttention(\n",
            "          (dropout_module): FairseqDropout()\n",
            "          (k_proj): Linear(in_features=256, out_features=256, bias=True)\n",
            "          (v_proj): Linear(in_features=256, out_features=256, bias=True)\n",
            "          (q_proj): Linear(in_features=256, out_features=256, bias=True)\n",
            "          (out_proj): Linear(in_features=256, out_features=256, bias=True)\n",
            "        )\n",
            "        (activation_dropout_module): FairseqDropout()\n",
            "        (self_attn_layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
            "        (encoder_attn): MultiheadAttention(\n",
            "          (dropout_module): FairseqDropout()\n",
            "          (k_proj): Linear(in_features=256, out_features=256, bias=True)\n",
            "          (v_proj): Linear(in_features=256, out_features=256, bias=True)\n",
            "          (q_proj): Linear(in_features=256, out_features=256, bias=True)\n",
            "          (out_proj): Linear(in_features=256, out_features=256, bias=True)\n",
            "        )\n",
            "        (encoder_attn_layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
            "        (fc1): Linear(in_features=256, out_features=512, bias=True)\n",
            "        (fc2): Linear(in_features=512, out_features=256, bias=True)\n",
            "        (final_layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
            "      )\n",
            "      (2): TransformerDecoderLayerBase(\n",
            "        (dropout_module): FairseqDropout()\n",
            "        (self_attn): MultiheadAttention(\n",
            "          (dropout_module): FairseqDropout()\n",
            "          (k_proj): Linear(in_features=256, out_features=256, bias=True)\n",
            "          (v_proj): Linear(in_features=256, out_features=256, bias=True)\n",
            "          (q_proj): Linear(in_features=256, out_features=256, bias=True)\n",
            "          (out_proj): Linear(in_features=256, out_features=256, bias=True)\n",
            "        )\n",
            "        (activation_dropout_module): FairseqDropout()\n",
            "        (self_attn_layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
            "        (encoder_attn): MultiheadAttention(\n",
            "          (dropout_module): FairseqDropout()\n",
            "          (k_proj): Linear(in_features=256, out_features=256, bias=True)\n",
            "          (v_proj): Linear(in_features=256, out_features=256, bias=True)\n",
            "          (q_proj): Linear(in_features=256, out_features=256, bias=True)\n",
            "          (out_proj): Linear(in_features=256, out_features=256, bias=True)\n",
            "        )\n",
            "        (encoder_attn_layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
            "        (fc1): Linear(in_features=256, out_features=512, bias=True)\n",
            "        (fc2): Linear(in_features=512, out_features=256, bias=True)\n",
            "        (final_layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
            "      )\n",
            "    )\n",
            "    (output_projection): Linear(in_features=256, out_features=15992, bias=False)\n",
            "  )\n",
            ")\n",
            "2022-03-10 06:13:07 | INFO | fairseq_cli.train | task: TranslationTask\n",
            "2022-03-10 06:13:07 | INFO | fairseq_cli.train | model: TransformerModel\n",
            "2022-03-10 06:13:07 | INFO | fairseq_cli.train | criterion: CrossEntropyCriterion\n",
            "2022-03-10 06:13:07 | INFO | fairseq_cli.train | num. shared model params: 16,922,624 (num. trained: 16,922,624)\n",
            "2022-03-10 06:13:07 | INFO | fairseq_cli.train | num. expert model params: 0 (num. trained: 0)\n",
            "2022-03-10 06:13:07 | INFO | fairseq.data.data_utils | loaded 1,000 examples from: /content/content/nmt-kn-te/tokenized.kn-te/valid.kn-te.kn\n",
            "2022-03-10 06:13:07 | INFO | fairseq.data.data_utils | loaded 1,000 examples from: /content/content/nmt-kn-te/tokenized.kn-te/valid.kn-te.te\n",
            "2022-03-10 06:13:07 | INFO | fairseq.tasks.translation | /content/content/nmt-kn-te/tokenized.kn-te valid kn-te 1000 examples\n",
            "2022-03-10 06:13:16 | INFO | fairseq.utils | ***********************CUDA enviroments for all 1 workers***********************\n",
            "2022-03-10 06:13:16 | INFO | fairseq.utils | rank   0: capabilities =  3.7  ; total memory = 11.173 GB ; name = Tesla K80                               \n",
            "2022-03-10 06:13:16 | INFO | fairseq.utils | ***********************CUDA enviroments for all 1 workers***********************\n",
            "2022-03-10 06:13:16 | INFO | fairseq_cli.train | training on 1 devices (GPUs/TPUs)\n",
            "2022-03-10 06:13:16 | INFO | fairseq_cli.train | max tokens per device = None and max sentences per device = 128\n",
            "2022-03-10 06:13:16 | INFO | fairseq.trainer | Preparing to load checkpoint checkpoints/checkpoint_last.pt\n",
            "2022-03-10 06:13:16 | INFO | fairseq.trainer | No existing checkpoint found checkpoints/checkpoint_last.pt\n",
            "2022-03-10 06:13:16 | INFO | fairseq.trainer | loading train data for epoch 1\n",
            "2022-03-10 06:13:16 | INFO | fairseq.data.data_utils | loaded 88,488 examples from: /content/content/nmt-kn-te/tokenized.kn-te/train.kn-te.kn\n",
            "2022-03-10 06:13:16 | INFO | fairseq.data.data_utils | loaded 88,488 examples from: /content/content/nmt-kn-te/tokenized.kn-te/train.kn-te.te\n",
            "2022-03-10 06:13:16 | INFO | fairseq.tasks.translation | /content/content/nmt-kn-te/tokenized.kn-te train kn-te 88488 examples\n",
            "2022-03-10 06:13:16 | INFO | fairseq.data.iterators | grouped total_num_itrs = 692\n",
            "epoch 001:   0% 0/692 [00:00<?, ?it/s]\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mrahul2023_usa\u001b[0m (use `wandb login --relogin` to force relogin)\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.12.11\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in \u001b[35m\u001b[1m/content/wandb/run-20220310_061317-epd9hqwt\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run \u001b[1m`wandb offline`\u001b[0m to turn off syncing.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33mcheckpoints\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at \u001b[34m\u001b[4mhttps://wandb.ai/rahul2023_usa/Transformer%20from%20scratch%20-%2010%20March%20-%20kn%20to%20te\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run at \u001b[34m\u001b[4mhttps://wandb.ai/rahul2023_usa/Transformer%20from%20scratch%20-%2010%20March%20-%20kn%20to%20te/runs/epd9hqwt\u001b[0m\n",
            "2022-03-10 06:13:20 | INFO | fairseq.trainer | begin training epoch 1\n",
            "2022-03-10 06:13:20 | INFO | fairseq_cli.train | Start iterating over samples\n",
            "/content/fairseq/fairseq/utils.py:375: UserWarning: amp_C fused kernels unavailable, disabling multi_tensor_l2norm; you may get better performance by installing NVIDIA's apex library\n",
            "  \"amp_C fused kernels unavailable, disabling multi_tensor_l2norm; \"\n",
            "epoch 001:  11% 76/692 [00:17<01:36,  6.39it/s]2022-03-10 06:13:36 | WARNING | fairseq.trainer | OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 1.69 GiB (GPU 0; 11.17 GiB total capacity; 9.21 GiB already allocated; 1.09 GiB free; 9.56 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
            "2022-03-10 06:13:36 | WARNING | fairseq.trainer | |===========================================================================|\n",
            "|                  PyTorch CUDA memory summary, device ID 0                 |\n",
            "|---------------------------------------------------------------------------|\n",
            "|            CUDA OOMs: 1            |        cudaMalloc retries: 2         |\n",
            "|===========================================================================|\n",
            "|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Allocated memory      |    7700 MB |    9434 MB |  133910 MB |  126210 MB |\n",
            "|       from large pool |    7651 MB |    9385 MB |  123674 MB |  116022 MB |\n",
            "|       from small pool |      48 MB |      60 MB |   10236 MB |   10187 MB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Active memory         |    7700 MB |    9434 MB |  133910 MB |  126210 MB |\n",
            "|       from large pool |    7651 MB |    9385 MB |  123674 MB |  116022 MB |\n",
            "|       from small pool |      48 MB |      60 MB |   10236 MB |   10187 MB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| GPU reserved memory   |    9788 MB |   10494 MB |   14228 MB |    4440 MB |\n",
            "|       from large pool |    9728 MB |   10360 MB |   14084 MB |    4356 MB |\n",
            "|       from small pool |      60 MB |     134 MB |     144 MB |      84 MB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Non-releasable memory |  362362 KB |    1006 MB |  112481 MB |  112127 MB |\n",
            "|       from large pool |  350456 KB |     994 MB |  101449 MB |  101107 MB |\n",
            "|       from small pool |   11906 KB |      40 MB |   11031 MB |   11019 MB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Allocations           |     590    |     593    |   85176    |   84586    |\n",
            "|       from large pool |     160    |     164    |   24880    |   24720    |\n",
            "|       from small pool |     430    |     514    |   60296    |   59866    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Active allocs         |     590    |     593    |   85176    |   84586    |\n",
            "|       from large pool |     160    |     164    |   24880    |   24720    |\n",
            "|       from small pool |     430    |     514    |   60296    |   59866    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| GPU reserved segments |      67    |     109    |     121    |      54    |\n",
            "|       from large pool |      37    |      42    |      49    |      12    |\n",
            "|       from small pool |      30    |      67    |      72    |      42    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Non-releasable allocs |      55    |      65    |   38275    |   38220    |\n",
            "|       from large pool |      21    |      21    |   15794    |   15773    |\n",
            "|       from small pool |      34    |      59    |   22481    |   22447    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Oversize allocations  |       0    |       0    |       0    |       0    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Oversize GPU segments |       0    |       0    |       0    |       0    |\n",
            "|===========================================================================|\n",
            "\n",
            "2022-03-10 06:13:36 | WARNING | fairseq.trainer | attempting to recover from OOM in forward/backward pass\n",
            "epoch 001:  37% 254/692 [00:54<01:21,  5.39it/s, loss=7.849, ppl=230.58, wps=10087.3, ups=5.5, wpb=1832.9, bsz=128, num_updates=200, lr=0.0005, gnorm=1.07, train_wall=18, gb_free=10.5, wall=43]2022-03-10 06:14:12 | WARNING | fairseq.trainer | OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 1.46 GiB (GPU 0; 11.17 GiB total capacity; 9.37 GiB already allocated; 291.81 MiB free; 10.37 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
            "2022-03-10 06:14:12 | WARNING | fairseq.trainer | |===========================================================================|\n",
            "|                  PyTorch CUDA memory summary, device ID 0                 |\n",
            "|---------------------------------------------------------------------------|\n",
            "|            CUDA OOMs: 2            |        cudaMalloc retries: 4         |\n",
            "|===========================================================================|\n",
            "|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Allocated memory      |    8105 MB |    9597 MB |  475382 MB |  467277 MB |\n",
            "|       from large pool |    8056 MB |    9548 MB |  444124 MB |  436068 MB |\n",
            "|       from small pool |      48 MB |      60 MB |   31258 MB |   31209 MB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Active memory         |    8105 MB |    9597 MB |  475382 MB |  467277 MB |\n",
            "|       from large pool |    8056 MB |    9548 MB |  444124 MB |  436068 MB |\n",
            "|       from small pool |      48 MB |      60 MB |   31258 MB |   31209 MB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| GPU reserved memory   |   10616 MB |   10616 MB |   17970 MB |    7354 MB |\n",
            "|       from large pool |   10556 MB |   10556 MB |   17748 MB |    7192 MB |\n",
            "|       from small pool |      60 MB |     138 MB |     222 MB |     162 MB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Non-releasable memory |    1018 MB |    1152 MB |  439906 MB |  438887 MB |\n",
            "|       from large pool |    1007 MB |    1140 MB |  406203 MB |  405195 MB |\n",
            "|       from small pool |      11 MB |      40 MB |   33702 MB |   33691 MB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Allocations           |     590    |     593    |  283337    |  282747    |\n",
            "|       from large pool |     160    |     164    |   86937    |   86777    |\n",
            "|       from small pool |     430    |     514    |  196400    |  195970    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Active allocs         |     590    |     593    |  283337    |  282747    |\n",
            "|       from large pool |     160    |     164    |   86937    |   86777    |\n",
            "|       from small pool |     430    |     514    |  196400    |  195970    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| GPU reserved segments |      67    |     107    |     163    |      96    |\n",
            "|       from large pool |      37    |      38    |      52    |      15    |\n",
            "|       from small pool |      30    |      69    |     111    |      81    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Non-releasable allocs |      56    |      66    |  128420    |  128364    |\n",
            "|       from large pool |      25    |      25    |   54800    |   54775    |\n",
            "|       from small pool |      31    |      62    |   73620    |   73589    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Oversize allocations  |       0    |       0    |       0    |       0    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Oversize GPU segments |       0    |       0    |       0    |       0    |\n",
            "|===========================================================================|\n",
            "\n",
            "2022-03-10 06:14:12 | WARNING | fairseq.trainer | attempting to recover from OOM in forward/backward pass\n",
            "epoch 001:  82% 565/692 [02:05<00:29,  4.32it/s, loss=6.98, ppl=126.24, wps=8792, ups=4.54, wpb=1937.7, bsz=128, num_updates=500, lr=0.0005, gnorm=1.018, train_wall=22, gb_free=10.7, wall=109]2022-03-10 06:15:23 | WARNING | fairseq.trainer | OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 2.11 GiB (GPU 0; 11.17 GiB total capacity; 7.85 GiB already allocated; 1.56 GiB free; 9.09 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
            "epoch 001:  82% 566/692 [02:06<00:57,  2.18it/s, loss=6.98, ppl=126.24, wps=8792, ups=4.54, wpb=1937.7, bsz=128, num_updates=500, lr=0.0005, gnorm=1.018, train_wall=22, gb_free=10.7, wall=109]2022-03-10 06:15:23 | WARNING | fairseq.trainer | |===========================================================================|\n",
            "|                  PyTorch CUDA memory summary, device ID 0                 |\n",
            "|---------------------------------------------------------------------------|\n",
            "|            CUDA OOMs: 3            |        cudaMalloc retries: 8         |\n",
            "|===========================================================================|\n",
            "|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Allocated memory      |    8034 MB |    8045 MB |    1149 GB |    1141 GB |\n",
            "|       from large pool |    7985 MB |    7996 MB |    1082 GB |    1075 GB |\n",
            "|       from small pool |      49 MB |      60 MB |      66 GB |      66 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Active memory         |    8034 MB |    8045 MB |    1149 GB |    1141 GB |\n",
            "|       from large pool |    7985 MB |    7996 MB |    1082 GB |    1075 GB |\n",
            "|       from small pool |      49 MB |      60 MB |      66 GB |      66 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| GPU reserved memory   |    9310 MB |   10692 MB |   25716 MB |   16406 MB |\n",
            "|       from large pool |    9250 MB |   10554 MB |   25272 MB |   16022 MB |\n",
            "|       from small pool |      60 MB |     138 MB |     444 MB |     384 MB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Non-releasable memory |    1275 MB |    1961 MB |    1116 GB |    1115 GB |\n",
            "|       from large pool |    1264 MB |    1950 MB |    1045 GB |    1043 GB |\n",
            "|       from small pool |      10 MB |      19 MB |      71 GB |      71 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Allocations           |     590    |     593    |  630489    |  629899    |\n",
            "|       from large pool |     162    |     164    |  196129    |  195967    |\n",
            "|       from small pool |     428    |     514    |  434360    |  433932    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Active allocs         |     590    |     593    |  630489    |  629899    |\n",
            "|       from large pool |     162    |     164    |  196129    |  195967    |\n",
            "|       from small pool |     428    |     514    |  434360    |  433932    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| GPU reserved segments |      63    |     103    |     279    |     216    |\n",
            "|       from large pool |      33    |      34    |      57    |      24    |\n",
            "|       from small pool |      30    |      69    |     222    |     192    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Non-releasable allocs |      53    |      53    |  284431    |  284378    |\n",
            "|       from large pool |      20    |      20    |  123379    |  123359    |\n",
            "|       from small pool |      33    |      44    |  161052    |  161019    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Oversize allocations  |       0    |       0    |       0    |       0    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Oversize GPU segments |       0    |       0    |       0    |       0    |\n",
            "|===========================================================================|\n",
            "\n",
            "2022-03-10 06:15:23 | WARNING | fairseq.trainer | attempting to recover from OOM in forward/backward pass\n",
            "epoch 001:  88% 611/692 [02:18<00:12,  6.38it/s, loss=6.883, ppl=118, wps=7878.2, ups=3.55, wpb=2217.3, bsz=128, num_updates=600, lr=0.0005, gnorm=0.983, train_wall=27, gb_free=10.3, wall=137]2022-03-10 06:15:36 | WARNING | fairseq.trainer | OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 1.74 GiB (GPU 0; 11.17 GiB total capacity; 8.31 GiB already allocated; 1.40 GiB free; 9.25 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
            "2022-03-10 06:15:36 | WARNING | fairseq.trainer | |===========================================================================|\n",
            "|                  PyTorch CUDA memory summary, device ID 0                 |\n",
            "|---------------------------------------------------------------------------|\n",
            "|            CUDA OOMs: 4            |        cudaMalloc retries: 10        |\n",
            "|===========================================================================|\n",
            "|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Allocated memory      |    8508 MB |    8572 MB |    1276 GB |    1267 GB |\n",
            "|       from large pool |    8459 MB |    8524 MB |    1205 GB |    1196 GB |\n",
            "|       from small pool |      48 MB |      60 MB |      71 GB |      70 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Active memory         |    8508 MB |    8572 MB |    1276 GB |    1267 GB |\n",
            "|       from large pool |    8459 MB |    8524 MB |    1205 GB |    1196 GB |\n",
            "|       from small pool |      48 MB |      60 MB |      71 GB |      70 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| GPU reserved memory   |    9476 MB |   10746 MB |   28934 MB |   19458 MB |\n",
            "|       from large pool |    9416 MB |   10618 MB |   28422 MB |   19006 MB |\n",
            "|       from small pool |      60 MB |     128 MB |     512 MB |     452 MB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Non-releasable memory |     967 MB |    1358 MB |    1241 GB |    1240 GB |\n",
            "|       from large pool |     956 MB |    1346 MB |    1165 GB |    1164 GB |\n",
            "|       from small pool |      11 MB |      15 MB |      76 GB |      76 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Allocations           |     591    |     593    |  681159    |  680568    |\n",
            "|       from large pool |     160    |     164    |  212499    |  212339    |\n",
            "|       from small pool |     431    |     514    |  468660    |  468229    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Active allocs         |     591    |     593    |  681159    |  680568    |\n",
            "|       from large pool |     160    |     164    |  212499    |  212339    |\n",
            "|       from small pool |     431    |     514    |  468660    |  468229    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| GPU reserved segments |      63    |      98    |     315    |     252    |\n",
            "|       from large pool |      33    |      34    |      59    |      26    |\n",
            "|       from small pool |      30    |      64    |     256    |     226    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Non-releasable allocs |      62    |      62    |  306917    |  306855    |\n",
            "|       from large pool |      29    |      29    |  133565    |  133536    |\n",
            "|       from small pool |      33    |      45    |  173352    |  173319    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Oversize allocations  |       0    |       0    |       0    |       0    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Oversize GPU segments |       0    |       0    |       0    |       0    |\n",
            "|===========================================================================|\n",
            "\n",
            "2022-03-10 06:15:36 | WARNING | fairseq.trainer | attempting to recover from OOM in forward/backward pass\n",
            "epoch 001:  90% 621/692 [02:21<00:14,  4.81it/s, loss=6.883, ppl=118, wps=7878.2, ups=3.55, wpb=2217.3, bsz=128, num_updates=600, lr=0.0005, gnorm=0.983, train_wall=27, gb_free=10.3, wall=137]2022-03-10 06:15:39 | WARNING | fairseq.trainer | OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 2.01 GiB (GPU 0; 11.17 GiB total capacity; 7.85 GiB already allocated; 1.40 GiB free; 9.26 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
            "2022-03-10 06:15:39 | WARNING | fairseq.trainer | |===========================================================================|\n",
            "|                  PyTorch CUDA memory summary, device ID 0                 |\n",
            "|---------------------------------------------------------------------------|\n",
            "|            CUDA OOMs: 5            |        cudaMalloc retries: 11        |\n",
            "|===========================================================================|\n",
            "|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Allocated memory      |    8036 MB |    8052 MB |    1308 GB |    1300 GB |\n",
            "|       from large pool |    7986 MB |    8003 MB |    1235 GB |    1228 GB |\n",
            "|       from small pool |      49 MB |      60 MB |      72 GB |      72 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Active memory         |    8036 MB |    8052 MB |    1308 GB |    1300 GB |\n",
            "|       from large pool |    7986 MB |    8003 MB |    1235 GB |    1228 GB |\n",
            "|       from small pool |      49 MB |      60 MB |      72 GB |      72 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| GPU reserved memory   |    9478 MB |    9554 MB |   29012 MB |   19534 MB |\n",
            "|       from large pool |    9416 MB |    9416 MB |   28422 MB |   19006 MB |\n",
            "|       from small pool |      62 MB |     138 MB |     590 MB |     528 MB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Non-releasable memory |    1441 MB |    1974 MB |    1276 GB |    1274 GB |\n",
            "|       from large pool |    1429 MB |    1960 MB |    1197 GB |    1196 GB |\n",
            "|       from small pool |      12 MB |      27 MB |      78 GB |      78 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Allocations           |     590    |     593    |  691596    |  691006    |\n",
            "|       from large pool |     162    |     164    |  215629    |  215467    |\n",
            "|       from small pool |     428    |     514    |  475967    |  475539    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Active allocs         |     590    |     593    |  691596    |  691006    |\n",
            "|       from large pool |     162    |     164    |  215629    |  215467    |\n",
            "|       from small pool |     428    |     514    |  475967    |  475539    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| GPU reserved segments |      64    |     102    |     354    |     290    |\n",
            "|       from large pool |      33    |      33    |      59    |      26    |\n",
            "|       from small pool |      31    |      69    |     295    |     264    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Non-releasable allocs |      61    |      61    |  311689    |  311628    |\n",
            "|       from large pool |      28    |      28    |  135464    |  135436    |\n",
            "|       from small pool |      33    |      51    |  176225    |  176192    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Oversize allocations  |       0    |       0    |       0    |       0    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Oversize GPU segments |       0    |       0    |       0    |       0    |\n",
            "|===========================================================================|\n",
            "\n",
            "2022-03-10 06:15:39 | WARNING | fairseq.trainer | attempting to recover from OOM in forward/backward pass\n",
            "epoch 001:  93% 645/692 [02:27<00:10,  4.58it/s, loss=6.883, ppl=118, wps=7878.2, ups=3.55, wpb=2217.3, bsz=128, num_updates=600, lr=0.0005, gnorm=0.983, train_wall=27, gb_free=10.3, wall=137]2022-03-10 06:15:45 | WARNING | fairseq.trainer | OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 2.50 GiB (GPU 0; 11.17 GiB total capacity; 8.45 GiB already allocated; 1.40 GiB free; 9.25 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
            "2022-03-10 06:15:45 | WARNING | fairseq.trainer | |===========================================================================|\n",
            "|                  PyTorch CUDA memory summary, device ID 0                 |\n",
            "|---------------------------------------------------------------------------|\n",
            "|            CUDA OOMs: 6            |        cudaMalloc retries: 12        |\n",
            "|===========================================================================|\n",
            "|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Allocated memory      |    8612 MB |    8653 MB |    1373 GB |    1365 GB |\n",
            "|       from large pool |    8562 MB |    8603 MB |    1299 GB |    1290 GB |\n",
            "|       from small pool |      50 MB |      60 MB |      74 GB |      74 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Active memory         |    8612 MB |    8653 MB |    1373 GB |    1365 GB |\n",
            "|       from large pool |    8562 MB |    8603 MB |    1299 GB |    1290 GB |\n",
            "|       from small pool |      50 MB |      60 MB |      74 GB |      74 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| GPU reserved memory   |    9476 MB |    9550 MB |   29084 MB |   19608 MB |\n",
            "|       from large pool |    9416 MB |    9416 MB |   28422 MB |   19006 MB |\n",
            "|       from small pool |      60 MB |     134 MB |     662 MB |     602 MB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Non-releasable memory |     863 MB |    1882 MB |    1345 GB |    1345 GB |\n",
            "|       from large pool |     853 MB |    1872 MB |    1264 GB |    1264 GB |\n",
            "|       from small pool |       9 MB |      17 MB |      80 GB |      80 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Allocations           |     591    |     593    |  717615    |  717024    |\n",
            "|       from large pool |     162    |     163    |  224084    |  223922    |\n",
            "|       from small pool |     429    |     514    |  493531    |  493102    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Active allocs         |     591    |     593    |  717615    |  717024    |\n",
            "|       from large pool |     162    |     163    |  224084    |  223922    |\n",
            "|       from small pool |     429    |     514    |  493531    |  493102    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| GPU reserved segments |      63    |     100    |     390    |     327    |\n",
            "|       from large pool |      33    |      33    |      59    |      26    |\n",
            "|       from small pool |      30    |      67    |     331    |     301    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Non-releasable allocs |      54    |      56    |  323104    |  323050    |\n",
            "|       from large pool |      22    |      24    |  140722    |  140700    |\n",
            "|       from small pool |      32    |      37    |  182382    |  182350    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Oversize allocations  |       0    |       0    |       0    |       0    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Oversize GPU segments |       0    |       0    |       0    |       0    |\n",
            "|===========================================================================|\n",
            "\n",
            "2022-03-10 06:15:45 | WARNING | fairseq.trainer | attempting to recover from OOM in forward/backward pass\n",
            "epoch 001: 100% 691/692 [02:37<00:00,  4.39it/s, loss=6.883, ppl=118, wps=7878.2, ups=3.55, wpb=2217.3, bsz=128, num_updates=600, lr=0.0005, gnorm=0.983, train_wall=27, gb_free=10.3, wall=137]2022-03-10 06:15:54 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "\n",
            "epoch 001 | valid on 'valid' subset:   0% 0/8 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 001 | valid on 'valid' subset:  12% 1/8 [00:00<00:00,  7.27it/s]\u001b[A\n",
            "epoch 001 | valid on 'valid' subset:  25% 2/8 [00:00<00:00,  8.09it/s]\u001b[A\n",
            "epoch 001 | valid on 'valid' subset:  50% 4/8 [00:00<00:00,  9.90it/s]\u001b[A\n",
            "epoch 001 | valid on 'valid' subset:  62% 5/8 [00:00<00:00,  9.54it/s]\u001b[A\n",
            "epoch 001 | valid on 'valid' subset:  75% 6/8 [00:00<00:00,  9.33it/s]\u001b[A\n",
            "epoch 001 | valid on 'valid' subset:  88% 7/8 [00:00<00:00,  8.90it/s]\u001b[A\n",
            "epoch 001 | valid on 'valid' subset: 100% 8/8 [00:01<00:00,  4.96it/s]\u001b[A\n",
            "                                                                      \u001b[A2022-03-10 06:15:56 | INFO | valid | epoch 001 | valid on 'valid' subset | loss 6.393 | ppl 84.06 | wps 14011.7 | wpb 1992.9 | bsz 125 | num_updates 686\n",
            "2022-03-10 06:15:56 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 1 @ 686 updates\n",
            "2022-03-10 06:15:56 | INFO | fairseq.trainer | Saving checkpoint to /content/checkpoints/checkpoint1.pt\n",
            "2022-03-10 06:15:56 | INFO | fairseq.trainer | Finished saving checkpoint to /content/checkpoints/checkpoint1.pt\n",
            "2022-03-10 06:15:58 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/checkpoint1.pt (epoch 1 @ 686 updates, score 6.393) (writing took 1.9597411070000135 seconds)\n",
            "2022-03-10 06:15:58 | INFO | fairseq_cli.train | end of epoch 1 (average epoch stats below)\n",
            "2022-03-10 06:15:58 | INFO | train | epoch 001 | loss 7.448 | ppl 174.57 | wps 8514.4 | ups 4.37 | wpb 1946.5 | bsz 127.9 | num_updates 686 | lr 0.0005 | gnorm 1.119 | train_wall 146 | gb_free 10.6 | wall 161\n",
            "epoch 002:   0% 0/692 [00:00<?, ?it/s]2022-03-10 06:15:58 | INFO | fairseq.data.iterators | grouped total_num_itrs = 692\n",
            "2022-03-10 06:15:58 | INFO | fairseq.trainer | begin training epoch 2\n",
            "2022-03-10 06:15:58 | INFO | fairseq_cli.train | Start iterating over samples\n",
            "epoch 002:   6% 40/692 [00:10<03:12,  3.38it/s, loss=6.39, ppl=83.84, wps=7232.6, ups=3.7, wpb=1956.2, bsz=128, num_updates=700, lr=0.0005, gnorm=0.938, train_wall=21, gb_free=10.6, wall=164]2022-03-10 06:16:09 | WARNING | fairseq.trainer | OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 1.46 GiB (GPU 0; 11.17 GiB total capacity; 9.38 GiB already allocated; 1.19 GiB free; 9.46 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
            "2022-03-10 06:16:09 | WARNING | fairseq.trainer | |===========================================================================|\n",
            "|                  PyTorch CUDA memory summary, device ID 0                 |\n",
            "|---------------------------------------------------------------------------|\n",
            "|            CUDA OOMs: 7            |        cudaMalloc retries: 15        |\n",
            "|===========================================================================|\n",
            "|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Allocated memory      |    8115 MB |    9607 MB |    1571 GB |    1563 GB |\n",
            "|       from large pool |    8067 MB |    9559 MB |    1486 GB |    1478 GB |\n",
            "|       from small pool |      48 MB |      60 MB |      85 GB |      85 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Active memory         |    8115 MB |    9607 MB |    1571 GB |    1563 GB |\n",
            "|       from large pool |    8067 MB |    9559 MB |    1486 GB |    1478 GB |\n",
            "|       from small pool |      48 MB |      60 MB |      85 GB |      85 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| GPU reserved memory   |    9690 MB |    9824 MB |   40100 MB |   30410 MB |\n",
            "|       from large pool |    9630 MB |    9690 MB |   39286 MB |   29656 MB |\n",
            "|       from small pool |      60 MB |     134 MB |     814 MB |     754 MB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Non-releasable memory |   84024 KB |  183399 KB |    1537 GB |    1536 GB |\n",
            "|       from large pool |   72699 KB |  170289 KB |    1444 GB |    1444 GB |\n",
            "|       from small pool |   11325 KB |   19784 KB |      92 GB |      92 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Allocations           |     592    |     595    |     817 K  |     816 K  |\n",
            "|       from large pool |     160    |     164    |     256 K  |     255 K  |\n",
            "|       from small pool |     432    |     516    |     561 K  |     560 K  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Active allocs         |     592    |     595    |     817 K  |     816 K  |\n",
            "|       from large pool |     160    |     164    |     256 K  |     255 K  |\n",
            "|       from small pool |     432    |     516    |     561 K  |     560 K  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| GPU reserved segments |     124    |     164    |     556    |     432    |\n",
            "|       from large pool |      94    |      97    |     149    |      55    |\n",
            "|       from small pool |      30    |      67    |     407    |     377    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Non-releasable allocs |      62    |      63    |  368291    |  368229    |\n",
            "|       from large pool |      26    |      27    |  160513    |  160487    |\n",
            "|       from small pool |      36    |      48    |  207778    |  207742    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Oversize allocations  |       0    |       0    |       0    |       0    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Oversize GPU segments |       0    |       0    |       0    |       0    |\n",
            "|===========================================================================|\n",
            "\n",
            "2022-03-10 06:16:09 | WARNING | fairseq.trainer | attempting to recover from OOM in forward/backward pass\n",
            "epoch 002:  31% 213/692 [00:44<01:06,  7.22it/s, loss=5.841, ppl=57.34, wps=8174.9, ups=4.77, wpb=1713.5, bsz=128, num_updates=800, lr=0.0005, gnorm=1.008, train_wall=20, gb_free=8.6, wall=185]2022-03-10 06:16:43 | WARNING | fairseq.trainer | OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 1.69 GiB (GPU 0; 11.17 GiB total capacity; 9.21 GiB already allocated; 343.81 MiB free; 10.32 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
            "2022-03-10 06:16:43 | WARNING | fairseq.trainer | |===========================================================================|\n",
            "|                  PyTorch CUDA memory summary, device ID 0                 |\n",
            "|---------------------------------------------------------------------------|\n",
            "|            CUDA OOMs: 8            |        cudaMalloc retries: 18        |\n",
            "|===========================================================================|\n",
            "|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Allocated memory      |    7699 MB |    9433 MB |    1875 GB |    1867 GB |\n",
            "|       from large pool |    7651 MB |    9385 MB |    1767 GB |    1760 GB |\n",
            "|       from small pool |      48 MB |      60 MB |     107 GB |     107 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Active memory         |    7699 MB |    9433 MB |    1875 GB |    1867 GB |\n",
            "|       from large pool |    7651 MB |    9385 MB |    1767 GB |    1760 GB |\n",
            "|       from small pool |      48 MB |      60 MB |     107 GB |     107 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| GPU reserved memory   |   10564 MB |   10596 MB |   48942 MB |   38378 MB |\n",
            "|       from large pool |   10504 MB |   10536 MB |   47972 MB |   37468 MB |\n",
            "|       from small pool |      60 MB |     138 MB |     970 MB |     910 MB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Non-releasable memory |    1130 MB |    1633 MB |    1843 GB |    1842 GB |\n",
            "|       from large pool |    1118 MB |    1621 MB |    1727 GB |    1726 GB |\n",
            "|       from small pool |      11 MB |      32 MB |     115 GB |     115 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Allocations           |     592    |     595    |    1008 K  |    1007 K  |\n",
            "|       from large pool |     160    |     164    |     313 K  |     313 K  |\n",
            "|       from small pool |     432    |     516    |     694 K  |     694 K  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Active allocs         |     592    |     595    |    1008 K  |    1007 K  |\n",
            "|       from large pool |     160    |     164    |     313 K  |     313 K  |\n",
            "|       from small pool |     432    |     516    |     694 K  |     694 K  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| GPU reserved segments |      79    |     143    |     641    |     562    |\n",
            "|       from large pool |      49    |      74    |     156    |     107    |\n",
            "|       from small pool |      30    |      69    |     485    |     455    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Non-releasable allocs |      67    |      70    |  456809    |  456742    |\n",
            "|       from large pool |      33    |      35    |  198024    |  197991    |\n",
            "|       from small pool |      34    |      49    |  258785    |  258751    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Oversize allocations  |       0    |       0    |       0    |       0    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Oversize GPU segments |       0    |       0    |       0    |       0    |\n",
            "|===========================================================================|\n",
            "\n",
            "2022-03-10 06:16:43 | WARNING | fairseq.trainer | attempting to recover from OOM in forward/backward pass\n",
            "epoch 002:  54% 373/692 [01:19<00:56,  5.61it/s, loss=5.857, ppl=57.94, wps=9270.2, ups=4.6, wpb=2014.4, bsz=128, num_updates=1000, lr=0.0005, gnorm=1.027, train_wall=21, gb_free=10.2, wall=229]2022-03-10 06:17:18 | WARNING | fairseq.trainer | OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 2.50 GiB (GPU 0; 11.17 GiB total capacity; 8.45 GiB already allocated; 929.81 MiB free; 9.74 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
            "2022-03-10 06:17:18 | WARNING | fairseq.trainer | |===========================================================================|\n",
            "|                  PyTorch CUDA memory summary, device ID 0                 |\n",
            "|---------------------------------------------------------------------------|\n",
            "|            CUDA OOMs: 9            |        cudaMalloc retries: 19        |\n",
            "|===========================================================================|\n",
            "|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Allocated memory      |    8609 MB |    8651 MB |    2210 GB |    2201 GB |\n",
            "|       from large pool |    8559 MB |    8600 MB |    2085 GB |    2076 GB |\n",
            "|       from small pool |      50 MB |      60 MB |     124 GB |     124 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Active memory         |    8609 MB |    8651 MB |    2210 GB |    2201 GB |\n",
            "|       from large pool |    8559 MB |    8600 MB |    2085 GB |    2076 GB |\n",
            "|       from small pool |      50 MB |      60 MB |     124 GB |     124 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| GPU reserved memory   |    9978 MB |   10052 MB |   50164 MB |   40186 MB |\n",
            "|       from large pool |    9918 MB |    9918 MB |   49120 MB |   39202 MB |\n",
            "|       from small pool |      60 MB |     134 MB |    1044 MB |     984 MB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Non-releasable memory |    1368 MB |    2242 MB |    2233 GB |    2231 GB |\n",
            "|       from large pool |    1358 MB |    2231 MB |    2098 GB |    2096 GB |\n",
            "|       from small pool |       9 MB |      16 MB |     134 GB |     134 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Allocations           |     593    |     595    |    1186 K  |    1186 K  |\n",
            "|       from large pool |     162    |     163    |     370 K  |     370 K  |\n",
            "|       from small pool |     431    |     516    |     815 K  |     815 K  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Active allocs         |     593    |     595    |    1186 K  |    1186 K  |\n",
            "|       from large pool |     162    |     163    |     370 K  |     370 K  |\n",
            "|       from small pool |     431    |     516    |     815 K  |     815 K  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| GPU reserved segments |      79    |     116    |     679    |     600    |\n",
            "|       from large pool |      49    |      49    |     157    |     108    |\n",
            "|       from small pool |      30    |      67    |     522    |     492    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Non-releasable allocs |      74    |      75    |  538027    |  537953    |\n",
            "|       from large pool |      41    |      42    |  235195    |  235154    |\n",
            "|       from small pool |      33    |      40    |  302832    |  302799    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Oversize allocations  |       0    |       0    |       0    |       0    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Oversize GPU segments |       0    |       0    |       0    |       0    |\n",
            "|===========================================================================|\n",
            "\n",
            "2022-03-10 06:17:18 | WARNING | fairseq.trainer | attempting to recover from OOM in forward/backward pass\n",
            "epoch 002:  72% 499/692 [01:47<01:19,  2.42it/s, loss=5.661, ppl=50.61, wps=8939.5, ups=4.8, wpb=1862.2, bsz=128, num_updates=1100, lr=0.0005, gnorm=1.072, train_wall=20, gb_free=9, wall=250]2022-03-10 06:17:46 | WARNING | fairseq.trainer | OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 2.01 GiB (GPU 0; 11.17 GiB total capacity; 9.79 GiB already allocated; 91.81 MiB free; 10.56 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
            "2022-03-10 06:17:46 | WARNING | fairseq.trainer | |===========================================================================|\n",
            "|                  PyTorch CUDA memory summary, device ID 0                 |\n",
            "|---------------------------------------------------------------------------|\n",
            "|            CUDA OOMs: 10           |        cudaMalloc retries: 23        |\n",
            "|===========================================================================|\n",
            "|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Allocated memory      |   10025 MB |   10100 MB |    2474 GB |    2464 GB |\n",
            "|       from large pool |    9976 MB |   10050 MB |    2333 GB |    2323 GB |\n",
            "|       from small pool |      49 MB |      60 MB |     141 GB |     141 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Active memory         |   10025 MB |   10100 MB |    2474 GB |    2464 GB |\n",
            "|       from large pool |    9976 MB |   10050 MB |    2333 GB |    2323 GB |\n",
            "|       from small pool |      49 MB |      60 MB |     141 GB |     141 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| GPU reserved memory   |   10816 MB |   10816 MB |   57094 MB |   46278 MB |\n",
            "|       from large pool |   10754 MB |   10754 MB |   55878 MB |   45124 MB |\n",
            "|       from small pool |      62 MB |      84 MB |    1216 MB |    1154 MB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Non-releasable memory |     790 MB |    1723 MB |    2545 GB |    2544 GB |\n",
            "|       from large pool |     777 MB |    1710 MB |    2393 GB |    2392 GB |\n",
            "|       from small pool |      12 MB |      18 MB |     152 GB |     152 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Allocations           |     593    |     595    |    1326 K  |    1325 K  |\n",
            "|       from large pool |     160    |     164    |     411 K  |     411 K  |\n",
            "|       from small pool |     433    |     516    |     914 K  |     913 K  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Active allocs         |     593    |     595    |    1326 K  |    1325 K  |\n",
            "|       from large pool |     160    |     164    |     411 K  |     411 K  |\n",
            "|       from small pool |     433    |     516    |     914 K  |     913 K  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| GPU reserved segments |      79    |      90    |     769    |     690    |\n",
            "|       from large pool |      48    |      48    |     161    |     113    |\n",
            "|       from small pool |      31    |      42    |     608    |     577    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Non-releasable allocs |      76    |      77    |  601693    |  601617    |\n",
            "|       from large pool |      40    |      40    |  261758    |  261718    |\n",
            "|       from small pool |      36    |      49    |  339935    |  339899    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Oversize allocations  |       0    |       0    |       0    |       0    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Oversize GPU segments |       0    |       0    |       0    |       0    |\n",
            "|===========================================================================|\n",
            "\n",
            "2022-03-10 06:17:46 | WARNING | fairseq.trainer | attempting to recover from OOM in forward/backward pass\n",
            "epoch 002:  83% 571/692 [02:04<00:28,  4.22it/s, loss=5.638, ppl=49.8, wps=8029.4, ups=4.04, wpb=1987.1, bsz=128, num_updates=1200, lr=0.0005, gnorm=1.033, train_wall=23, gb_free=10.7, wall=274]2022-03-10 06:18:03 | WARNING | fairseq.trainer | OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 1.46 GiB (GPU 0; 11.17 GiB total capacity; 7.60 GiB already allocated; 1.43 GiB free; 9.22 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
            "2022-03-10 06:18:03 | WARNING | fairseq.trainer | |===========================================================================|\n",
            "|                  PyTorch CUDA memory summary, device ID 0                 |\n",
            "|---------------------------------------------------------------------------|\n",
            "|            CUDA OOMs: 11           |        cudaMalloc retries: 24        |\n",
            "|===========================================================================|\n",
            "|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Allocated memory      |    6294 MB |    7785 MB |    2637 GB |    2631 GB |\n",
            "|       from large pool |    6237 MB |    7729 MB |    2488 GB |    2482 GB |\n",
            "|       from small pool |      56 MB |      60 MB |     148 GB |     148 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Active memory         |    6294 MB |    7785 MB |    2637 GB |    2631 GB |\n",
            "|       from large pool |    6237 MB |    7729 MB |    2488 GB |    2482 GB |\n",
            "|       from small pool |      56 MB |      60 MB |     148 GB |     148 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| GPU reserved memory   |    9442 MB |   10884 MB |   57162 MB |   47720 MB |\n",
            "|       from large pool |    9378 MB |   10754 MB |   55878 MB |   46500 MB |\n",
            "|       from small pool |      64 MB |     130 MB |    1284 MB |    1220 MB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Non-releasable memory |    1085 MB |    1656 MB |    2741 GB |    2740 GB |\n",
            "|       from large pool |    1078 MB |    1648 MB |    2580 GB |    2579 GB |\n",
            "|       from small pool |       7 MB |      17 MB |     160 GB |     160 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Allocations           |     592    |     595    |    1405 K  |    1405 K  |\n",
            "|       from large pool |     150    |     154    |     437 K  |     437 K  |\n",
            "|       from small pool |     442    |     516    |     968 K  |     967 K  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Active allocs         |     592    |     595    |    1405 K  |    1405 K  |\n",
            "|       from large pool |     150    |     154    |     437 K  |     437 K  |\n",
            "|       from small pool |     442    |     516    |     968 K  |     967 K  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| GPU reserved segments |      78    |     113    |     803    |     725    |\n",
            "|       from large pool |      46    |      48    |     161    |     115    |\n",
            "|       from small pool |      32    |      65    |     642    |     610    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Non-releasable allocs |      58    |      59    |  638295    |  638237    |\n",
            "|       from large pool |      24    |      25    |  278371    |  278347    |\n",
            "|       from small pool |      34    |      38    |  359924    |  359890    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Oversize allocations  |       0    |       0    |       0    |       0    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Oversize GPU segments |       0    |       0    |       0    |       0    |\n",
            "|===========================================================================|\n",
            "\n",
            "2022-03-10 06:18:03 | WARNING | fairseq.trainer | attempting to recover from OOM in forward/backward pass\n",
            "epoch 002:  93% 644/692 [02:20<00:09,  5.04it/s, loss=5.476, ppl=44.51, wps=8959.4, ups=4.74, wpb=1888.7, bsz=128, num_updates=1300, lr=0.0005, gnorm=1.096, train_wall=20, gb_free=10.5, wall=296]2022-03-10 06:18:19 | WARNING | fairseq.trainer | OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 1.74 GiB (GPU 0; 11.17 GiB total capacity; 10.05 GiB already allocated; 91.81 MiB free; 10.56 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
            "2022-03-10 06:18:19 | WARNING | fairseq.trainer | |===========================================================================|\n",
            "|                  PyTorch CUDA memory summary, device ID 0                 |\n",
            "|---------------------------------------------------------------------------|\n",
            "|            CUDA OOMs: 12           |        cudaMalloc retries: 26        |\n",
            "|===========================================================================|\n",
            "|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Allocated memory      |    8510 MB |   10290 MB |    2790 GB |    2782 GB |\n",
            "|       from large pool |    8461 MB |   10241 MB |    2633 GB |    2624 GB |\n",
            "|       from small pool |      48 MB |      60 MB |     157 GB |     157 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Active memory         |    8510 MB |   10290 MB |    2790 GB |    2782 GB |\n",
            "|       from large pool |    8461 MB |   10241 MB |    2633 GB |    2624 GB |\n",
            "|       from small pool |      48 MB |      60 MB |     157 GB |     157 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| GPU reserved memory   |   10816 MB |   10816 MB |   63536 MB |   52720 MB |\n",
            "|       from large pool |   10756 MB |   10756 MB |   62178 MB |   51422 MB |\n",
            "|       from small pool |      60 MB |     138 MB |    1358 MB |    1298 MB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Non-releasable memory |  536259 KB |    1479 MB |    2914 GB |    2914 GB |\n",
            "|       from large pool |  524912 KB |    1467 MB |    2744 GB |    2744 GB |\n",
            "|       from small pool |   11347 KB |      29 MB |     170 GB |     170 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Allocations           |     592    |     595    |    1486 K  |    1485 K  |\n",
            "|       from large pool |     160    |     164    |     462 K  |     461 K  |\n",
            "|       from small pool |     432    |     516    |    1024 K  |    1023 K  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Active allocs         |     592    |     595    |    1486 K  |    1485 K  |\n",
            "|       from large pool |     160    |     164    |     462 K  |     461 K  |\n",
            "|       from small pool |     432    |     516    |    1024 K  |    1023 K  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| GPU reserved segments |      77    |     116    |     844    |     767    |\n",
            "|       from large pool |      47    |      47    |     165    |     118    |\n",
            "|       from small pool |      30    |      69    |     679    |     649    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Non-releasable allocs |      69    |      70    |  675649    |  675580    |\n",
            "|       from large pool |      34    |      35    |  294355    |  294321    |\n",
            "|       from small pool |      35    |      59    |  381294    |  381259    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Oversize allocations  |       0    |       0    |       0    |       0    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Oversize GPU segments |       0    |       0    |       0    |       0    |\n",
            "|===========================================================================|\n",
            "\n",
            "2022-03-10 06:18:19 | WARNING | fairseq.trainer | attempting to recover from OOM in forward/backward pass\n",
            "epoch 002:  97% 673/692 [02:27<00:05,  3.61it/s, loss=5.476, ppl=44.51, wps=8959.4, ups=4.74, wpb=1888.7, bsz=128, num_updates=1300, lr=0.0005, gnorm=1.096, train_wall=20, gb_free=10.5, wall=296]2022-03-10 06:18:26 | WARNING | fairseq.trainer | OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 2.11 GiB (GPU 0; 11.17 GiB total capacity; 7.85 GiB already allocated; 1.73 GiB free; 8.92 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
            "2022-03-10 06:18:26 | WARNING | fairseq.trainer | |===========================================================================|\n",
            "|                  PyTorch CUDA memory summary, device ID 0                 |\n",
            "|---------------------------------------------------------------------------|\n",
            "|            CUDA OOMs: 13           |        cudaMalloc retries: 28        |\n",
            "|===========================================================================|\n",
            "|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Allocated memory      |    8033 MB |    8044 MB |    2858 GB |    2850 GB |\n",
            "|       from large pool |    7984 MB |    7995 MB |    2698 GB |    2690 GB |\n",
            "|       from small pool |      49 MB |      60 MB |     159 GB |     159 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Active memory         |    8033 MB |    8044 MB |    2858 GB |    2850 GB |\n",
            "|       from large pool |    7984 MB |    7995 MB |    2698 GB |    2690 GB |\n",
            "|       from small pool |      49 MB |      60 MB |     159 GB |     159 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| GPU reserved memory   |    9136 MB |    9136 MB |   65758 MB |   56622 MB |\n",
            "|       from large pool |    9076 MB |    9076 MB |   64342 MB |   55266 MB |\n",
            "|       from small pool |      60 MB |     118 MB |    1416 MB |    1356 MB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Non-releasable memory |    1102 MB |    2088 MB |    2993 GB |    2992 GB |\n",
            "|       from large pool |    1091 MB |    2077 MB |    2820 GB |    2819 GB |\n",
            "|       from small pool |      10 MB |      19 MB |     172 GB |     172 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Allocations           |     592    |     595    |    1518 K  |    1517 K  |\n",
            "|       from large pool |     162    |     164    |     473 K  |     472 K  |\n",
            "|       from small pool |     430    |     516    |    1045 K  |    1044 K  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Active allocs         |     592    |     595    |    1518 K  |    1517 K  |\n",
            "|       from large pool |     162    |     164    |     473 K  |     472 K  |\n",
            "|       from small pool |     430    |     516    |    1045 K  |    1044 K  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| GPU reserved segments |      76    |     105    |     874    |     798    |\n",
            "|       from large pool |      46    |      46    |     166    |     120    |\n",
            "|       from small pool |      30    |      59    |     708    |     678    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Non-releasable allocs |      69    |      69    |  690455    |  690386    |\n",
            "|       from large pool |      35    |      35    |  301399    |  301364    |\n",
            "|       from small pool |      34    |      48    |  389056    |  389022    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Oversize allocations  |       0    |       0    |       0    |       0    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Oversize GPU segments |       0    |       0    |       0    |       0    |\n",
            "|===========================================================================|\n",
            "\n",
            "2022-03-10 06:18:26 | WARNING | fairseq.trainer | attempting to recover from OOM in forward/backward pass\n",
            "epoch 002: 100% 691/692 [02:34<00:00,  2.72it/s, loss=5.476, ppl=44.51, wps=8959.4, ups=4.74, wpb=1888.7, bsz=128, num_updates=1300, lr=0.0005, gnorm=1.096, train_wall=20, gb_free=10.5, wall=296]2022-03-10 06:18:33 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "\n",
            "epoch 002 | valid on 'valid' subset:   0% 0/8 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 002 | valid on 'valid' subset:  12% 1/8 [00:00<00:00,  7.80it/s]\u001b[A\n",
            "epoch 002 | valid on 'valid' subset:  25% 2/8 [00:00<00:00,  8.74it/s]\u001b[A\n",
            "epoch 002 | valid on 'valid' subset:  50% 4/8 [00:00<00:00, 10.32it/s]\u001b[A\n",
            "epoch 002 | valid on 'valid' subset:  75% 6/8 [00:00<00:00,  9.51it/s]\u001b[A\n",
            "epoch 002 | valid on 'valid' subset:  88% 7/8 [00:00<00:00,  9.08it/s]\u001b[A\n",
            "epoch 002 | valid on 'valid' subset: 100% 8/8 [00:01<00:00,  5.31it/s]\u001b[A\n",
            "                                                                      \u001b[A2022-03-10 06:18:34 | INFO | valid | epoch 002 | valid on 'valid' subset | loss 5.462 | ppl 44.06 | wps 14183.7 | wpb 1992.9 | bsz 125 | num_updates 1371 | best_loss 5.462\n",
            "2022-03-10 06:18:34 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 2 @ 1371 updates\n",
            "2022-03-10 06:18:34 | INFO | fairseq.trainer | Saving checkpoint to /content/checkpoints/checkpoint2.pt\n",
            "2022-03-10 06:18:35 | INFO | fairseq.trainer | Finished saving checkpoint to /content/checkpoints/checkpoint2.pt\n",
            "epoch 002: 100% 692/692 [02:38<00:00,  1.42s/it, loss=5.476, ppl=44.51, wps=8959.4, ups=4.74, wpb=1888.7, bsz=128, num_updates=1300, lr=0.0005, gnorm=1.096, train_wall=20, gb_free=10.5, wall=296]2022-03-10 06:18:36 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/checkpoint2.pt (epoch 2 @ 1371 updates, score 5.462) (writing took 2.0337758819999863 seconds)\n",
            "2022-03-10 06:18:36 | INFO | fairseq_cli.train | end of epoch 2 (average epoch stats below)\n",
            "2022-03-10 06:18:36 | INFO | train | epoch 002 | loss 5.716 | ppl 52.57 | wps 8379.2 | ups 4.32 | wpb 1941.5 | bsz 127.9 | num_updates 1371 | lr 0.0005 | gnorm 1.041 | train_wall 146 | gb_free 7 | wall 320\n",
            "epoch 003:   0% 0/692 [00:00<?, ?it/s]2022-03-10 06:18:36 | INFO | fairseq.data.iterators | grouped total_num_itrs = 692\n",
            "2022-03-10 06:18:36 | INFO | fairseq.trainer | begin training epoch 3\n",
            "2022-03-10 06:18:36 | INFO | fairseq_cli.train | Start iterating over samples\n",
            "epoch 003:  28% 192/692 [00:40<02:10,  3.84it/s, loss=5.021, ppl=32.46, wps=8796, ups=4.56, wpb=1927.3, bsz=128, num_updates=1500, lr=0.0005, gnorm=1.083, train_wall=22, gb_free=10.7, wall=348]2022-03-10 06:19:18 | WARNING | fairseq.trainer | OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 2.11 GiB (GPU 0; 11.17 GiB total capacity; 7.85 GiB already allocated; 1.92 GiB free; 8.74 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
            "2022-03-10 06:19:18 | WARNING | fairseq.trainer | |===========================================================================|\n",
            "|                  PyTorch CUDA memory summary, device ID 0                 |\n",
            "|---------------------------------------------------------------------------|\n",
            "|            CUDA OOMs: 14           |        cudaMalloc retries: 31        |\n",
            "|===========================================================================|\n",
            "|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Allocated memory      |    8033 MB |    8044 MB |    3324 GB |    3316 GB |\n",
            "|       from large pool |    7984 MB |    7995 MB |    3140 GB |    3132 GB |\n",
            "|       from small pool |      49 MB |      60 MB |     184 GB |     184 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Active memory         |    8033 MB |    8044 MB |    3324 GB |    3316 GB |\n",
            "|       from large pool |    7984 MB |    7995 MB |    3140 GB |    3132 GB |\n",
            "|       from small pool |      49 MB |      60 MB |     184 GB |     184 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| GPU reserved memory   |    8946 MB |   10806 MB |   71426 MB |   62480 MB |\n",
            "|       from large pool |    8886 MB |   10668 MB |   69788 MB |   60902 MB |\n",
            "|       from small pool |      60 MB |     138 MB |    1638 MB |    1578 MB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Non-releasable memory |     912 MB |    1837 MB |    3539 GB |    3538 GB |\n",
            "|       from large pool |     901 MB |    1825 MB |    3340 GB |    3339 GB |\n",
            "|       from small pool |      10 MB |      15 MB |     199 GB |     199 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Allocations           |     592    |     595    |    1755 K  |    1755 K  |\n",
            "|       from large pool |     162    |     164    |     548 K  |     548 K  |\n",
            "|       from small pool |     430    |     516    |    1206 K  |    1206 K  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Active allocs         |     592    |     595    |    1755 K  |    1755 K  |\n",
            "|       from large pool |     162    |     164    |     548 K  |     548 K  |\n",
            "|       from small pool |     430    |     516    |    1206 K  |    1206 K  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| GPU reserved segments |      75    |     115    |     989    |     914    |\n",
            "|       from large pool |      45    |      46    |     170    |     125    |\n",
            "|       from small pool |      30    |      69    |     819    |     789    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Non-releasable allocs |      68    |      68    |     798 K  |     798 K  |\n",
            "|       from large pool |      34    |      34    |     350 K  |     350 K  |\n",
            "|       from small pool |      34    |      40    |     448 K  |     448 K  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Oversize allocations  |       0    |       0    |       0    |       0    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Oversize GPU segments |       0    |       0    |       0    |       0    |\n",
            "|===========================================================================|\n",
            "\n",
            "2022-03-10 06:19:18 | WARNING | fairseq.trainer | attempting to recover from OOM in forward/backward pass\n",
            "epoch 003:  29% 203/692 [00:44<03:22,  2.42it/s, loss=5.021, ppl=32.46, wps=8796, ups=4.56, wpb=1927.3, bsz=128, num_updates=1500, lr=0.0005, gnorm=1.083, train_wall=22, gb_free=10.7, wall=348]2022-03-10 06:19:22 | WARNING | fairseq.trainer | OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 2.01 GiB (GPU 0; 11.17 GiB total capacity; 9.79 GiB already allocated; 297.81 MiB free; 10.36 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
            "2022-03-10 06:19:22 | WARNING | fairseq.trainer | |===========================================================================|\n",
            "|                  PyTorch CUDA memory summary, device ID 0                 |\n",
            "|---------------------------------------------------------------------------|\n",
            "|            CUDA OOMs: 15           |        cudaMalloc retries: 33        |\n",
            "|===========================================================================|\n",
            "|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Allocated memory      |   10024 MB |   10099 MB |    3375 GB |    3365 GB |\n",
            "|       from large pool |    9975 MB |   10050 MB |    3189 GB |    3180 GB |\n",
            "|       from small pool |      49 MB |      60 MB |     185 GB |     185 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Active memory         |   10024 MB |   10099 MB |    3375 GB |    3365 GB |\n",
            "|       from large pool |    9975 MB |   10050 MB |    3189 GB |    3180 GB |\n",
            "|       from small pool |      49 MB |      60 MB |     185 GB |     185 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| GPU reserved memory   |   10610 MB |   10610 MB |   74630 MB |   64020 MB |\n",
            "|       from large pool |   10550 MB |   10550 MB |   72944 MB |   62394 MB |\n",
            "|       from small pool |      60 MB |     108 MB |    1686 MB |    1626 MB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Non-releasable memory |  599188 KB |    1721 MB |    3591 GB |    3590 GB |\n",
            "|       from large pool |  588248 KB |    1709 MB |    3391 GB |    3390 GB |\n",
            "|       from small pool |   10940 KB |      34 MB |     200 GB |     200 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Allocations           |     593    |     595    |    1767 K  |    1766 K  |\n",
            "|       from large pool |     160    |     164    |     552 K  |     552 K  |\n",
            "|       from small pool |     433    |     516    |    1214 K  |    1214 K  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Active allocs         |     593    |     595    |    1767 K  |    1766 K  |\n",
            "|       from large pool |     160    |     164    |     552 K  |     552 K  |\n",
            "|       from small pool |     433    |     516    |    1214 K  |    1214 K  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| GPU reserved segments |      76    |     100    |    1015    |     939    |\n",
            "|       from large pool |      46    |      46    |     172    |     126    |\n",
            "|       from small pool |      30    |      54    |     843    |     813    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Non-releasable allocs |      73    |      74    |     803 K  |     803 K  |\n",
            "|       from large pool |      39    |      39    |     352 K  |     352 K  |\n",
            "|       from small pool |      34    |      52    |     451 K  |     451 K  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Oversize allocations  |       0    |       0    |       0    |       0    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Oversize GPU segments |       0    |       0    |       0    |       0    |\n",
            "|===========================================================================|\n",
            "\n",
            "2022-03-10 06:19:22 | WARNING | fairseq.trainer | attempting to recover from OOM in forward/backward pass\n",
            "epoch 003:  45% 311/692 [01:07<00:58,  6.50it/s, loss=4.922, ppl=30.31, wps=8230, ups=4.34, wpb=1898.4, bsz=127.1, num_updates=1600, lr=0.0005, gnorm=1.137, train_wall=21, gb_free=10.7, wall=371]2022-03-10 06:19:45 | WARNING | fairseq.trainer | OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 1.74 GiB (GPU 0; 11.17 GiB total capacity; 8.31 GiB already allocated; 295.81 MiB free; 10.36 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
            "2022-03-10 06:19:45 | WARNING | fairseq.trainer | |===========================================================================|\n",
            "|                  PyTorch CUDA memory summary, device ID 0                 |\n",
            "|---------------------------------------------------------------------------|\n",
            "|            CUDA OOMs: 16           |        cudaMalloc retries: 34        |\n",
            "|===========================================================================|\n",
            "|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Allocated memory      |    8510 MB |    8575 MB |    3591 GB |    3583 GB |\n",
            "|       from large pool |    8461 MB |    8526 MB |    3392 GB |    3384 GB |\n",
            "|       from small pool |      48 MB |      60 MB |     199 GB |     199 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Active memory         |    8510 MB |    8575 MB |    3591 GB |    3583 GB |\n",
            "|       from large pool |    8461 MB |    8526 MB |    3392 GB |    3384 GB |\n",
            "|       from small pool |      48 MB |      60 MB |     199 GB |     199 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| GPU reserved memory   |   10612 MB |   10688 MB |   74708 MB |   64096 MB |\n",
            "|       from large pool |   10550 MB |   10550 MB |   72944 MB |   62394 MB |\n",
            "|       from small pool |      62 MB |     138 MB |    1764 MB |    1702 MB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Non-releasable memory |    2101 MB |    2101 MB |    3850 GB |    3848 GB |\n",
            "|       from large pool |    2088 MB |    2088 MB |    3635 GB |    3633 GB |\n",
            "|       from small pool |      13 MB |      17 MB |     215 GB |     215 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Allocations           |     593    |     595    |    1886 K  |    1886 K  |\n",
            "|       from large pool |     160    |     164    |     587 K  |     587 K  |\n",
            "|       from small pool |     433    |     516    |    1298 K  |    1298 K  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Active allocs         |     593    |     595    |    1886 K  |    1886 K  |\n",
            "|       from large pool |     160    |     164    |     587 K  |     587 K  |\n",
            "|       from small pool |     433    |     516    |    1298 K  |    1298 K  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| GPU reserved segments |      77    |     115    |    1054    |     977    |\n",
            "|       from large pool |      46    |      46    |     172    |     126    |\n",
            "|       from small pool |      31    |      69    |     882    |     851    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Non-releasable allocs |      67    |      67    |     859 K  |     859 K  |\n",
            "|       from large pool |      34    |      34    |     375 K  |     375 K  |\n",
            "|       from small pool |      33    |      43    |     483 K  |     483 K  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Oversize allocations  |       0    |       0    |       0    |       0    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Oversize GPU segments |       0    |       0    |       0    |       0    |\n",
            "|===========================================================================|\n",
            "\n",
            "2022-03-10 06:19:45 | WARNING | fairseq.trainer | attempting to recover from OOM in forward/backward pass\n",
            "epoch 003:  58% 402/692 [01:27<01:14,  3.87it/s, loss=4.901, ppl=29.88, wps=8992.1, ups=4.59, wpb=1957.5, bsz=128, num_updates=1700, lr=0.0005, gnorm=1.079, train_wall=21, gb_free=10.7, wall=393]2022-03-10 06:20:04 | WARNING | fairseq.trainer | OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 1.69 GiB (GPU 0; 11.17 GiB total capacity; 7.52 GiB already allocated; 1.66 GiB free; 9.00 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
            "2022-03-10 06:20:04 | WARNING | fairseq.trainer | |===========================================================================|\n",
            "|                  PyTorch CUDA memory summary, device ID 0                 |\n",
            "|---------------------------------------------------------------------------|\n",
            "|            CUDA OOMs: 17           |        cudaMalloc retries: 36        |\n",
            "|===========================================================================|\n",
            "|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Allocated memory      |    7697 MB |    7757 MB |    3770 GB |    3762 GB |\n",
            "|       from large pool |    7649 MB |    7709 MB |    3560 GB |    3552 GB |\n",
            "|       from small pool |      48 MB |      60 MB |     210 GB |     210 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Active memory         |    7697 MB |    7757 MB |    3770 GB |    3762 GB |\n",
            "|       from large pool |    7649 MB |    7709 MB |    3560 GB |    3552 GB |\n",
            "|       from small pool |      48 MB |      60 MB |     210 GB |     210 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| GPU reserved memory   |    9212 MB |   10778 MB |   76042 MB |   66830 MB |\n",
            "|       from large pool |    9152 MB |   10644 MB |   74132 MB |   64980 MB |\n",
            "|       from small pool |      60 MB |     134 MB |    1910 MB |    1850 MB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Non-releasable memory |    1514 MB |    1514 MB |    4059 GB |    4058 GB |\n",
            "|       from large pool |    1502 MB |    1502 MB |    3832 GB |    3830 GB |\n",
            "|       from small pool |      11 MB |      17 MB |     227 GB |     227 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Allocations           |     593    |     595    |    1987 K  |    1986 K  |\n",
            "|       from large pool |     160    |     164    |     618 K  |     618 K  |\n",
            "|       from small pool |     433    |     516    |    1368 K  |    1368 K  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Active allocs         |     593    |     595    |    1987 K  |    1986 K  |\n",
            "|       from large pool |     160    |     164    |     618 K  |     618 K  |\n",
            "|       from small pool |     433    |     516    |    1368 K  |    1368 K  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| GPU reserved segments |      75    |     113    |    1128    |    1053    |\n",
            "|       from large pool |      45    |      46    |     173    |     128    |\n",
            "|       from small pool |      30    |      67    |     955    |     925    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Non-releasable allocs |      67    |      67    |     904 K  |     904 K  |\n",
            "|       from large pool |      34    |      34    |     395 K  |     395 K  |\n",
            "|       from small pool |      33    |      49    |     509 K  |     509 K  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Oversize allocations  |       0    |       0    |       0    |       0    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Oversize GPU segments |       0    |       0    |       0    |       0    |\n",
            "|===========================================================================|\n",
            "\n",
            "2022-03-10 06:20:04 | WARNING | fairseq.trainer | attempting to recover from OOM in forward/backward pass\n",
            "epoch 003:  91% 629/692 [02:17<00:14,  4.37it/s, loss=4.845, ppl=28.74, wps=8871.7, ups=4.26, wpb=2084, bsz=128, num_updates=1900, lr=0.0005, gnorm=1.068, train_wall=23, gb_free=9.7, wall=437]2022-03-10 06:20:55 | WARNING | fairseq.trainer | OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 2.50 GiB (GPU 0; 11.17 GiB total capacity; 8.45 GiB already allocated; 203.81 MiB free; 10.45 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
            "2022-03-10 06:20:55 | WARNING | fairseq.trainer | |===========================================================================|\n",
            "|                  PyTorch CUDA memory summary, device ID 0                 |\n",
            "|---------------------------------------------------------------------------|\n",
            "|            CUDA OOMs: 18           |        cudaMalloc retries: 38        |\n",
            "|===========================================================================|\n",
            "|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Allocated memory      |    8610 MB |    8651 MB |    4258 GB |    4250 GB |\n",
            "|       from large pool |    8560 MB |    8601 MB |    4023 GB |    4014 GB |\n",
            "|       from small pool |      50 MB |      60 MB |     235 GB |     235 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Active memory         |    8610 MB |    8651 MB |    4258 GB |    4250 GB |\n",
            "|       from large pool |    8560 MB |    8601 MB |    4023 GB |    4014 GB |\n",
            "|       from small pool |      50 MB |      60 MB |     235 GB |     235 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| GPU reserved memory   |   10704 MB |   10782 MB |   78892 MB |   68188 MB |\n",
            "|       from large pool |   10644 MB |   10644 MB |   76836 MB |   66192 MB |\n",
            "|       from small pool |      60 MB |     138 MB |    2056 MB |    1996 MB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Non-releasable memory |    2093 MB |    2385 MB |    4647 GB |    4645 GB |\n",
            "|       from large pool |    2083 MB |    2374 MB |    4392 GB |    4390 GB |\n",
            "|       from small pool |       9 MB |      15 MB |     254 GB |     254 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Allocations           |     593    |     595    |    2240 K  |    2240 K  |\n",
            "|       from large pool |     162    |     163    |     699 K  |     699 K  |\n",
            "|       from small pool |     431    |     516    |    1541 K  |    1540 K  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Active allocs         |     593    |     595    |    2240 K  |    2240 K  |\n",
            "|       from large pool |     162    |     163    |     699 K  |     699 K  |\n",
            "|       from small pool |     431    |     516    |    1541 K  |    1540 K  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| GPU reserved segments |      75    |     114    |    1203    |    1128    |\n",
            "|       from large pool |      45    |      45    |     175    |     130    |\n",
            "|       from small pool |      30    |      69    |    1028    |     998    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Non-releasable allocs |      69    |      69    |    1020 K  |    1020 K  |\n",
            "|       from large pool |      39    |      39    |     447 K  |     447 K  |\n",
            "|       from small pool |      30    |      44    |     573 K  |     573 K  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Oversize allocations  |       0    |       0    |       0    |       0    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Oversize GPU segments |       0    |       0    |       0    |       0    |\n",
            "|===========================================================================|\n",
            "\n",
            "2022-03-10 06:20:55 | WARNING | fairseq.trainer | attempting to recover from OOM in forward/backward pass\n",
            "epoch 003:  96% 666/692 [02:27<00:10,  2.37it/s, loss=4.771, ppl=27.31, wps=9007.8, ups=4.51, wpb=1998.1, bsz=128, num_updates=2000, lr=0.0005, gnorm=1.096, train_wall=21, gb_free=10.5, wall=460]2022-03-10 06:21:05 | WARNING | fairseq.trainer | OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 1.46 GiB (GPU 0; 11.17 GiB total capacity; 7.92 GiB already allocated; 187.81 MiB free; 10.47 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
            "2022-03-10 06:21:05 | WARNING | fairseq.trainer | |===========================================================================|\n",
            "|                  PyTorch CUDA memory summary, device ID 0                 |\n",
            "|---------------------------------------------------------------------------|\n",
            "|            CUDA OOMs: 19           |        cudaMalloc retries: 40        |\n",
            "|===========================================================================|\n",
            "|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Allocated memory      |    8106 MB |    8165 MB |    4351 GB |    4343 GB |\n",
            "|       from large pool |    8057 MB |    8116 MB |    4111 GB |    4104 GB |\n",
            "|       from small pool |      48 MB |      60 MB |     239 GB |     239 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Active memory         |    8106 MB |    8165 MB |    4351 GB |    4343 GB |\n",
            "|       from large pool |    8057 MB |    8116 MB |    4111 GB |    4104 GB |\n",
            "|       from small pool |      48 MB |      60 MB |     239 GB |     239 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| GPU reserved memory   |   10720 MB |   10768 MB |   81732 MB |   71012 MB |\n",
            "|       from large pool |   10660 MB |   10676 MB |   79572 MB |   68912 MB |\n",
            "|       from small pool |      60 MB |      92 MB |    2160 MB |    2100 MB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Non-releasable memory |    2613 MB |    2613 MB |    4762 GB |    4760 GB |\n",
            "|       from large pool |    2602 MB |    2602 MB |    4503 GB |    4501 GB |\n",
            "|       from small pool |      11 MB |      21 MB |     258 GB |     258 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Allocations           |     593    |     595    |    2281 K  |    2280 K  |\n",
            "|       from large pool |     160    |     164    |     713 K  |     712 K  |\n",
            "|       from small pool |     433    |     516    |    1568 K  |    1567 K  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Active allocs         |     593    |     595    |    2281 K  |    2280 K  |\n",
            "|       from large pool |     160    |     164    |     713 K  |     712 K  |\n",
            "|       from small pool |     433    |     516    |    1568 K  |    1567 K  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| GPU reserved segments |      74    |      91    |    1257    |    1183    |\n",
            "|       from large pool |      44    |      45    |     177    |     133    |\n",
            "|       from small pool |      30    |      46    |    1080    |    1050    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Non-releasable allocs |      67    |      67    |    1039 K  |    1039 K  |\n",
            "|       from large pool |      35    |      35    |     455 K  |     455 K  |\n",
            "|       from small pool |      32    |      41    |     583 K  |     583 K  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Oversize allocations  |       0    |       0    |       0    |       0    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Oversize GPU segments |       0    |       0    |       0    |       0    |\n",
            "|===========================================================================|\n",
            "\n",
            "2022-03-10 06:21:05 | WARNING | fairseq.trainer | attempting to recover from OOM in forward/backward pass\n",
            "epoch 003: 100% 691/692 [02:33<00:00,  5.04it/s, loss=4.771, ppl=27.31, wps=9007.8, ups=4.51, wpb=1998.1, bsz=128, num_updates=2000, lr=0.0005, gnorm=1.096, train_wall=21, gb_free=10.5, wall=460]2022-03-10 06:21:10 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "\n",
            "epoch 003 | valid on 'valid' subset:   0% 0/8 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 003 | valid on 'valid' subset:  12% 1/8 [00:00<00:00,  7.24it/s]\u001b[A\n",
            "epoch 003 | valid on 'valid' subset:  25% 2/8 [00:00<00:00,  8.15it/s]\u001b[A\n",
            "epoch 003 | valid on 'valid' subset:  50% 4/8 [00:00<00:00,  9.97it/s]\u001b[A\n",
            "epoch 003 | valid on 'valid' subset:  62% 5/8 [00:00<00:00,  9.59it/s]\u001b[A\n",
            "epoch 003 | valid on 'valid' subset:  75% 6/8 [00:00<00:00,  9.37it/s]\u001b[A\n",
            "epoch 003 | valid on 'valid' subset:  88% 7/8 [00:00<00:00,  8.91it/s]\u001b[A\n",
            "epoch 003 | valid on 'valid' subset: 100% 8/8 [00:01<00:00,  4.97it/s]\u001b[A\n",
            "                                                                      \u001b[A2022-03-10 06:21:11 | INFO | valid | epoch 003 | valid on 'valid' subset | loss 5.103 | ppl 34.36 | wps 14121.4 | wpb 1992.9 | bsz 125 | num_updates 2057 | best_loss 5.103\n",
            "2022-03-10 06:21:11 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 3 @ 2057 updates\n",
            "2022-03-10 06:21:11 | INFO | fairseq.trainer | Saving checkpoint to /content/checkpoints/checkpoint3.pt\n",
            "2022-03-10 06:21:12 | INFO | fairseq.trainer | Finished saving checkpoint to /content/checkpoints/checkpoint3.pt\n",
            "epoch 003: 100% 692/692 [02:36<00:00,  1.10s/it, loss=4.771, ppl=27.31, wps=9007.8, ups=4.51, wpb=1998.1, bsz=128, num_updates=2000, lr=0.0005, gnorm=1.096, train_wall=21, gb_free=10.5, wall=460]2022-03-10 06:21:13 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/checkpoint3.pt (epoch 3 @ 2057 updates, score 5.103) (writing took 1.8873957770000516 seconds)\n",
            "2022-03-10 06:21:13 | INFO | fairseq_cli.train | end of epoch 3 (average epoch stats below)\n",
            "2022-03-10 06:21:13 | INFO | train | epoch 003 | loss 4.863 | ppl 29.1 | wps 8524 | ups 4.38 | wpb 1946.5 | bsz 127.9 | num_updates 2057 | lr 0.0005 | gnorm 1.097 | train_wall 146 | gb_free 10.6 | wall 477\n",
            "epoch 004:   0% 0/692 [00:00<?, ?it/s]2022-03-10 06:21:13 | INFO | fairseq.data.iterators | grouped total_num_itrs = 692\n",
            "2022-03-10 06:21:13 | INFO | fairseq.trainer | begin training epoch 4\n",
            "2022-03-10 06:21:13 | INFO | fairseq_cli.train | Start iterating over samples\n",
            "epoch 004:  14% 99/692 [00:19<01:44,  5.68it/s, loss=4.477, ppl=22.27, wps=7511.1, ups=4, wpb=1876.7, bsz=128, num_updates=2100, lr=0.0005, gnorm=1.111, train_wall=21, gb_free=10.4, wall=485]2022-03-10 06:21:33 | WARNING | fairseq.trainer | OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 1.46 GiB (GPU 0; 11.17 GiB total capacity; 7.92 GiB already allocated; 203.81 MiB free; 10.45 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
            "2022-03-10 06:21:33 | WARNING | fairseq.trainer | |===========================================================================|\n",
            "|                  PyTorch CUDA memory summary, device ID 0                 |\n",
            "|---------------------------------------------------------------------------|\n",
            "|            CUDA OOMs: 20           |        cudaMalloc retries: 41        |\n",
            "|===========================================================================|\n",
            "|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Allocated memory      |    8105 MB |    8164 MB |    4602 GB |    4594 GB |\n",
            "|       from large pool |    8056 MB |    8115 MB |    4347 GB |    4339 GB |\n",
            "|       from small pool |      48 MB |      60 MB |     255 GB |     255 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Active memory         |    8105 MB |    8164 MB |    4602 GB |    4594 GB |\n",
            "|       from large pool |    8056 MB |    8115 MB |    4347 GB |    4339 GB |\n",
            "|       from small pool |      48 MB |      60 MB |     255 GB |     255 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| GPU reserved memory   |   10704 MB |   10798 MB |   81810 MB |   71106 MB |\n",
            "|       from large pool |   10644 MB |   10660 MB |   79572 MB |   68928 MB |\n",
            "|       from small pool |      60 MB |     138 MB |    2238 MB |    2178 MB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Non-releasable memory |    2598 MB |    2598 MB |    5063 GB |    5060 GB |\n",
            "|       from large pool |    2587 MB |    2587 MB |    4787 GB |    4784 GB |\n",
            "|       from small pool |      11 MB |      21 MB |     276 GB |     276 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Allocations           |     593    |     595    |    2422 K  |    2421 K  |\n",
            "|       from large pool |     160    |     164    |     755 K  |     755 K  |\n",
            "|       from small pool |     433    |     516    |    1666 K  |    1665 K  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Active allocs         |     593    |     595    |    2422 K  |    2421 K  |\n",
            "|       from large pool |     160    |     164    |     755 K  |     755 K  |\n",
            "|       from small pool |     433    |     516    |    1666 K  |    1665 K  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| GPU reserved segments |      73    |     113    |    1296    |    1223    |\n",
            "|       from large pool |      43    |      44    |     177    |     134    |\n",
            "|       from small pool |      30    |      69    |    1119    |    1089    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Non-releasable allocs |      68    |      69    |    1104 K  |    1104 K  |\n",
            "|       from large pool |      36    |      36    |     483 K  |     483 K  |\n",
            "|       from small pool |      32    |      43    |     621 K  |     621 K  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Oversize allocations  |       0    |       0    |       0    |       0    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Oversize GPU segments |       0    |       0    |       0    |       0    |\n",
            "|===========================================================================|\n",
            "\n",
            "2022-03-10 06:21:33 | WARNING | fairseq.trainer | attempting to recover from OOM in forward/backward pass\n",
            "epoch 004:  19% 130/692 [00:26<01:45,  5.35it/s, loss=4.477, ppl=22.27, wps=7511.1, ups=4, wpb=1876.7, bsz=128, num_updates=2100, lr=0.0005, gnorm=1.111, train_wall=21, gb_free=10.4, wall=485]2022-03-10 06:21:40 | WARNING | fairseq.trainer | OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 2.01 GiB (GPU 0; 11.17 GiB total capacity; 9.79 GiB already allocated; 203.81 MiB free; 10.45 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
            "2022-03-10 06:21:40 | WARNING | fairseq.trainer | |===========================================================================|\n",
            "|                  PyTorch CUDA memory summary, device ID 0                 |\n",
            "|---------------------------------------------------------------------------|\n",
            "|            CUDA OOMs: 21           |        cudaMalloc retries: 42        |\n",
            "|===========================================================================|\n",
            "|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Allocated memory      |   10024 MB |   10099 MB |    4668 GB |    4658 GB |\n",
            "|       from large pool |    9975 MB |   10049 MB |    4409 GB |    4400 GB |\n",
            "|       from small pool |      49 MB |      60 MB |     258 GB |     258 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Active memory         |   10024 MB |   10099 MB |    4668 GB |    4658 GB |\n",
            "|       from large pool |    9975 MB |   10049 MB |    4409 GB |    4400 GB |\n",
            "|       from small pool |      49 MB |      60 MB |     258 GB |     258 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| GPU reserved memory   |   10704 MB |   10762 MB |   81868 MB |   71164 MB |\n",
            "|       from large pool |   10644 MB |   10644 MB |   79572 MB |   68928 MB |\n",
            "|       from small pool |      60 MB |     118 MB |    2296 MB |    2236 MB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Non-releasable memory |  695684 KB |    1569 MB |    5139 GB |    5139 GB |\n",
            "|       from large pool |  684744 KB |    1556 MB |    4860 GB |    4859 GB |\n",
            "|       from small pool |   10940 KB |      23 MB |     279 GB |     279 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Allocations           |     593    |     595    |    2456 K  |    2455 K  |\n",
            "|       from large pool |     160    |     164    |     766 K  |     766 K  |\n",
            "|       from small pool |     433    |     516    |    1689 K  |    1688 K  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Active allocs         |     593    |     595    |    2456 K  |    2455 K  |\n",
            "|       from large pool |     160    |     164    |     766 K  |     766 K  |\n",
            "|       from small pool |     433    |     516    |    1689 K  |    1688 K  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| GPU reserved segments |      73    |     102    |    1325    |    1252    |\n",
            "|       from large pool |      43    |      43    |     177    |     134    |\n",
            "|       from small pool |      30    |      59    |    1148    |    1118    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Non-releasable allocs |      68    |      69    |    1120 K  |    1120 K  |\n",
            "|       from large pool |      36    |      36    |     490 K  |     490 K  |\n",
            "|       from small pool |      32    |      55    |     630 K  |     629 K  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Oversize allocations  |       0    |       0    |       0    |       0    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Oversize GPU segments |       0    |       0    |       0    |       0    |\n",
            "|===========================================================================|\n",
            "\n",
            "2022-03-10 06:21:40 | WARNING | fairseq.trainer | attempting to recover from OOM in forward/backward pass\n",
            "epoch 004:  29% 202/692 [00:42<02:09,  3.79it/s, loss=4.239, ppl=18.88, wps=8409.7, ups=4.7, wpb=1790.1, bsz=128, num_updates=2200, lr=0.0005, gnorm=1.14, train_wall=19, gb_free=8.8, wall=506]2022-03-10 06:21:56 | WARNING | fairseq.trainer | OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 2.11 GiB (GPU 0; 11.17 GiB total capacity; 9.88 GiB already allocated; 101.81 MiB free; 10.55 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
            "2022-03-10 06:21:56 | WARNING | fairseq.trainer | |===========================================================================|\n",
            "|                  PyTorch CUDA memory summary, device ID 0                 |\n",
            "|---------------------------------------------------------------------------|\n",
            "|            CUDA OOMs: 22           |        cudaMalloc retries: 44        |\n",
            "|===========================================================================|\n",
            "|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Allocated memory      |   10120 MB |   10195 MB |    4826 GB |    4817 GB |\n",
            "|       from large pool |   10071 MB |   10146 MB |    4560 GB |    4551 GB |\n",
            "|       from small pool |      49 MB |      60 MB |     266 GB |     265 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Active memory         |   10120 MB |   10195 MB |    4826 GB |    4817 GB |\n",
            "|       from large pool |   10071 MB |   10146 MB |    4560 GB |    4551 GB |\n",
            "|       from small pool |      49 MB |      60 MB |     266 GB |     265 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| GPU reserved memory   |   10806 MB |   10806 MB |   84104 MB |   73298 MB |\n",
            "|       from large pool |   10746 MB |   10746 MB |   81736 MB |   70990 MB |\n",
            "|       from small pool |      60 MB |     132 MB |    2368 MB |    2308 MB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Non-releasable memory |  702069 KB |    1659 MB |    5327 GB |    5327 GB |\n",
            "|       from large pool |  691078 KB |    1647 MB |    5039 GB |    5039 GB |\n",
            "|       from small pool |   10991 KB |      15 MB |     287 GB |     287 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Allocations           |     593    |     595    |    2535 K  |    2535 K  |\n",
            "|       from large pool |     160    |     164    |     793 K  |     793 K  |\n",
            "|       from small pool |     433    |     516    |    1742 K  |    1742 K  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Active allocs         |     593    |     595    |    2535 K  |    2535 K  |\n",
            "|       from large pool |     160    |     164    |     793 K  |     793 K  |\n",
            "|       from small pool |     433    |     516    |    1742 K  |    1742 K  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| GPU reserved segments |      73    |     109    |    1362    |    1289    |\n",
            "|       from large pool |      43    |      43    |     178    |     135    |\n",
            "|       from small pool |      30    |      66    |    1184    |    1154    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Non-releasable allocs |      64    |      65    |    1156 K  |    1156 K  |\n",
            "|       from large pool |      34    |      34    |     507 K  |     507 K  |\n",
            "|       from small pool |      30    |      41    |     649 K  |     649 K  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Oversize allocations  |       0    |       0    |       0    |       0    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Oversize GPU segments |       0    |       0    |       0    |       0    |\n",
            "|===========================================================================|\n",
            "\n",
            "2022-03-10 06:21:56 | WARNING | fairseq.trainer | attempting to recover from OOM in forward/backward pass\n",
            "epoch 004:  33% 230/692 [00:48<01:35,  4.86it/s, loss=4.239, ppl=18.88, wps=8409.7, ups=4.7, wpb=1790.1, bsz=128, num_updates=2200, lr=0.0005, gnorm=1.14, train_wall=19, gb_free=8.8, wall=506]2022-03-10 06:22:03 | WARNING | fairseq.trainer | OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 1.69 GiB (GPU 0; 11.17 GiB total capacity; 7.52 GiB already allocated; 1.44 GiB free; 9.22 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
            "2022-03-10 06:22:03 | WARNING | fairseq.trainer | |===========================================================================|\n",
            "|                  PyTorch CUDA memory summary, device ID 0                 |\n",
            "|---------------------------------------------------------------------------|\n",
            "|            CUDA OOMs: 23           |        cudaMalloc retries: 45        |\n",
            "|===========================================================================|\n",
            "|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Allocated memory      |    7696 MB |    7756 MB |    4888 GB |    4880 GB |\n",
            "|       from large pool |    7648 MB |    7708 MB |    4619 GB |    4612 GB |\n",
            "|       from small pool |      48 MB |      60 MB |     268 GB |     268 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Active memory         |    7696 MB |    7756 MB |    4888 GB |    4880 GB |\n",
            "|       from large pool |    7648 MB |    7708 MB |    4619 GB |    4612 GB |\n",
            "|       from small pool |      48 MB |      60 MB |     268 GB |     268 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| GPU reserved memory   |    9438 MB |   10874 MB |   84172 MB |   74734 MB |\n",
            "|       from large pool |    9378 MB |   10746 MB |   81736 MB |   72358 MB |\n",
            "|       from small pool |      60 MB |     128 MB |    2436 MB |    2376 MB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Non-releasable memory |    1741 MB |    1741 MB |    5403 GB |    5401 GB |\n",
            "|       from large pool |    1729 MB |    1729 MB |    5112 GB |    5110 GB |\n",
            "|       from small pool |      11 MB |      31 MB |     290 GB |     290 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Allocations           |     593    |     595    |    2567 K  |    2566 K  |\n",
            "|       from large pool |     160    |     164    |     803 K  |     803 K  |\n",
            "|       from small pool |     433    |     516    |    1763 K  |    1763 K  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Active allocs         |     593    |     595    |    2567 K  |    2566 K  |\n",
            "|       from large pool |     160    |     164    |     803 K  |     803 K  |\n",
            "|       from small pool |     433    |     516    |    1763 K  |    1763 K  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| GPU reserved segments |      72    |     107    |    1396    |    1324    |\n",
            "|       from large pool |      42    |      43    |     178    |     136    |\n",
            "|       from small pool |      30    |      64    |    1218    |    1188    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Non-releasable allocs |      62    |      62    |    1171 K  |    1170 K  |\n",
            "|       from large pool |      32    |      32    |     513 K  |     513 K  |\n",
            "|       from small pool |      30    |      53    |     657 K  |     657 K  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Oversize allocations  |       0    |       0    |       0    |       0    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Oversize GPU segments |       0    |       0    |       0    |       0    |\n",
            "|===========================================================================|\n",
            "\n",
            "2022-03-10 06:22:03 | WARNING | fairseq.trainer | attempting to recover from OOM in forward/backward pass\n",
            "epoch 004:  37% 254/692 [00:54<01:24,  5.21it/s, loss=4.223, ppl=18.68, wps=8420.6, ups=4.23, wpb=1988.8, bsz=128, num_updates=2300, lr=0.0005, gnorm=1.11, train_wall=21, gb_free=10.7, wall=529]2022-03-10 06:22:08 | WARNING | fairseq.trainer | OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 2.50 GiB (GPU 0; 11.17 GiB total capacity; 8.45 GiB already allocated; 1.44 GiB free; 9.22 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
            "2022-03-10 06:22:08 | WARNING | fairseq.trainer | |===========================================================================|\n",
            "|                  PyTorch CUDA memory summary, device ID 0                 |\n",
            "|---------------------------------------------------------------------------|\n",
            "|            CUDA OOMs: 24           |        cudaMalloc retries: 46        |\n",
            "|===========================================================================|\n",
            "|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Allocated memory      |    8610 MB |    8651 MB |    4946 GB |    4937 GB |\n",
            "|       from large pool |    8560 MB |    8601 MB |    4674 GB |    4666 GB |\n",
            "|       from small pool |      50 MB |      60 MB |     271 GB |     271 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Active memory         |    8610 MB |    8651 MB |    4946 GB |    4937 GB |\n",
            "|       from large pool |    8560 MB |    8601 MB |    4674 GB |    4666 GB |\n",
            "|       from small pool |      50 MB |      60 MB |     271 GB |     271 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| GPU reserved memory   |    9438 MB |    9516 MB |   84250 MB |   74812 MB |\n",
            "|       from large pool |    9378 MB |    9378 MB |   81736 MB |   72358 MB |\n",
            "|       from small pool |      60 MB |     138 MB |    2514 MB |    2454 MB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Non-releasable memory |     827 MB |    2267 MB |    5471 GB |    5470 GB |\n",
            "|       from large pool |     817 MB |    2256 MB |    5177 GB |    5176 GB |\n",
            "|       from small pool |       9 MB |      16 MB |     293 GB |     293 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Allocations           |     593    |     595    |    2592 K  |    2591 K  |\n",
            "|       from large pool |     162    |     163    |     811 K  |     811 K  |\n",
            "|       from small pool |     431    |     516    |    1780 K  |    1780 K  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Active allocs         |     593    |     595    |    2592 K  |    2591 K  |\n",
            "|       from large pool |     162    |     163    |     811 K  |     811 K  |\n",
            "|       from small pool |     431    |     516    |    1780 K  |    1780 K  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| GPU reserved segments |      72    |     111    |    1435    |    1363    |\n",
            "|       from large pool |      42    |      42    |     178    |     136    |\n",
            "|       from small pool |      30    |      69    |    1257    |    1227    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Non-releasable allocs |      67    |      67    |    1182 K  |    1182 K  |\n",
            "|       from large pool |      32    |      32    |     518 K  |     518 K  |\n",
            "|       from small pool |      35    |      42    |     663 K  |     663 K  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Oversize allocations  |       0    |       0    |       0    |       0    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Oversize GPU segments |       0    |       0    |       0    |       0    |\n",
            "|===========================================================================|\n",
            "\n",
            "2022-03-10 06:22:08 | WARNING | fairseq.trainer | attempting to recover from OOM in forward/backward pass\n",
            "epoch 004:  44% 306/692 [01:08<01:11,  5.41it/s, loss=4.223, ppl=18.68, wps=8420.6, ups=4.23, wpb=1988.8, bsz=128, num_updates=2300, lr=0.0005, gnorm=1.11, train_wall=21, gb_free=10.7, wall=529]2022-03-10 06:22:22 | WARNING | fairseq.trainer | OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 1.74 GiB (GPU 0; 11.17 GiB total capacity; 8.31 GiB already allocated; 1.45 GiB free; 9.20 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
            "2022-03-10 06:22:22 | WARNING | fairseq.trainer | |===========================================================================|\n",
            "|                  PyTorch CUDA memory summary, device ID 0                 |\n",
            "|---------------------------------------------------------------------------|\n",
            "|            CUDA OOMs: 25           |        cudaMalloc retries: 47        |\n",
            "|===========================================================================|\n",
            "|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Allocated memory      |    8511 MB |    8575 MB |    5085 GB |    5076 GB |\n",
            "|       from large pool |    8462 MB |    8526 MB |    4808 GB |    4799 GB |\n",
            "|       from small pool |      48 MB |      60 MB |     276 GB |     276 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Active memory         |    8511 MB |    8575 MB |    5085 GB |    5076 GB |\n",
            "|       from large pool |    8462 MB |    8526 MB |    4808 GB |    4799 GB |\n",
            "|       from small pool |      48 MB |      60 MB |     276 GB |     276 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| GPU reserved memory   |    9422 MB |   10868 MB |   85680 MB |   76258 MB |\n",
            "|       from large pool |    9362 MB |   10730 MB |   83088 MB |   73726 MB |\n",
            "|       from small pool |      60 MB |     138 MB |    2592 MB |    2532 MB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Non-releasable memory |     910 MB |    2221 MB |    5642 GB |    5641 GB |\n",
            "|       from large pool |     899 MB |    2210 MB |    5343 GB |    5342 GB |\n",
            "|       from small pool |      11 MB |      20 MB |     299 GB |     299 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Allocations           |     593    |     595    |    2650 K  |    2649 K  |\n",
            "|       from large pool |     160    |     164    |     831 K  |     830 K  |\n",
            "|       from small pool |     433    |     516    |    1818 K  |    1818 K  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Active allocs         |     593    |     595    |    2650 K  |    2649 K  |\n",
            "|       from large pool |     160    |     164    |     831 K  |     830 K  |\n",
            "|       from small pool |     433    |     516    |    1818 K  |    1818 K  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| GPU reserved segments |      72    |     112    |    1475    |    1403    |\n",
            "|       from large pool |      42    |      43    |     179    |     137    |\n",
            "|       from small pool |      30    |      69    |    1296    |    1266    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Non-releasable allocs |      61    |      62    |    1208 K  |    1208 K  |\n",
            "|       from large pool |      27    |      27    |     531 K  |     531 K  |\n",
            "|       from small pool |      34    |      41    |     677 K  |     677 K  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Oversize allocations  |       0    |       0    |       0    |       0    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Oversize GPU segments |       0    |       0    |       0    |       0    |\n",
            "|===========================================================================|\n",
            "\n",
            "2022-03-10 06:22:22 | WARNING | fairseq.trainer | attempting to recover from OOM in forward/backward pass\n",
            "epoch 004:  63% 434/692 [01:38<00:48,  5.36it/s, loss=4.356, ppl=20.48, wps=8331.5, ups=3.84, wpb=2170.4, bsz=128, num_updates=2400, lr=0.0005, gnorm=1.085, train_wall=24, gb_free=10.7, wall=556]2022-03-10 06:22:52 | WARNING | fairseq.trainer | OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 1.46 GiB (GPU 0; 11.17 GiB total capacity; 7.60 GiB already allocated; 1.21 GiB free; 9.44 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
            "2022-03-10 06:22:52 | WARNING | fairseq.trainer | |===========================================================================|\n",
            "|                  PyTorch CUDA memory summary, device ID 0                 |\n",
            "|---------------------------------------------------------------------------|\n",
            "|            CUDA OOMs: 26           |        cudaMalloc retries: 50        |\n",
            "|===========================================================================|\n",
            "|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Allocated memory      |    6295 MB |    7787 MB |    5377 GB |    5371 GB |\n",
            "|       from large pool |    6238 MB |    7730 MB |    5086 GB |    5080 GB |\n",
            "|       from small pool |      56 MB |      60 MB |     291 GB |     291 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Active memory         |    6295 MB |    7787 MB |    5377 GB |    5371 GB |\n",
            "|       from large pool |    6238 MB |    7730 MB |    5086 GB |    5080 GB |\n",
            "|       from small pool |      56 MB |      60 MB |     291 GB |     291 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| GPU reserved memory   |    9668 MB |   10884 MB |   90788 MB |   81120 MB |\n",
            "|       from large pool |    9604 MB |   10746 MB |   88042 MB |   78438 MB |\n",
            "|       from small pool |      64 MB |     138 MB |    2746 MB |    2682 MB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Non-releasable memory |    1880 MB |    1880 MB |    5981 GB |    5979 GB |\n",
            "|       from large pool |    1873 MB |    1873 MB |    5666 GB |    5664 GB |\n",
            "|       from small pool |       7 MB |      19 MB |     315 GB |     315 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Allocations           |     592    |     595    |    2792 K  |    2792 K  |\n",
            "|       from large pool |     150    |     154    |     876 K  |     876 K  |\n",
            "|       from small pool |     442    |     516    |    1916 K  |    1915 K  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Active allocs         |     592    |     595    |    2792 K  |    2792 K  |\n",
            "|       from large pool |     150    |     154    |     876 K  |     876 K  |\n",
            "|       from small pool |     442    |     516    |    1916 K  |    1915 K  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| GPU reserved segments |      74    |     112    |    1556    |    1482    |\n",
            "|       from large pool |      42    |      43    |     183    |     141    |\n",
            "|       from small pool |      32    |      69    |    1373    |    1341    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Non-releasable allocs |      58    |      58    |    1273 K  |    1273 K  |\n",
            "|       from large pool |      25    |      25    |     559 K  |     559 K  |\n",
            "|       from small pool |      33    |      40    |     713 K  |     713 K  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Oversize allocations  |       0    |       0    |       0    |       0    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Oversize GPU segments |       0    |       0    |       0    |       0    |\n",
            "|===========================================================================|\n",
            "\n",
            "2022-03-10 06:22:52 | WARNING | fairseq.trainer | attempting to recover from OOM in forward/backward pass\n",
            "epoch 004:  86% 597/692 [02:12<00:14,  6.63it/s, loss=4.373, ppl=20.72, wps=9509.2, ups=4.69, wpb=2027.6, bsz=127.1, num_updates=2600, lr=0.0005, gnorm=1.183, train_wall=21, gb_free=10.7, wall=600]2022-03-10 06:23:26 | WARNING | fairseq.trainer | OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 1.34 GiB (GPU 0; 11.17 GiB total capacity; 7.45 GiB already allocated; 1.33 GiB free; 9.32 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
            "2022-03-10 06:23:26 | WARNING | fairseq.trainer | |===========================================================================|\n",
            "|                  PyTorch CUDA memory summary, device ID 0                 |\n",
            "|---------------------------------------------------------------------------|\n",
            "|            CUDA OOMs: 27           |        cudaMalloc retries: 51        |\n",
            "|===========================================================================|\n",
            "|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Allocated memory      |    6262 MB |    7629 MB |    5690 GB |    5684 GB |\n",
            "|       from large pool |    6214 MB |    7581 MB |    5379 GB |    5373 GB |\n",
            "|       from small pool |      48 MB |      60 MB |     311 GB |     310 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Active memory         |    6262 MB |    7629 MB |    5690 GB |    5684 GB |\n",
            "|       from large pool |    6214 MB |    7581 MB |    5379 GB |    5373 GB |\n",
            "|       from small pool |      48 MB |      60 MB |     311 GB |     310 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| GPU reserved memory   |    9542 MB |    9614 MB |   92226 MB |   82684 MB |\n",
            "|       from large pool |    9480 MB |    9480 MB |   89410 MB |   79930 MB |\n",
            "|       from small pool |      62 MB |     134 MB |    2816 MB |    2754 MB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Non-releasable memory |    1911 MB |    1912 MB |    6370 GB |    6368 GB |\n",
            "|       from large pool |    1897 MB |    1898 MB |    6033 GB |    6031 GB |\n",
            "|       from small pool |      13 MB |      17 MB |     336 GB |     336 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Allocations           |     592    |     595    |    2973 K  |    2972 K  |\n",
            "|       from large pool |     160    |     164    |     931 K  |     931 K  |\n",
            "|       from small pool |     432    |     516    |    2042 K  |    2041 K  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Active allocs         |     592    |     595    |    2973 K  |    2972 K  |\n",
            "|       from large pool |     160    |     164    |     931 K  |     931 K  |\n",
            "|       from small pool |     432    |     516    |    2042 K  |    2041 K  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| GPU reserved segments |      73    |     109    |    1592    |    1519    |\n",
            "|       from large pool |      42    |      42    |     184    |     142    |\n",
            "|       from small pool |      31    |      67    |    1408    |    1377    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Non-releasable allocs |      61    |      62    |    1354 K  |    1353 K  |\n",
            "|       from large pool |      25    |      26    |     594 K  |     594 K  |\n",
            "|       from small pool |      36    |      47    |     759 K  |     759 K  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Oversize allocations  |       0    |       0    |       0    |       0    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Oversize GPU segments |       0    |       0    |       0    |       0    |\n",
            "|===========================================================================|\n",
            "\n",
            "2022-03-10 06:23:26 | WARNING | fairseq.trainer | attempting to recover from OOM in forward/backward pass\n",
            "epoch 004: 100% 691/692 [02:31<00:00,  5.78it/s, loss=4.3, ppl=19.7, wps=9114.9, ups=4.83, wpb=1886.8, bsz=128, num_updates=2700, lr=0.0005, gnorm=1.207, train_wall=20, gb_free=10.4, wall=621]2022-03-10 06:23:45 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "\n",
            "epoch 004 | valid on 'valid' subset:   0% 0/8 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 004 | valid on 'valid' subset:  12% 1/8 [00:00<00:01,  6.59it/s]\u001b[A\n",
            "epoch 004 | valid on 'valid' subset:  25% 2/8 [00:00<00:00,  8.12it/s]\u001b[A\n",
            "epoch 004 | valid on 'valid' subset:  50% 4/8 [00:00<00:00,  9.93it/s]\u001b[A\n",
            "epoch 004 | valid on 'valid' subset:  62% 5/8 [00:00<00:00,  9.58it/s]\u001b[A\n",
            "epoch 004 | valid on 'valid' subset:  75% 6/8 [00:00<00:00,  9.37it/s]\u001b[A\n",
            "epoch 004 | valid on 'valid' subset:  88% 7/8 [00:00<00:00,  8.93it/s]\u001b[A\n",
            "epoch 004 | valid on 'valid' subset: 100% 8/8 [00:01<00:00,  4.97it/s]\u001b[A\n",
            "                                                                      \u001b[A2022-03-10 06:23:46 | INFO | valid | epoch 004 | valid on 'valid' subset | loss 4.996 | ppl 31.92 | wps 14174.7 | wpb 1992.9 | bsz 125 | num_updates 2741 | best_loss 4.996\n",
            "2022-03-10 06:23:46 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 4 @ 2741 updates\n",
            "2022-03-10 06:23:46 | INFO | fairseq.trainer | Saving checkpoint to /content/checkpoints/checkpoint4.pt\n",
            "2022-03-10 06:23:46 | INFO | fairseq.trainer | Finished saving checkpoint to /content/checkpoints/checkpoint4.pt\n",
            "2022-03-10 06:23:48 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/checkpoint4.pt (epoch 4 @ 2741 updates, score 4.996) (writing took 2.333762247000095 seconds)\n",
            "2022-03-10 06:23:48 | INFO | fairseq_cli.train | end of epoch 4 (average epoch stats below)\n",
            "2022-03-10 06:23:48 | INFO | train | epoch 004 | loss 4.294 | ppl 19.62 | wps 8515.1 | ups 4.4 | wpb 1933.9 | bsz 127.9 | num_updates 2741 | lr 0.0005 | gnorm 1.154 | train_wall 143 | gb_free 10.6 | wall 632\n",
            "2022-03-10 06:23:48 | INFO | fairseq.data.iterators | grouped total_num_itrs = 692\n",
            "epoch 005:   0% 0/692 [00:00<?, ?it/s]2022-03-10 06:23:48 | INFO | fairseq.trainer | begin training epoch 5\n",
            "2022-03-10 06:23:48 | INFO | fairseq_cli.train | Start iterating over samples\n",
            "epoch 005:  15% 104/692 [00:19<01:49,  5.35it/s, loss=3.899, ppl=14.92, wps=8041.6, ups=4.44, wpb=1813, bsz=128, num_updates=2800, lr=0.0005, gnorm=1.163, train_wall=19, gb_free=10.2, wall=643]2022-03-10 06:24:09 | WARNING | fairseq.trainer | OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 1.74 GiB (GPU 0; 11.17 GiB total capacity; 8.31 GiB already allocated; 357.81 MiB free; 10.30 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
            "2022-03-10 06:24:09 | WARNING | fairseq.trainer | |===========================================================================|\n",
            "|                  PyTorch CUDA memory summary, device ID 0                 |\n",
            "|---------------------------------------------------------------------------|\n",
            "|            CUDA OOMs: 28           |        cudaMalloc retries: 52        |\n",
            "|===========================================================================|\n",
            "|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Allocated memory      |    8511 MB |    8575 MB |    6058 GB |    6050 GB |\n",
            "|       from large pool |    8462 MB |    8526 MB |    5724 GB |    5715 GB |\n",
            "|       from small pool |      48 MB |      60 MB |     334 GB |     334 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Active memory         |    8511 MB |    8575 MB |    6058 GB |    6050 GB |\n",
            "|       from large pool |    8462 MB |    8526 MB |    5724 GB |    5715 GB |\n",
            "|       from small pool |      48 MB |      60 MB |     334 GB |     334 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| GPU reserved memory   |   10550 MB |   10626 MB |   94678 MB |   84128 MB |\n",
            "|       from large pool |   10488 MB |   10488 MB |   91786 MB |   81298 MB |\n",
            "|       from small pool |      62 MB |     138 MB |    2892 MB |    2830 MB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Non-releasable memory |    2038 MB |    2038 MB |    6822 GB |    6820 GB |\n",
            "|       from large pool |    2025 MB |    2025 MB |    6460 GB |    6458 GB |\n",
            "|       from small pool |      13 MB |      17 MB |     362 GB |     362 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Allocations           |     593    |     595    |    3196 K  |    3196 K  |\n",
            "|       from large pool |     160    |     164    |    1000 K  |    1000 K  |\n",
            "|       from small pool |     433    |     516    |    2195 K  |    2195 K  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Active allocs         |     593    |     595    |    3196 K  |    3196 K  |\n",
            "|       from large pool |     160    |     164    |    1000 K  |    1000 K  |\n",
            "|       from small pool |     433    |     516    |    2195 K  |    2195 K  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| GPU reserved segments |      74    |     112    |    1632    |    1558    |\n",
            "|       from large pool |      43    |      43    |     186    |     143    |\n",
            "|       from small pool |      31    |      69    |    1446    |    1415    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Non-releasable allocs |      62    |      62    |    1456 K  |    1456 K  |\n",
            "|       from large pool |      29    |      29    |     638 K  |     638 K  |\n",
            "|       from small pool |      33    |      39    |     817 K  |     817 K  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Oversize allocations  |       0    |       0    |       0    |       0    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Oversize GPU segments |       0    |       0    |       0    |       0    |\n",
            "|===========================================================================|\n",
            "\n",
            "2022-03-10 06:24:09 | WARNING | fairseq.trainer | attempting to recover from OOM in forward/backward pass\n",
            "epoch 005:  17% 120/692 [00:22<01:30,  6.33it/s, loss=3.899, ppl=14.92, wps=8041.6, ups=4.44, wpb=1813, bsz=128, num_updates=2800, lr=0.0005, gnorm=1.163, train_wall=19, gb_free=10.2, wall=643]2022-03-10 06:24:12 | WARNING | fairseq.trainer | OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 2.01 GiB (GPU 0; 11.17 GiB total capacity; 9.79 GiB already allocated; 359.81 MiB free; 10.30 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
            "2022-03-10 06:24:12 | WARNING | fairseq.trainer | |===========================================================================|\n",
            "|                  PyTorch CUDA memory summary, device ID 0                 |\n",
            "|---------------------------------------------------------------------------|\n",
            "|            CUDA OOMs: 29           |        cudaMalloc retries: 53        |\n",
            "|===========================================================================|\n",
            "|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Allocated memory      |   10024 MB |   10098 MB |    6092 GB |    6082 GB |\n",
            "|       from large pool |    9974 MB |   10049 MB |    5755 GB |    5746 GB |\n",
            "|       from small pool |      49 MB |      60 MB |     336 GB |     336 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Active memory         |   10024 MB |   10098 MB |    6092 GB |    6082 GB |\n",
            "|       from large pool |    9974 MB |   10049 MB |    5755 GB |    5746 GB |\n",
            "|       from small pool |      49 MB |      60 MB |     336 GB |     336 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| GPU reserved memory   |   10548 MB |   10616 MB |   94744 MB |   84196 MB |\n",
            "|       from large pool |   10488 MB |   10488 MB |   91786 MB |   81298 MB |\n",
            "|       from small pool |      60 MB |     128 MB |    2958 MB |    2898 MB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Non-releasable memory |  536484 KB |    1671 MB |    6859 GB |    6859 GB |\n",
            "|       from large pool |  525544 KB |    1658 MB |    6495 GB |    6495 GB |\n",
            "|       from small pool |   10940 KB |      42 MB |     364 GB |     363 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Allocations           |     593    |     595    |    3213 K  |    3213 K  |\n",
            "|       from large pool |     160    |     164    |    1006 K  |    1006 K  |\n",
            "|       from small pool |     433    |     516    |    2207 K  |    2206 K  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Active allocs         |     593    |     595    |    3213 K  |    3213 K  |\n",
            "|       from large pool |     160    |     164    |    1006 K  |    1006 K  |\n",
            "|       from small pool |     433    |     516    |    2207 K  |    2206 K  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| GPU reserved segments |      73    |     107    |    1665    |    1592    |\n",
            "|       from large pool |      43    |      43    |     186    |     143    |\n",
            "|       from small pool |      30    |      64    |    1479    |    1449    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Non-releasable allocs |      67    |      70    |    1464 K  |    1464 K  |\n",
            "|       from large pool |      35    |      35    |     642 K  |     642 K  |\n",
            "|       from small pool |      32    |      66    |     821 K  |     821 K  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Oversize allocations  |       0    |       0    |       0    |       0    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Oversize GPU segments |       0    |       0    |       0    |       0    |\n",
            "|===========================================================================|\n",
            "\n",
            "2022-03-10 06:24:12 | WARNING | fairseq.trainer | attempting to recover from OOM in forward/backward pass\n",
            "epoch 005:  28% 196/692 [00:38<01:20,  6.19it/s, loss=3.741, ppl=13.37, wps=8598.3, ups=4.75, wpb=1811.6, bsz=128, num_updates=2900, lr=0.0005, gnorm=1.185, train_wall=19, gb_free=10.6, wall=664]2022-03-10 06:24:27 | WARNING | fairseq.trainer | OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 1.46 GiB (GPU 0; 11.17 GiB total capacity; 7.60 GiB already allocated; 1.21 GiB free; 9.44 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
            "2022-03-10 06:24:27 | WARNING | fairseq.trainer | |===========================================================================|\n",
            "|                  PyTorch CUDA memory summary, device ID 0                 |\n",
            "|---------------------------------------------------------------------------|\n",
            "|            CUDA OOMs: 30           |        cudaMalloc retries: 55        |\n",
            "|===========================================================================|\n",
            "|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Allocated memory      |    6295 MB |    7787 MB |    6235 GB |    6229 GB |\n",
            "|       from large pool |    6238 MB |    7730 MB |    5890 GB |    5884 GB |\n",
            "|       from small pool |      56 MB |      60 MB |     345 GB |     345 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Active memory         |    6295 MB |    7787 MB |    6235 GB |    6229 GB |\n",
            "|       from large pool |    6238 MB |    7730 MB |    5890 GB |    5884 GB |\n",
            "|       from small pool |      56 MB |      60 MB |     345 GB |     345 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| GPU reserved memory   |    9668 MB |   10626 MB |   96314 MB |   86646 MB |\n",
            "|       from large pool |    9604 MB |   10488 MB |   93278 MB |   83674 MB |\n",
            "|       from small pool |      64 MB |     138 MB |    3036 MB |    2972 MB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Non-releasable memory |    1880 MB |    1880 MB |    7026 GB |    7024 GB |\n",
            "|       from large pool |    1873 MB |    1873 MB |    6652 GB |    6650 GB |\n",
            "|       from small pool |       7 MB |      17 MB |     373 GB |     373 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Allocations           |     592    |     595    |    3297 K  |    3296 K  |\n",
            "|       from large pool |     150    |     154    |    1032 K  |    1032 K  |\n",
            "|       from small pool |     442    |     516    |    2265 K  |    2264 K  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Active allocs         |     592    |     595    |    3297 K  |    3296 K  |\n",
            "|       from large pool |     150    |     154    |    1032 K  |    1032 K  |\n",
            "|       from small pool |     442    |     516    |    2265 K  |    2264 K  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| GPU reserved segments |      74    |     112    |    1705    |    1631    |\n",
            "|       from large pool |      42    |      43    |     187    |     145    |\n",
            "|       from small pool |      32    |      69    |    1518    |    1486    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Non-releasable allocs |      58    |      58    |    1502 K  |    1502 K  |\n",
            "|       from large pool |      25    |      25    |     659 K  |     659 K  |\n",
            "|       from small pool |      33    |      48    |     842 K  |     842 K  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Oversize allocations  |       0    |       0    |       0    |       0    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Oversize GPU segments |       0    |       0    |       0    |       0    |\n",
            "|===========================================================================|\n",
            "\n",
            "2022-03-10 06:24:27 | WARNING | fairseq.trainer | attempting to recover from OOM in forward/backward pass\n",
            "epoch 005:  35% 244/692 [00:50<01:41,  4.43it/s, loss=3.741, ppl=13.37, wps=8598.3, ups=4.75, wpb=1811.6, bsz=128, num_updates=2900, lr=0.0005, gnorm=1.185, train_wall=19, gb_free=10.6, wall=664]2022-03-10 06:24:39 | WARNING | fairseq.trainer | OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 2.50 GiB (GPU 0; 11.17 GiB total capacity; 8.45 GiB already allocated; 2.03 GiB free; 8.62 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
            "2022-03-10 06:24:39 | WARNING | fairseq.trainer | |===========================================================================|\n",
            "|                  PyTorch CUDA memory summary, device ID 0                 |\n",
            "|---------------------------------------------------------------------------|\n",
            "|            CUDA OOMs: 31           |        cudaMalloc retries: 56        |\n",
            "|===========================================================================|\n",
            "|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Allocated memory      |    8610 MB |    8651 MB |    6351 GB |    6342 GB |\n",
            "|       from large pool |    8560 MB |    8601 MB |    6000 GB |    5992 GB |\n",
            "|       from small pool |      50 MB |      60 MB |     350 GB |     350 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Active memory         |    8610 MB |    8651 MB |    6351 GB |    6342 GB |\n",
            "|       from large pool |    8560 MB |    8601 MB |    6000 GB |    5992 GB |\n",
            "|       from small pool |      50 MB |      60 MB |     350 GB |     350 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| GPU reserved memory   |    8828 MB |    8902 MB |   97040 MB |   88212 MB |\n",
            "|       from large pool |    8768 MB |    8768 MB |   93934 MB |   85166 MB |\n",
            "|       from small pool |      60 MB |     134 MB |    3106 MB |    3046 MB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Non-releasable memory |  222312 KB |    2376 MB |    7166 GB |    7166 GB |\n",
            "|       from large pool |  212346 KB |    2365 MB |    6786 GB |    6786 GB |\n",
            "|       from small pool |    9966 KB |      18 MB |     379 GB |     379 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Allocations           |     593    |     595    |    3350 K  |    3349 K  |\n",
            "|       from large pool |     162    |     163    |    1048 K  |    1048 K  |\n",
            "|       from small pool |     431    |     516    |    2301 K  |    2301 K  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Active allocs         |     593    |     595    |    3350 K  |    3349 K  |\n",
            "|       from large pool |     162    |     163    |    1048 K  |    1048 K  |\n",
            "|       from small pool |     431    |     516    |    2301 K  |    2301 K  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| GPU reserved segments |      77    |     114    |    1746    |    1669    |\n",
            "|       from large pool |      47    |      47    |     193    |     146    |\n",
            "|       from small pool |      30    |      67    |    1553    |    1523    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Non-releasable allocs |      67    |      67    |    1525 K  |    1525 K  |\n",
            "|       from large pool |      36    |      36    |     669 K  |     669 K  |\n",
            "|       from small pool |      31    |      44    |     855 K  |     855 K  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Oversize allocations  |       0    |       0    |       0    |       0    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Oversize GPU segments |       0    |       0    |       0    |       0    |\n",
            "|===========================================================================|\n",
            "\n",
            "2022-03-10 06:24:39 | WARNING | fairseq.trainer | attempting to recover from OOM in forward/backward pass\n",
            "epoch 005:  37% 259/692 [00:53<01:17,  5.60it/s, loss=3.741, ppl=13.37, wps=8598.3, ups=4.75, wpb=1811.6, bsz=128, num_updates=2900, lr=0.0005, gnorm=1.185, train_wall=19, gb_free=10.6, wall=664]2022-03-10 06:24:42 | WARNING | fairseq.trainer | OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 1.46 GiB (GPU 0; 11.17 GiB total capacity; 7.92 GiB already allocated; 599.81 MiB free; 10.07 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
            "2022-03-10 06:24:42 | WARNING | fairseq.trainer | |===========================================================================|\n",
            "|                  PyTorch CUDA memory summary, device ID 0                 |\n",
            "|---------------------------------------------------------------------------|\n",
            "|            CUDA OOMs: 32           |        cudaMalloc retries: 57        |\n",
            "|===========================================================================|\n",
            "|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Allocated memory      |    8106 MB |    8166 MB |    6385 GB |    6377 GB |\n",
            "|       from large pool |    8057 MB |    8117 MB |    6032 GB |    6024 GB |\n",
            "|       from small pool |      48 MB |      60 MB |     352 GB |     352 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Active memory         |    8106 MB |    8166 MB |    6385 GB |    6377 GB |\n",
            "|       from large pool |    8057 MB |    8117 MB |    6032 GB |    6024 GB |\n",
            "|       from small pool |      48 MB |      60 MB |     352 GB |     352 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| GPU reserved memory   |   10308 MB |   10388 MB |   98600 MB |   88292 MB |\n",
            "|       from large pool |   10244 MB |   10260 MB |   95426 MB |   85182 MB |\n",
            "|       from small pool |      64 MB |     128 MB |    3174 MB |    3110 MB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Non-releasable memory |    2201 MB |    2201 MB |    7210 GB |    7207 GB |\n",
            "|       from large pool |    2186 MB |    2186 MB |    6828 GB |    6826 GB |\n",
            "|       from small pool |      15 MB |      34 MB |     381 GB |     381 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Allocations           |     593    |     595    |    3366 K  |    3365 K  |\n",
            "|       from large pool |     160    |     164    |    1053 K  |    1053 K  |\n",
            "|       from small pool |     433    |     516    |    2313 K  |    2312 K  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Active allocs         |     593    |     595    |    3366 K  |    3365 K  |\n",
            "|       from large pool |     160    |     164    |    1053 K  |    1053 K  |\n",
            "|       from small pool |     433    |     516    |    2313 K  |    2312 K  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| GPU reserved segments |      79    |     112    |    1781    |    1702    |\n",
            "|       from large pool |      47    |      48    |     194    |     147    |\n",
            "|       from small pool |      32    |      64    |    1587    |    1555    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Non-releasable allocs |      68    |      69    |    1532 K  |    1532 K  |\n",
            "|       from large pool |      35    |      35    |     672 K  |     672 K  |\n",
            "|       from small pool |      33    |      53    |     860 K  |     860 K  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Oversize allocations  |       0    |       0    |       0    |       0    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Oversize GPU segments |       0    |       0    |       0    |       0    |\n",
            "|===========================================================================|\n",
            "\n",
            "2022-03-10 06:24:42 | WARNING | fairseq.trainer | attempting to recover from OOM in forward/backward pass\n",
            "epoch 005:  66% 457/692 [01:36<00:35,  6.68it/s, loss=4.005, ppl=16.06, wps=9486.1, ups=4.53, wpb=2094, bsz=128, num_updates=3100, lr=0.0005, gnorm=1.19, train_wall=22, gb_free=10.5, wall=709]2022-03-10 06:25:26 | WARNING | fairseq.trainer | OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 1.34 GiB (GPU 0; 11.17 GiB total capacity; 7.45 GiB already allocated; 509.81 MiB free; 10.15 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
            "2022-03-10 06:25:26 | WARNING | fairseq.trainer | |===========================================================================|\n",
            "|                  PyTorch CUDA memory summary, device ID 0                 |\n",
            "|---------------------------------------------------------------------------|\n",
            "|            CUDA OOMs: 33           |        cudaMalloc retries: 59        |\n",
            "|===========================================================================|\n",
            "|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Allocated memory      |    6262 MB |    7629 MB |    6798 GB |    6792 GB |\n",
            "|       from large pool |    6214 MB |    7580 MB |    6422 GB |    6415 GB |\n",
            "|       from small pool |      48 MB |      60 MB |     376 GB |     376 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Active memory         |    6262 MB |    7629 MB |    6798 GB |    6792 GB |\n",
            "|       from large pool |    6214 MB |    7580 MB |    6422 GB |    6415 GB |\n",
            "|       from small pool |      48 MB |      60 MB |     376 GB |     376 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| GPU reserved memory   |   10398 MB |   10472 MB |   99932 MB |   89534 MB |\n",
            "|       from large pool |   10338 MB |   10338 MB |   96614 MB |   86276 MB |\n",
            "|       from small pool |      60 MB |     134 MB |    3318 MB |    3258 MB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Non-releasable memory |    1971 MB |    2768 MB |    7681 GB |    7680 GB |\n",
            "|       from large pool |    1959 MB |    2757 MB |    7274 GB |    7272 GB |\n",
            "|       from small pool |      11 MB |      36 MB |     407 GB |     407 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Allocations           |     592    |     595    |    3586 K  |    3586 K  |\n",
            "|       from large pool |     160    |     164    |    1120 K  |    1120 K  |\n",
            "|       from small pool |     432    |     516    |    2466 K  |    2465 K  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Active allocs         |     592    |     595    |    3586 K  |    3586 K  |\n",
            "|       from large pool |     160    |     164    |    1120 K  |    1120 K  |\n",
            "|       from small pool |     432    |     516    |    2466 K  |    2465 K  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| GPU reserved segments |      77    |     114    |    1854    |    1777    |\n",
            "|       from large pool |      47    |      47    |     195    |     148    |\n",
            "|       from small pool |      30    |      67    |    1659    |    1629    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Non-releasable allocs |      62    |      64    |    1632 K  |    1632 K  |\n",
            "|       from large pool |      30    |      31    |     715 K  |     715 K  |\n",
            "|       from small pool |      32    |      59    |     917 K  |     917 K  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Oversize allocations  |       0    |       0    |       0    |       0    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Oversize GPU segments |       0    |       0    |       0    |       0    |\n",
            "|===========================================================================|\n",
            "\n",
            "2022-03-10 06:25:26 | WARNING | fairseq.trainer | attempting to recover from OOM in forward/backward pass\n",
            "epoch 005:  85% 589/692 [02:08<00:23,  4.32it/s, loss=3.859, ppl=14.51, wps=9304.8, ups=4.93, wpb=1887.1, bsz=128, num_updates=3300, lr=0.0005, gnorm=1.254, train_wall=20, gb_free=9.6, wall=752]2022-03-10 06:25:57 | WARNING | fairseq.trainer | OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 2.11 GiB (GPU 0; 11.17 GiB total capacity; 7.84 GiB already allocated; 1.25 GiB free; 9.40 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
            "2022-03-10 06:25:57 | WARNING | fairseq.trainer | |===========================================================================|\n",
            "|                  PyTorch CUDA memory summary, device ID 0                 |\n",
            "|---------------------------------------------------------------------------|\n",
            "|            CUDA OOMs: 34           |        cudaMalloc retries: 62        |\n",
            "|===========================================================================|\n",
            "|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Allocated memory      |    8031 MB |    8041 MB |    7095 GB |    7088 GB |\n",
            "|       from large pool |    7982 MB |    7992 MB |    6704 GB |    6696 GB |\n",
            "|       from small pool |      49 MB |      60 MB |     391 GB |     391 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Active memory         |    8031 MB |    8041 MB |    7095 GB |    7088 GB |\n",
            "|       from large pool |    7982 MB |    7992 MB |    6704 GB |    6696 GB |\n",
            "|       from small pool |      49 MB |      60 MB |     391 GB |     391 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| GPU reserved memory   |    9626 MB |    9700 MB |  106472 MB |   96846 MB |\n",
            "|       from large pool |    9562 MB |    9562 MB |  102976 MB |   93414 MB |\n",
            "|       from small pool |      64 MB |     138 MB |    3496 MB |    3432 MB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Non-releasable memory |    1594 MB |    2173 MB |    8020 GB |    8019 GB |\n",
            "|       from large pool |    1579 MB |    2157 MB |    7597 GB |    7595 GB |\n",
            "|       from small pool |      14 MB |      54 MB |     423 GB |     423 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Allocations           |     592    |     595    |    3733 K  |    3733 K  |\n",
            "|       from large pool |     162    |     164    |    1167 K  |    1167 K  |\n",
            "|       from small pool |     430    |     516    |    2565 K  |    2565 K  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Active allocs         |     592    |     595    |    3733 K  |    3733 K  |\n",
            "|       from large pool |     162    |     164    |    1167 K  |    1167 K  |\n",
            "|       from small pool |     430    |     516    |    2565 K  |    2565 K  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| GPU reserved segments |      76    |     113    |    1948    |    1872    |\n",
            "|       from large pool |      44    |      44    |     200    |     156    |\n",
            "|       from small pool |      32    |      69    |    1748    |    1716    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Non-releasable allocs |      66    |      67    |    1699 K  |    1699 K  |\n",
            "|       from large pool |      32    |      34    |     745 K  |     745 K  |\n",
            "|       from small pool |      34    |      59    |     953 K  |     953 K  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Oversize allocations  |       0    |       0    |       0    |       0    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Oversize GPU segments |       0    |       0    |       0    |       0    |\n",
            "|===========================================================================|\n",
            "\n",
            "2022-03-10 06:25:57 | WARNING | fairseq.trainer | attempting to recover from OOM in forward/backward pass\n",
            "epoch 005:  87% 603/692 [02:11<00:18,  4.82it/s, loss=3.859, ppl=14.51, wps=9304.8, ups=4.93, wpb=1887.1, bsz=128, num_updates=3300, lr=0.0005, gnorm=1.254, train_wall=20, gb_free=9.6, wall=752]2022-03-10 06:26:00 | WARNING | fairseq.trainer | OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 1.34 GiB (GPU 0; 11.17 GiB total capacity; 7.01 GiB already allocated; 1.24 GiB free; 9.41 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
            "2022-03-10 06:26:00 | WARNING | fairseq.trainer | |===========================================================================|\n",
            "|                  PyTorch CUDA memory summary, device ID 0                 |\n",
            "|---------------------------------------------------------------------------|\n",
            "|            CUDA OOMs: 35           |        cudaMalloc retries: 64        |\n",
            "|===========================================================================|\n",
            "|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Allocated memory      |    5815 MB |    7182 MB |    7125 GB |    7119 GB |\n",
            "|       from large pool |    5767 MB |    7134 MB |    6732 GB |    6727 GB |\n",
            "|       from small pool |      48 MB |      60 MB |     392 GB |     392 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Active memory         |    5815 MB |    7182 MB |    7125 GB |    7119 GB |\n",
            "|       from large pool |    5767 MB |    7134 MB |    6732 GB |    6727 GB |\n",
            "|       from small pool |      48 MB |      60 MB |     392 GB |     392 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| GPU reserved memory   |    9638 MB |    9690 MB |  107904 MB |   98266 MB |\n",
            "|       from large pool |    9578 MB |    9578 MB |  104344 MB |   94766 MB |\n",
            "|       from small pool |      60 MB |     128 MB |    3560 MB |    3500 MB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Non-releasable memory |    2454 MB |    2455 MB |    8056 GB |    8054 GB |\n",
            "|       from large pool |    2442 MB |    2443 MB |    7631 GB |    7629 GB |\n",
            "|       from small pool |      11 MB |      21 MB |     424 GB |     424 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Allocations           |     592    |     595    |    3748 K  |    3748 K  |\n",
            "|       from large pool |     160    |     164    |    1173 K  |    1172 K  |\n",
            "|       from small pool |     432    |     516    |    2575 K  |    2575 K  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Active allocs         |     592    |     595    |    3748 K  |    3748 K  |\n",
            "|       from large pool |     160    |     164    |    1173 K  |    1172 K  |\n",
            "|       from small pool |     432    |     516    |    2575 K  |    2575 K  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| GPU reserved segments |      74    |     108    |    1981    |    1907    |\n",
            "|       from large pool |      44    |      44    |     201    |     157    |\n",
            "|       from small pool |      30    |      64    |    1780    |    1750    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Non-releasable allocs |      60    |      61    |    1706 K  |    1705 K  |\n",
            "|       from large pool |      27    |      28    |     749 K  |     749 K  |\n",
            "|       from small pool |      33    |      51    |     956 K  |     956 K  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Oversize allocations  |       0    |       0    |       0    |       0    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Oversize GPU segments |       0    |       0    |       0    |       0    |\n",
            "|===========================================================================|\n",
            "\n",
            "2022-03-10 06:26:00 | WARNING | fairseq.trainer | attempting to recover from OOM in forward/backward pass\n",
            "epoch 005:  89% 617/692 [02:14<00:15,  4.99it/s, loss=3.859, ppl=14.51, wps=9304.8, ups=4.93, wpb=1887.1, bsz=128, num_updates=3300, lr=0.0005, gnorm=1.254, train_wall=20, gb_free=9.6, wall=752]2022-03-10 06:26:03 | WARNING | fairseq.trainer | OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 1.69 GiB (GPU 0; 11.17 GiB total capacity; 9.21 GiB already allocated; 661.81 MiB free; 10.01 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
            "2022-03-10 06:26:03 | WARNING | fairseq.trainer | |===========================================================================|\n",
            "|                  PyTorch CUDA memory summary, device ID 0                 |\n",
            "|---------------------------------------------------------------------------|\n",
            "|            CUDA OOMs: 36           |        cudaMalloc retries: 66        |\n",
            "|===========================================================================|\n",
            "|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Allocated memory      |    7699 MB |    9433 MB |    7153 GB |    7146 GB |\n",
            "|       from large pool |    7650 MB |    9384 MB |    6759 GB |    6752 GB |\n",
            "|       from small pool |      48 MB |      60 MB |     394 GB |     394 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Active memory         |    7699 MB |    9433 MB |    7153 GB |    7146 GB |\n",
            "|       from large pool |    7650 MB |    9384 MB |    6759 GB |    6752 GB |\n",
            "|       from small pool |      48 MB |      60 MB |     394 GB |     394 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| GPU reserved memory   |   10246 MB |   10246 MB |  111426 MB |  101180 MB |\n",
            "|       from large pool |   10186 MB |   10186 MB |  107812 MB |   97626 MB |\n",
            "|       from small pool |      60 MB |     114 MB |    3614 MB |    3554 MB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Non-releasable memory |     812 MB |    1576 MB |    8086 GB |    8085 GB |\n",
            "|       from large pool |     801 MB |    1564 MB |    7659 GB |    7659 GB |\n",
            "|       from small pool |      11 MB |      14 MB |     426 GB |     426 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Allocations           |     592    |     595    |    3763 K  |    3762 K  |\n",
            "|       from large pool |     160    |     164    |    1177 K  |    1177 K  |\n",
            "|       from small pool |     432    |     516    |    2586 K  |    2585 K  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Active allocs         |     592    |     595    |    3763 K  |    3762 K  |\n",
            "|       from large pool |     160    |     164    |    1177 K  |    1177 K  |\n",
            "|       from small pool |     432    |     516    |    2586 K  |    2585 K  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| GPU reserved segments |      74    |     101    |    2010    |    1936    |\n",
            "|       from large pool |      44    |      44    |     203    |     159    |\n",
            "|       from small pool |      30    |      57    |    1807    |    1777    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Non-releasable allocs |      62    |      62    |    1712 K  |    1712 K  |\n",
            "|       from large pool |      30    |      30    |     752 K  |     751 K  |\n",
            "|       from small pool |      32    |      41    |     960 K  |     960 K  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Oversize allocations  |       0    |       0    |       0    |       0    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Oversize GPU segments |       0    |       0    |       0    |       0    |\n",
            "|===========================================================================|\n",
            "\n",
            "2022-03-10 06:26:03 | WARNING | fairseq.trainer | attempting to recover from OOM in forward/backward pass\n",
            "epoch 005: 100% 691/692 [02:32<00:00,  4.48it/s, loss=4.053, ppl=16.6, wps=7403.6, ups=3.66, wpb=2024.2, bsz=128, num_updates=3400, lr=0.0005, gnorm=1.259, train_wall=24, gb_free=10.7, wall=779]2022-03-10 06:26:20 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "\n",
            "epoch 005 | valid on 'valid' subset:   0% 0/8 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 005 | valid on 'valid' subset:  12% 1/8 [00:00<00:01,  6.76it/s]\u001b[A\n",
            "epoch 005 | valid on 'valid' subset:  25% 2/8 [00:00<00:00,  7.91it/s]\u001b[A\n",
            "epoch 005 | valid on 'valid' subset:  50% 4/8 [00:00<00:00,  9.84it/s]\u001b[A\n",
            "epoch 005 | valid on 'valid' subset:  62% 5/8 [00:00<00:00,  9.44it/s]\u001b[A\n",
            "epoch 005 | valid on 'valid' subset:  75% 6/8 [00:00<00:00,  9.26it/s]\u001b[A\n",
            "epoch 005 | valid on 'valid' subset:  88% 7/8 [00:00<00:00,  8.84it/s]\u001b[A\n",
            "epoch 005 | valid on 'valid' subset: 100% 8/8 [00:01<00:00,  4.96it/s]\u001b[A\n",
            "                                                                      \u001b[A2022-03-10 06:26:22 | INFO | valid | epoch 005 | valid on 'valid' subset | loss 4.912 | ppl 30.11 | wps 14165.8 | wpb 1992.9 | bsz 125 | num_updates 3424 | best_loss 4.912\n",
            "2022-03-10 06:26:22 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 5 @ 3424 updates\n",
            "2022-03-10 06:26:22 | INFO | fairseq.trainer | Saving checkpoint to /content/checkpoints/checkpoint5.pt\n",
            "2022-03-10 06:26:22 | INFO | fairseq.trainer | Finished saving checkpoint to /content/checkpoints/checkpoint5.pt\n",
            "2022-03-10 06:26:24 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/checkpoint5.pt (epoch 5 @ 3424 updates, score 4.912) (writing took 2.003036543999997 seconds)\n",
            "2022-03-10 06:26:24 | INFO | fairseq_cli.train | end of epoch 5 (average epoch stats below)\n",
            "2022-03-10 06:26:24 | INFO | train | epoch 005 | loss 3.89 | ppl 14.82 | wps 8487.8 | ups 4.39 | wpb 1931.8 | bsz 127.9 | num_updates 3424 | lr 0.0005 | gnorm 1.213 | train_wall 142 | gb_free 10.5 | wall 787\n",
            "epoch 006:   0% 0/692 [00:00<?, ?it/s]2022-03-10 06:26:24 | INFO | fairseq.data.iterators | grouped total_num_itrs = 692\n",
            "2022-03-10 06:26:24 | INFO | fairseq.trainer | begin training epoch 6\n",
            "2022-03-10 06:26:24 | INFO | fairseq_cli.train | Start iterating over samples\n",
            "epoch 006:  11% 75/692 [00:16<01:30,  6.79it/s]2022-03-10 06:26:41 | WARNING | fairseq.trainer | OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 2.50 GiB (GPU 0; 11.17 GiB total capacity; 8.45 GiB already allocated; 1.15 GiB free; 9.50 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
            "2022-03-10 06:26:41 | WARNING | fairseq.trainer | |===========================================================================|\n",
            "|                  PyTorch CUDA memory summary, device ID 0                 |\n",
            "|---------------------------------------------------------------------------|\n",
            "|            CUDA OOMs: 37           |        cudaMalloc retries: 67        |\n",
            "|===========================================================================|\n",
            "|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Allocated memory      |    8610 MB |    8651 MB |    7496 GB |    7488 GB |\n",
            "|       from large pool |    8560 MB |    8601 MB |    7085 GB |    7076 GB |\n",
            "|       from small pool |      50 MB |      60 MB |     411 GB |     411 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Active memory         |    8610 MB |    8651 MB |    7496 GB |    7488 GB |\n",
            "|       from large pool |    8560 MB |    8601 MB |    7085 GB |    7076 GB |\n",
            "|       from small pool |      50 MB |      60 MB |     411 GB |     411 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| GPU reserved memory   |    9726 MB |    9804 MB |  112718 MB |  102992 MB |\n",
            "|       from large pool |    9664 MB |    9664 MB |  109024 MB |   99360 MB |\n",
            "|       from small pool |      62 MB |     140 MB |    3694 MB |    3632 MB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Non-releasable memory |    1115 MB |    2296 MB |    8492 GB |    8491 GB |\n",
            "|       from large pool |    1103 MB |    2282 MB |    8047 GB |    8046 GB |\n",
            "|       from small pool |      11 MB |      19 MB |     445 GB |     445 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Allocations           |     593    |     595    |    3932 K  |    3931 K  |\n",
            "|       from large pool |     162    |     163    |    1231 K  |    1231 K  |\n",
            "|       from small pool |     431    |     516    |    2700 K  |    2700 K  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Active allocs         |     593    |     595    |    3932 K  |    3931 K  |\n",
            "|       from large pool |     162    |     163    |    1231 K  |    1231 K  |\n",
            "|       from small pool |     431    |     516    |    2700 K  |    2700 K  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| GPU reserved segments |      75    |     114    |    2051    |    1976    |\n",
            "|       from large pool |      44    |      44    |     204    |     160    |\n",
            "|       from small pool |      31    |      70    |    1847    |    1816    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Non-releasable allocs |      68    |      68    |    1789 K  |    1789 K  |\n",
            "|       from large pool |      33    |      33    |     786 K  |     786 K  |\n",
            "|       from small pool |      35    |      44    |    1003 K  |    1003 K  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Oversize allocations  |       0    |       0    |       0    |       0    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Oversize GPU segments |       0    |       0    |       0    |       0    |\n",
            "|===========================================================================|\n",
            "\n",
            "2022-03-10 06:26:41 | WARNING | fairseq.trainer | attempting to recover from OOM in forward/backward pass\n",
            "epoch 006:  15% 104/692 [00:24<01:34,  6.20it/s, loss=3.503, ppl=11.34, wps=8054.9, ups=3.96, wpb=2031.7, bsz=127.1, num_updates=3500, lr=0.0005, gnorm=1.132, train_wall=21, gb_free=2.7, wall=805]2022-03-10 06:26:49 | WARNING | fairseq.trainer | OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 2.11 GiB (GPU 0; 11.17 GiB total capacity; 7.84 GiB already allocated; 1.16 GiB free; 9.50 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
            "2022-03-10 06:26:49 | WARNING | fairseq.trainer | |===========================================================================|\n",
            "|                  PyTorch CUDA memory summary, device ID 0                 |\n",
            "|---------------------------------------------------------------------------|\n",
            "|            CUDA OOMs: 38           |        cudaMalloc retries: 68        |\n",
            "|===========================================================================|\n",
            "|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Allocated memory      |    8030 MB |    8041 MB |    7574 GB |    7566 GB |\n",
            "|       from large pool |    7981 MB |    7991 MB |    7160 GB |    7152 GB |\n",
            "|       from small pool |      49 MB |      60 MB |     414 GB |     414 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Active memory         |    8030 MB |    8041 MB |    7574 GB |    7566 GB |\n",
            "|       from large pool |    7981 MB |    7991 MB |    7160 GB |    7152 GB |\n",
            "|       from small pool |      49 MB |      60 MB |     414 GB |     414 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| GPU reserved memory   |    9724 MB |    9782 MB |  112774 MB |  103050 MB |\n",
            "|       from large pool |    9664 MB |    9664 MB |  109024 MB |   99360 MB |\n",
            "|       from small pool |      60 MB |     118 MB |    3750 MB |    3690 MB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Non-releasable memory |    1693 MB |    2271 MB |    8583 GB |    8581 GB |\n",
            "|       from large pool |    1682 MB |    2259 MB |    8135 GB |    8133 GB |\n",
            "|       from small pool |      10 MB |      19 MB |     448 GB |     448 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Allocations           |     592    |     595    |    3964 K  |    3963 K  |\n",
            "|       from large pool |     162    |     164    |    1242 K  |    1242 K  |\n",
            "|       from small pool |     430    |     516    |    2721 K  |    2721 K  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Active allocs         |     592    |     595    |    3964 K  |    3963 K  |\n",
            "|       from large pool |     162    |     164    |    1242 K  |    1242 K  |\n",
            "|       from small pool |     430    |     516    |    2721 K  |    2721 K  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| GPU reserved segments |      74    |     103    |    2079    |    2005    |\n",
            "|       from large pool |      44    |      44    |     204    |     160    |\n",
            "|       from small pool |      30    |      59    |    1875    |    1845    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Non-releasable allocs |      68    |      68    |    1804 K  |    1804 K  |\n",
            "|       from large pool |      34    |      34    |     793 K  |     793 K  |\n",
            "|       from small pool |      34    |      44    |    1011 K  |    1011 K  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Oversize allocations  |       0    |       0    |       0    |       0    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Oversize GPU segments |       0    |       0    |       0    |       0    |\n",
            "|===========================================================================|\n",
            "\n",
            "2022-03-10 06:26:49 | WARNING | fairseq.trainer | attempting to recover from OOM in forward/backward pass\n",
            "epoch 006:  32% 222/692 [00:50<02:15,  3.46it/s, loss=3.482, ppl=11.18, wps=8751.6, ups=4.54, wpb=1928.3, bsz=128, num_updates=3600, lr=0.0005, gnorm=1.204, train_wall=21, gb_free=10.7, wall=827]2022-03-10 06:27:15 | WARNING | fairseq.trainer | OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 1.46 GiB (GPU 0; 11.17 GiB total capacity; 7.92 GiB already allocated; 1.17 GiB free; 9.48 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
            "2022-03-10 06:27:15 | WARNING | fairseq.trainer | |===========================================================================|\n",
            "|                  PyTorch CUDA memory summary, device ID 0                 |\n",
            "|---------------------------------------------------------------------------|\n",
            "|            CUDA OOMs: 39           |        cudaMalloc retries: 69        |\n",
            "|===========================================================================|\n",
            "|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Allocated memory      |    8106 MB |    8165 MB |    7830 GB |    7822 GB |\n",
            "|       from large pool |    8057 MB |    8116 MB |    7402 GB |    7394 GB |\n",
            "|       from small pool |      48 MB |      60 MB |     427 GB |     427 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Active memory         |    8106 MB |    8165 MB |    7830 GB |    7822 GB |\n",
            "|       from large pool |    8057 MB |    8116 MB |    7402 GB |    7394 GB |\n",
            "|       from small pool |      48 MB |      60 MB |     427 GB |     427 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| GPU reserved memory   |    9708 MB |    9798 MB |  112848 MB |  103140 MB |\n",
            "|       from large pool |    9648 MB |    9664 MB |  109024 MB |   99376 MB |\n",
            "|       from small pool |      60 MB |     134 MB |    3824 MB |    3764 MB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Non-releasable memory |    1601 MB |    1601 MB |    8879 GB |    8877 GB |\n",
            "|       from large pool |    1590 MB |    1590 MB |    8416 GB |    8415 GB |\n",
            "|       from small pool |      11 MB |      14 MB |     462 GB |     462 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Allocations           |     593    |     595    |    4095 K  |    4094 K  |\n",
            "|       from large pool |     160    |     164    |    1284 K  |    1283 K  |\n",
            "|       from small pool |     433    |     516    |    2811 K  |    2810 K  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Active allocs         |     593    |     595    |    4095 K  |    4094 K  |\n",
            "|       from large pool |     160    |     164    |    1284 K  |    1283 K  |\n",
            "|       from small pool |     433    |     516    |    2811 K  |    2810 K  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| GPU reserved segments |      73    |     111    |    2116    |    2043    |\n",
            "|       from large pool |      43    |      44    |     204    |     161    |\n",
            "|       from small pool |      30    |      67    |    1912    |    1882    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Non-releasable allocs |      66    |      66    |    1863 K  |    1863 K  |\n",
            "|       from large pool |      34    |      34    |     819 K  |     819 K  |\n",
            "|       from small pool |      32    |      35    |    1044 K  |    1044 K  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Oversize allocations  |       0    |       0    |       0    |       0    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Oversize GPU segments |       0    |       0    |       0    |       0    |\n",
            "|===========================================================================|\n",
            "\n",
            "2022-03-10 06:27:15 | WARNING | fairseq.trainer | attempting to recover from OOM in forward/backward pass\n",
            "epoch 006:  44% 306/692 [01:06<00:59,  6.53it/s, loss=3.541, ppl=11.64, wps=9099.1, ups=4.53, wpb=2007.2, bsz=128, num_updates=3700, lr=0.0005, gnorm=1.237, train_wall=21, gb_free=10.5, wall=849]2022-03-10 06:27:30 | WARNING | fairseq.trainer | OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 1.25 GiB (GPU 0; 11.17 GiB total capacity; 7.23 GiB already allocated; 1.17 GiB free; 9.48 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
            "2022-03-10 06:27:30 | WARNING | fairseq.trainer | |===========================================================================|\n",
            "|                  PyTorch CUDA memory summary, device ID 0                 |\n",
            "|---------------------------------------------------------------------------|\n",
            "|            CUDA OOMs: 40           |        cudaMalloc retries: 70        |\n",
            "|===========================================================================|\n",
            "|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Allocated memory      |    6123 MB |    7404 MB |    7969 GB |    7963 GB |\n",
            "|       from large pool |    6075 MB |    7356 MB |    7531 GB |    7525 GB |\n",
            "|       from small pool |      48 MB |      60 MB |     438 GB |     438 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Active memory         |    6123 MB |    7404 MB |    7969 GB |    7963 GB |\n",
            "|       from large pool |    6075 MB |    7356 MB |    7531 GB |    7525 GB |\n",
            "|       from small pool |      48 MB |      60 MB |     438 GB |     438 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| GPU reserved memory   |    9710 MB |    9786 MB |  112926 MB |  103216 MB |\n",
            "|       from large pool |    9648 MB |    9648 MB |  109024 MB |   99376 MB |\n",
            "|       from small pool |      62 MB |     138 MB |    3902 MB |    3840 MB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Non-releasable memory |    1422 MB |    2305 MB |    9048 GB |    9046 GB |\n",
            "|       from large pool |    1408 MB |    2291 MB |    8573 GB |    8572 GB |\n",
            "|       from small pool |      13 MB |      20 MB |     474 GB |     474 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Allocations           |     592    |     595    |    4188 K  |    4187 K  |\n",
            "|       from large pool |     160    |     164    |    1311 K  |    1311 K  |\n",
            "|       from small pool |     432    |     516    |    2876 K  |    2876 K  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Active allocs         |     592    |     595    |    4188 K  |    4187 K  |\n",
            "|       from large pool |     160    |     164    |    1311 K  |    1311 K  |\n",
            "|       from small pool |     432    |     516    |    2876 K  |    2876 K  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| GPU reserved segments |      74    |     112    |    2155    |    2081    |\n",
            "|       from large pool |      43    |      43    |     204    |     161    |\n",
            "|       from small pool |      31    |      69    |    1951    |    1920    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Non-releasable allocs |      71    |      72    |    1907 K  |    1906 K  |\n",
            "|       from large pool |      35    |      36    |     836 K  |     836 K  |\n",
            "|       from small pool |      36    |      55    |    1070 K  |    1069 K  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Oversize allocations  |       0    |       0    |       0    |       0    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Oversize GPU segments |       0    |       0    |       0    |       0    |\n",
            "|===========================================================================|\n",
            "\n",
            "2022-03-10 06:27:30 | WARNING | fairseq.trainer | attempting to recover from OOM in forward/backward pass\n",
            "epoch 006:  52% 359/692 [01:16<00:55,  6.03it/s, loss=3.541, ppl=11.64, wps=9099.1, ups=4.53, wpb=2007.2, bsz=128, num_updates=3700, lr=0.0005, gnorm=1.237, train_wall=21, gb_free=10.5, wall=849]2022-03-10 06:27:41 | WARNING | fairseq.trainer | OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 1.69 GiB (GPU 0; 11.17 GiB total capacity; 7.52 GiB already allocated; 1.59 GiB free; 9.06 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
            "2022-03-10 06:27:41 | WARNING | fairseq.trainer | |===========================================================================|\n",
            "|                  PyTorch CUDA memory summary, device ID 0                 |\n",
            "|---------------------------------------------------------------------------|\n",
            "|            CUDA OOMs: 41           |        cudaMalloc retries: 71        |\n",
            "|===========================================================================|\n",
            "|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Allocated memory      |    7702 MB |    7762 MB |    8064 GB |    8056 GB |\n",
            "|       from large pool |    7653 MB |    7713 MB |    7619 GB |    7611 GB |\n",
            "|       from small pool |      48 MB |      60 MB |     445 GB |     445 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Active memory         |    7702 MB |    7762 MB |    8064 GB |    8056 GB |\n",
            "|       from large pool |    7653 MB |    7713 MB |    7619 GB |    7611 GB |\n",
            "|       from small pool |      48 MB |      60 MB |     445 GB |     445 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| GPU reserved memory   |    9278 MB |    9352 MB |  114732 MB |  105454 MB |\n",
            "|       from large pool |    9218 MB |    9218 MB |  110758 MB |  101540 MB |\n",
            "|       from small pool |      60 MB |     134 MB |    3974 MB |    3914 MB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Non-releasable memory |    1575 MB |    1575 MB |    9160 GB |    9158 GB |\n",
            "|       from large pool |    1564 MB |    1564 MB |    8678 GB |    8677 GB |\n",
            "|       from small pool |      11 MB |      42 MB |     481 GB |     481 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Allocations           |     593    |     595    |    4246 K  |    4245 K  |\n",
            "|       from large pool |     160    |     164    |    1328 K  |    1328 K  |\n",
            "|       from small pool |     433    |     516    |    2917 K  |    2917 K  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Active allocs         |     593    |     595    |    4246 K  |    4245 K  |\n",
            "|       from large pool |     160    |     164    |    1328 K  |    1328 K  |\n",
            "|       from small pool |     433    |     516    |    2917 K  |    2917 K  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| GPU reserved segments |      73    |     110    |    2192    |    2119    |\n",
            "|       from large pool |      43    |      43    |     205    |     162    |\n",
            "|       from small pool |      30    |      67    |    1987    |    1957    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Non-releasable allocs |      58    |      69    |    1933 K  |    1933 K  |\n",
            "|       from large pool |      27    |      27    |     847 K  |     847 K  |\n",
            "|       from small pool |      31    |      65    |    1085 K  |    1085 K  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Oversize allocations  |       0    |       0    |       0    |       0    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Oversize GPU segments |       0    |       0    |       0    |       0    |\n",
            "|===========================================================================|\n",
            "\n",
            "2022-03-10 06:27:41 | WARNING | fairseq.trainer | attempting to recover from OOM in forward/backward pass\n",
            "epoch 006:  65% 453/692 [01:36<00:44,  5.37it/s, loss=3.492, ppl=11.25, wps=8398.9, ups=4.68, wpb=1795.3, bsz=128, num_updates=3800, lr=0.0005, gnorm=1.305, train_wall=20, gb_free=10.6, wall=870]2022-03-10 06:28:01 | WARNING | fairseq.trainer | OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 2.01 GiB (GPU 0; 11.17 GiB total capacity; 7.85 GiB already allocated; 1.27 GiB free; 9.38 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
            "2022-03-10 06:28:01 | WARNING | fairseq.trainer | |===========================================================================|\n",
            "|                  PyTorch CUDA memory summary, device ID 0                 |\n",
            "|---------------------------------------------------------------------------|\n",
            "|            CUDA OOMs: 42           |        cudaMalloc retries: 73        |\n",
            "|===========================================================================|\n",
            "|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Allocated memory      |    8037 MB |    8053 MB |    8256 GB |    8248 GB |\n",
            "|       from large pool |    7987 MB |    8004 MB |    7800 GB |    7792 GB |\n",
            "|       from small pool |      49 MB |      60 MB |     456 GB |     456 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Active memory         |    8037 MB |    8053 MB |    8256 GB |    8248 GB |\n",
            "|       from large pool |    7987 MB |    8004 MB |    7800 GB |    7792 GB |\n",
            "|       from small pool |      49 MB |      60 MB |     456 GB |     456 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| GPU reserved memory   |    9608 MB |    9608 MB |  116872 MB |  107264 MB |\n",
            "|       from large pool |    9546 MB |    9546 MB |  112820 MB |  103274 MB |\n",
            "|       from small pool |      62 MB |     138 MB |    4052 MB |    3990 MB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Non-releasable memory |    1570 MB |    2205 MB |    9378 GB |    9377 GB |\n",
            "|       from large pool |    1558 MB |    2192 MB |    8884 GB |    8883 GB |\n",
            "|       from small pool |      12 MB |      31 MB |     494 GB |     494 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Allocations           |     592    |     595    |    4350 K  |    4350 K  |\n",
            "|       from large pool |     162    |     164    |    1360 K  |    1360 K  |\n",
            "|       from small pool |     430    |     516    |    2990 K  |    2989 K  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Active allocs         |     592    |     595    |    4350 K  |    4350 K  |\n",
            "|       from large pool |     162    |     164    |    1360 K  |    1360 K  |\n",
            "|       from small pool |     430    |     516    |    2990 K  |    2989 K  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| GPU reserved segments |      74    |     112    |    2232    |    2158    |\n",
            "|       from large pool |      43    |      43    |     206    |     163    |\n",
            "|       from small pool |      31    |      69    |    2026    |    1995    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Non-releasable allocs |      71    |      71    |    1980 K  |    1980 K  |\n",
            "|       from large pool |      36    |      36    |     868 K  |     868 K  |\n",
            "|       from small pool |      35    |      48    |    1112 K  |    1112 K  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Oversize allocations  |       0    |       0    |       0    |       0    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Oversize GPU segments |       0    |       0    |       0    |       0    |\n",
            "|===========================================================================|\n",
            "\n",
            "2022-03-10 06:28:01 | WARNING | fairseq.trainer | attempting to recover from OOM in forward/backward pass\n",
            "epoch 006:  71% 494/692 [01:48<00:46,  4.30it/s, loss=3.5, ppl=11.31, wps=8963.4, ups=4.9, wpb=1827.7, bsz=128, num_updates=3900, lr=0.0005, gnorm=1.295, train_wall=19, gb_free=10.3, wall=890]2022-03-10 06:28:13 | WARNING | fairseq.trainer | OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 1.74 GiB (GPU 0; 11.17 GiB total capacity; 8.31 GiB already allocated; 1.09 GiB free; 9.57 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
            "2022-03-10 06:28:13 | WARNING | fairseq.trainer | |===========================================================================|\n",
            "|                  PyTorch CUDA memory summary, device ID 0                 |\n",
            "|---------------------------------------------------------------------------|\n",
            "|            CUDA OOMs: 43           |        cudaMalloc retries: 76        |\n",
            "|===========================================================================|\n",
            "|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Allocated memory      |    8510 MB |    8575 MB |    8373 GB |    8365 GB |\n",
            "|       from large pool |    8461 MB |    8526 MB |    7912 GB |    7903 GB |\n",
            "|       from small pool |      48 MB |      60 MB |     461 GB |     461 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Active memory         |    8510 MB |    8575 MB |    8373 GB |    8365 GB |\n",
            "|       from large pool |    8461 MB |    8526 MB |    7912 GB |    7903 GB |\n",
            "|       from small pool |      48 MB |      60 MB |     461 GB |     461 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| GPU reserved memory   |    9796 MB |    9818 MB |  120154 MB |  110358 MB |\n",
            "|       from large pool |    9734 MB |    9734 MB |  115954 MB |  106220 MB |\n",
            "|       from small pool |      62 MB |     132 MB |    4200 MB |    4138 MB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Non-releasable memory |    1285 MB |    1592 MB |    9505 GB |    9504 GB |\n",
            "|       from large pool |    1272 MB |    1577 MB |    9006 GB |    9005 GB |\n",
            "|       from small pool |      13 MB |      21 MB |     499 GB |     499 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Allocations           |     593    |     595    |    4395 K  |    4395 K  |\n",
            "|       from large pool |     160    |     164    |    1374 K  |    1374 K  |\n",
            "|       from small pool |     433    |     516    |    3021 K  |    3020 K  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Active allocs         |     593    |     595    |    4395 K  |    4395 K  |\n",
            "|       from large pool |     160    |     164    |    1374 K  |    1374 K  |\n",
            "|       from small pool |     433    |     516    |    3021 K  |    3020 K  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| GPU reserved segments |      74    |     109    |    2308    |    2234    |\n",
            "|       from large pool |      43    |      43    |     208    |     165    |\n",
            "|       from small pool |      31    |      66    |    2100    |    2069    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Non-releasable allocs |      64    |      65    |    2000 K  |    2000 K  |\n",
            "|       from large pool |      33    |      33    |     876 K  |     876 K  |\n",
            "|       from small pool |      31    |      51    |    1123 K  |    1123 K  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Oversize allocations  |       0    |       0    |       0    |       0    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Oversize GPU segments |       0    |       0    |       0    |       0    |\n",
            "|===========================================================================|\n",
            "\n",
            "2022-03-10 06:28:13 | WARNING | fairseq.trainer | attempting to recover from OOM in forward/backward pass\n",
            "epoch 006:  76% 529/692 [01:57<00:30,  5.31it/s, loss=3.5, ppl=11.31, wps=8963.4, ups=4.9, wpb=1827.7, bsz=128, num_updates=3900, lr=0.0005, gnorm=1.295, train_wall=19, gb_free=10.3, wall=890]2022-03-10 06:28:22 | WARNING | fairseq.trainer | OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 1.34 GiB (GPU 0; 11.17 GiB total capacity; 7.45 GiB already allocated; 1.07 GiB free; 9.58 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
            "2022-03-10 06:28:22 | WARNING | fairseq.trainer | |===========================================================================|\n",
            "|                  PyTorch CUDA memory summary, device ID 0                 |\n",
            "|---------------------------------------------------------------------------|\n",
            "|            CUDA OOMs: 44           |        cudaMalloc retries: 78        |\n",
            "|===========================================================================|\n",
            "|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Allocated memory      |    6263 MB |    7630 MB |    8458 GB |    8451 GB |\n",
            "|       from large pool |    6215 MB |    7582 MB |    7993 GB |    7986 GB |\n",
            "|       from small pool |      48 MB |      60 MB |     464 GB |     464 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Active memory         |    6263 MB |    7630 MB |    8458 GB |    8451 GB |\n",
            "|       from large pool |    6215 MB |    7582 MB |    7993 GB |    7986 GB |\n",
            "|       from small pool |      48 MB |      60 MB |     464 GB |     464 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| GPU reserved memory   |    9810 MB |    9862 MB |  121588 MB |  111778 MB |\n",
            "|       from large pool |    9750 MB |    9750 MB |  117322 MB |  107572 MB |\n",
            "|       from small pool |      60 MB |     128 MB |    4266 MB |    4206 MB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Non-releasable memory |    2178 MB |    2179 MB |    9604 GB |    9602 GB |\n",
            "|       from large pool |    2166 MB |    2167 MB |    9101 GB |    9099 GB |\n",
            "|       from small pool |      11 MB |      17 MB |     503 GB |     503 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Allocations           |     592    |     595    |    4434 K  |    4433 K  |\n",
            "|       from large pool |     160    |     164    |    1387 K  |    1387 K  |\n",
            "|       from small pool |     432    |     516    |    3046 K  |    3046 K  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Active allocs         |     592    |     595    |    4434 K  |    4433 K  |\n",
            "|       from large pool |     160    |     164    |    1387 K  |    1387 K  |\n",
            "|       from small pool |     432    |     516    |    3046 K  |    3046 K  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| GPU reserved segments |      73    |     107    |    2342    |    2269    |\n",
            "|       from large pool |      43    |      43    |     209    |     166    |\n",
            "|       from small pool |      30    |      64    |    2133    |    2103    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Non-releasable allocs |      58    |      59    |    2017 K  |    2017 K  |\n",
            "|       from large pool |      25    |      26    |     884 K  |     884 K  |\n",
            "|       from small pool |      33    |      40    |    1132 K  |    1132 K  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Oversize allocations  |       0    |       0    |       0    |       0    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Oversize GPU segments |       0    |       0    |       0    |       0    |\n",
            "|===========================================================================|\n",
            "\n",
            "2022-03-10 06:28:22 | WARNING | fairseq.trainer | attempting to recover from OOM in forward/backward pass\n",
            "epoch 006:  81% 560/692 [02:05<00:29,  4.47it/s, loss=3.5, ppl=11.31, wps=8963.4, ups=4.9, wpb=1827.7, bsz=128, num_updates=3900, lr=0.0005, gnorm=1.295, train_wall=19, gb_free=10.3, wall=890]2022-03-10 06:28:29 | WARNING | fairseq.trainer | OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 1.46 GiB (GPU 0; 11.17 GiB total capacity; 7.60 GiB already allocated; 1.21 GiB free; 9.45 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
            "2022-03-10 06:28:29 | WARNING | fairseq.trainer | |===========================================================================|\n",
            "|                  PyTorch CUDA memory summary, device ID 0                 |\n",
            "|---------------------------------------------------------------------------|\n",
            "|            CUDA OOMs: 45           |        cudaMalloc retries: 80        |\n",
            "|===========================================================================|\n",
            "|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Allocated memory      |    6293 MB |    7785 MB |    8529 GB |    8523 GB |\n",
            "|       from large pool |    6237 MB |    7729 MB |    8061 GB |    8055 GB |\n",
            "|       from small pool |      56 MB |      60 MB |     468 GB |     468 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Active memory         |    6293 MB |    7785 MB |    8529 GB |    8523 GB |\n",
            "|       from large pool |    6237 MB |    7729 MB |    8061 GB |    8055 GB |\n",
            "|       from small pool |      56 MB |      60 MB |     468 GB |     468 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| GPU reserved memory   |    9672 MB |    9672 MB |  124236 MB |  114564 MB |\n",
            "|       from large pool |    9608 MB |    9608 MB |  119900 MB |  110292 MB |\n",
            "|       from small pool |      64 MB |     130 MB |    4336 MB |    4272 MB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Non-releasable memory |    1886 MB |    1886 MB |    9687 GB |    9685 GB |\n",
            "|       from large pool |    1878 MB |    1878 MB |    9180 GB |    9178 GB |\n",
            "|       from small pool |       7 MB |      16 MB |     506 GB |     506 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Allocations           |     592    |     595    |    4468 K  |    4467 K  |\n",
            "|       from large pool |     150    |     154    |    1398 K  |    1398 K  |\n",
            "|       from small pool |     442    |     516    |    3069 K  |    3069 K  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Active allocs         |     592    |     595    |    4468 K  |    4467 K  |\n",
            "|       from large pool |     150    |     154    |    1398 K  |    1398 K  |\n",
            "|       from small pool |     442    |     516    |    3069 K  |    3069 K  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| GPU reserved segments |      75    |     108    |    2379    |    2304    |\n",
            "|       from large pool |      43    |      43    |     211    |     168    |\n",
            "|       from small pool |      32    |      65    |    2168    |    2136    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Non-releasable allocs |      59    |      59    |    2032 K  |    2032 K  |\n",
            "|       from large pool |      26    |      26    |     891 K  |     891 K  |\n",
            "|       from small pool |      33    |      41    |    1141 K  |    1141 K  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Oversize allocations  |       0    |       0    |       0    |       0    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Oversize GPU segments |       0    |       0    |       0    |       0    |\n",
            "|===========================================================================|\n",
            "\n",
            "2022-03-10 06:28:29 | WARNING | fairseq.trainer | attempting to recover from OOM in forward/backward pass\n",
            "epoch 006:  89% 618/692 [02:17<00:10,  6.85it/s, loss=3.781, ppl=13.75, wps=7703.9, ups=3.61, wpb=2132, bsz=128, num_updates=4000, lr=0.0005, gnorm=1.25, train_wall=25, gb_free=10.3, wall=918]2022-03-10 06:28:41 | WARNING | fairseq.trainer | OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 1.34 GiB (GPU 0; 11.17 GiB total capacity; 7.01 GiB already allocated; 1.33 GiB free; 9.32 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
            "2022-03-10 06:28:41 | WARNING | fairseq.trainer | |===========================================================================|\n",
            "|                  PyTorch CUDA memory summary, device ID 0                 |\n",
            "|---------------------------------------------------------------------------|\n",
            "|            CUDA OOMs: 46           |        cudaMalloc retries: 81        |\n",
            "|===========================================================================|\n",
            "|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Allocated memory      |    5814 MB |    7180 MB |    8641 GB |    8635 GB |\n",
            "|       from large pool |    5766 MB |    7132 MB |    8166 GB |    8160 GB |\n",
            "|       from small pool |      48 MB |      60 MB |     474 GB |     474 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Active memory         |    5814 MB |    7180 MB |    8641 GB |    8635 GB |\n",
            "|       from large pool |    5766 MB |    7132 MB |    8166 GB |    8160 GB |\n",
            "|       from small pool |      48 MB |      60 MB |     474 GB |     474 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| GPU reserved memory   |    9546 MB |    9616 MB |  125672 MB |  116126 MB |\n",
            "|       from large pool |    9484 MB |    9484 MB |  121268 MB |  111784 MB |\n",
            "|       from small pool |      62 MB |     132 MB |    4404 MB |    4342 MB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Non-releasable memory |    2363 MB |    2365 MB |    9820 GB |    9818 GB |\n",
            "|       from large pool |    2349 MB |    2351 MB |    9306 GB |    9304 GB |\n",
            "|       from small pool |      13 MB |      25 MB |     513 GB |     513 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Allocations           |     592    |     595    |    4532 K  |    4531 K  |\n",
            "|       from large pool |     160    |     164    |    1418 K  |    1418 K  |\n",
            "|       from small pool |     432    |     516    |    3113 K  |    3113 K  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Active allocs         |     592    |     595    |    4532 K  |    4531 K  |\n",
            "|       from large pool |     160    |     164    |    1418 K  |    1418 K  |\n",
            "|       from small pool |     432    |     516    |    3113 K  |    3113 K  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| GPU reserved segments |      74    |     109    |    2414    |    2340    |\n",
            "|       from large pool |      43    |      43    |     212    |     169    |\n",
            "|       from small pool |      31    |      66    |    2202    |    2171    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Non-releasable allocs |      61    |      63    |    2062 K  |    2061 K  |\n",
            "|       from large pool |      27    |      28    |     904 K  |     904 K  |\n",
            "|       from small pool |      34    |      59    |    1157 K  |    1157 K  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Oversize allocations  |       0    |       0    |       0    |       0    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Oversize GPU segments |       0    |       0    |       0    |       0    |\n",
            "|===========================================================================|\n",
            "\n",
            "2022-03-10 06:28:41 | WARNING | fairseq.trainer | attempting to recover from OOM in forward/backward pass\n",
            "epoch 006: 100% 691/692 [02:30<00:00,  5.65it/s, loss=3.51, ppl=11.4, wps=9169.4, ups=5.19, wpb=1768, bsz=128, num_updates=4100, lr=0.0005, gnorm=1.351, train_wall=18, gb_free=9.8, wall=937]2022-03-10 06:28:55 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "\n",
            "epoch 006 | valid on 'valid' subset:   0% 0/8 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 006 | valid on 'valid' subset:  12% 1/8 [00:00<00:01,  6.92it/s]\u001b[A\n",
            "epoch 006 | valid on 'valid' subset:  25% 2/8 [00:00<00:00,  8.04it/s]\u001b[A\n",
            "epoch 006 | valid on 'valid' subset:  38% 3/8 [00:00<00:00,  8.72it/s]\u001b[A\n",
            "epoch 006 | valid on 'valid' subset:  62% 5/8 [00:00<00:00,  9.53it/s]\u001b[A\n",
            "epoch 006 | valid on 'valid' subset:  75% 6/8 [00:00<00:00,  9.34it/s]\u001b[A\n",
            "epoch 006 | valid on 'valid' subset:  88% 7/8 [00:00<00:00,  8.92it/s]\u001b[A\n",
            "epoch 006 | valid on 'valid' subset: 100% 8/8 [00:01<00:00,  5.04it/s]\u001b[A\n",
            "                                                                      \u001b[A2022-03-10 06:28:56 | INFO | valid | epoch 006 | valid on 'valid' subset | loss 4.99 | ppl 31.78 | wps 13971.3 | wpb 1992.9 | bsz 125 | num_updates 4106 | best_loss 4.912\n",
            "2022-03-10 06:28:56 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 6 @ 4106 updates\n",
            "2022-03-10 06:28:56 | INFO | fairseq.trainer | Saving checkpoint to /content/checkpoints/checkpoint6.pt\n",
            "2022-03-10 06:28:57 | INFO | fairseq.trainer | Finished saving checkpoint to /content/checkpoints/checkpoint6.pt\n",
            "2022-03-10 06:28:57 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/checkpoint6.pt (epoch 6 @ 4106 updates, score 4.99) (writing took 1.4064550800001143 seconds)\n",
            "2022-03-10 06:28:57 | INFO | fairseq_cli.train | end of epoch 6 (average epoch stats below)\n",
            "2022-03-10 06:28:57 | INFO | train | epoch 006 | loss 3.534 | ppl 11.59 | wps 8525.2 | ups 4.44 | wpb 1921.7 | bsz 127.9 | num_updates 4106 | lr 0.0005 | gnorm 1.257 | train_wall 141 | gb_free 10.6 | wall 941\n",
            "2022-03-10 06:28:57 | INFO | fairseq.data.iterators | grouped total_num_itrs = 692\n",
            "epoch 007:   0% 0/692 [00:00<?, ?it/s]2022-03-10 06:28:57 | INFO | fairseq.trainer | begin training epoch 7\n",
            "2022-03-10 06:28:57 | INFO | fairseq_cli.train | Start iterating over samples\n",
            "epoch 007:   1% 9/692 [00:01<01:40,  6.82it/s]2022-03-10 06:29:00 | WARNING | fairseq.trainer | OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 2.11 GiB (GPU 0; 11.17 GiB total capacity; 7.85 GiB already allocated; 2.07 GiB free; 8.59 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
            "2022-03-10 06:29:00 | WARNING | fairseq.trainer | |===========================================================================|\n",
            "|                  PyTorch CUDA memory summary, device ID 0                 |\n",
            "|---------------------------------------------------------------------------|\n",
            "|            CUDA OOMs: 47           |        cudaMalloc retries: 83        |\n",
            "|===========================================================================|\n",
            "|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Allocated memory      |    8033 MB |    8043 MB |    8793 GB |    8785 GB |\n",
            "|       from large pool |    7984 MB |    7994 MB |    8307 GB |    8300 GB |\n",
            "|       from small pool |      49 MB |      60 MB |     485 GB |     485 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Active memory         |    8033 MB |    8043 MB |    8793 GB |    8785 GB |\n",
            "|       from large pool |    7984 MB |    7994 MB |    8307 GB |    8300 GB |\n",
            "|       from small pool |      49 MB |      60 MB |     485 GB |     485 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| GPU reserved memory   |    8792 MB |   10550 MB |  130208 MB |  121416 MB |\n",
            "|       from large pool |    8732 MB |   10412 MB |  125728 MB |  116996 MB |\n",
            "|       from small pool |      60 MB |     138 MB |    4480 MB |    4420 MB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Non-releasable memory |  776922 KB |    1500 MB |    9994 GB |    9994 GB |\n",
            "|       from large pool |  765929 KB |    1489 MB |    9469 GB |    9468 GB |\n",
            "|       from small pool |   10993 KB |      37 MB |     525 GB |     525 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Allocations           |     592    |     595    |    4625 K  |    4624 K  |\n",
            "|       from large pool |     162    |     164    |    1445 K  |    1445 K  |\n",
            "|       from small pool |     430    |     516    |    3179 K  |    3179 K  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Active allocs         |     592    |     595    |    4625 K  |    4624 K  |\n",
            "|       from large pool |     162    |     164    |    1445 K  |    1445 K  |\n",
            "|       from small pool |     430    |     516    |    3179 K  |    3179 K  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| GPU reserved segments |      73    |     113    |    2455    |    2382    |\n",
            "|       from large pool |      43    |      44    |     215    |     172    |\n",
            "|       from small pool |      30    |      69    |    2240    |    2210    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Non-releasable allocs |      64    |      65    |    2104 K  |    2103 K  |\n",
            "|       from large pool |      31    |      32    |     921 K  |     921 K  |\n",
            "|       from small pool |      33    |      55    |    1182 K  |    1182 K  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Oversize allocations  |       0    |       0    |       0    |       0    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Oversize GPU segments |       0    |       0    |       0    |       0    |\n",
            "|===========================================================================|\n",
            "\n",
            "2022-03-10 06:29:00 | WARNING | fairseq.trainer | attempting to recover from OOM in forward/backward pass\n",
            "epoch 007:   3% 24/692 [00:06<02:40,  4.16it/s]2022-03-10 06:29:04 | WARNING | fairseq.trainer | OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 2.01 GiB (GPU 0; 11.17 GiB total capacity; 9.79 GiB already allocated; 53.81 MiB free; 10.60 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
            "2022-03-10 06:29:04 | WARNING | fairseq.trainer | |===========================================================================|\n",
            "|                  PyTorch CUDA memory summary, device ID 0                 |\n",
            "|---------------------------------------------------------------------------|\n",
            "|            CUDA OOMs: 48           |        cudaMalloc retries: 85        |\n",
            "|===========================================================================|\n",
            "|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Allocated memory      |   10024 MB |   10098 MB |    8835 GB |    8825 GB |\n",
            "|       from large pool |    9974 MB |   10049 MB |    8347 GB |    8337 GB |\n",
            "|       from small pool |      49 MB |      60 MB |     487 GB |     487 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Active memory         |   10024 MB |   10098 MB |    8835 GB |    8825 GB |\n",
            "|       from large pool |    9974 MB |   10049 MB |    8347 GB |    8337 GB |\n",
            "|       from small pool |      49 MB |      60 MB |     487 GB |     487 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| GPU reserved memory   |   10854 MB |   10854 MB |  132344 MB |  121490 MB |\n",
            "|       from large pool |   10794 MB |   10794 MB |  127790 MB |  116996 MB |\n",
            "|       from small pool |      60 MB |     134 MB |    4554 MB |    4494 MB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Non-releasable memory |     829 MB |    1832 MB |   10038 GB |   10037 GB |\n",
            "|       from large pool |     819 MB |    1819 MB |    9510 GB |    9509 GB |\n",
            "|       from small pool |      10 MB |      17 MB |     527 GB |     527 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Allocations           |     593    |     595    |    4641 K  |    4640 K  |\n",
            "|       from large pool |     160    |     164    |    1450 K  |    1450 K  |\n",
            "|       from small pool |     433    |     516    |    3190 K  |    3190 K  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Active allocs         |     593    |     595    |    4641 K  |    4640 K  |\n",
            "|       from large pool |     160    |     164    |    1450 K  |    1450 K  |\n",
            "|       from small pool |     433    |     516    |    3190 K  |    3190 K  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| GPU reserved segments |      74    |     110    |    2493    |    2419    |\n",
            "|       from large pool |      44    |      44    |     216    |     172    |\n",
            "|       from small pool |      30    |      67    |    2277    |    2247    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Non-releasable allocs |      68    |      68    |    2111 K  |    2111 K  |\n",
            "|       from large pool |      37    |      37    |     924 K  |     924 K  |\n",
            "|       from small pool |      31    |      41    |    1186 K  |    1186 K  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Oversize allocations  |       0    |       0    |       0    |       0    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Oversize GPU segments |       0    |       0    |       0    |       0    |\n",
            "|===========================================================================|\n",
            "\n",
            "2022-03-10 06:29:04 | WARNING | fairseq.trainer | attempting to recover from OOM in forward/backward pass\n",
            "epoch 007:  14% 95/692 [00:24<02:01,  4.91it/s]2022-03-10 06:29:23 | WARNING | fairseq.trainer | OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 1.74 GiB (GPU 0; 11.17 GiB total capacity; 8.31 GiB already allocated; 731.81 MiB free; 9.94 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
            "2022-03-10 06:29:23 | WARNING | fairseq.trainer | |===========================================================================|\n",
            "|                  PyTorch CUDA memory summary, device ID 0                 |\n",
            "|---------------------------------------------------------------------------|\n",
            "|            CUDA OOMs: 49           |        cudaMalloc retries: 87        |\n",
            "|===========================================================================|\n",
            "|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Allocated memory      |    8510 MB |    8575 MB |    9015 GB |    9006 GB |\n",
            "|       from large pool |    8461 MB |    8526 MB |    8519 GB |    8511 GB |\n",
            "|       from small pool |      48 MB |      60 MB |     496 GB |     495 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Active memory         |    8510 MB |    8575 MB |    9015 GB |    9006 GB |\n",
            "|       from large pool |    8461 MB |    8526 MB |    8519 GB |    8511 GB |\n",
            "|       from small pool |      48 MB |      60 MB |     496 GB |     495 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| GPU reserved memory   |   10176 MB |   10248 MB |  135120 MB |  124944 MB |\n",
            "|       from large pool |   10116 MB |   10116 MB |  130494 MB |  120378 MB |\n",
            "|       from small pool |      60 MB |     132 MB |    4626 MB |    4566 MB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Non-releasable memory |    1665 MB |    1665 MB |   10244 GB |   10243 GB |\n",
            "|       from large pool |    1654 MB |    1654 MB |    9708 GB |    9706 GB |\n",
            "|       from small pool |      11 MB |      22 MB |     536 GB |     536 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Allocations           |     593    |     595    |    4719 K  |    4719 K  |\n",
            "|       from large pool |     160    |     164    |    1475 K  |    1475 K  |\n",
            "|       from small pool |     433    |     516    |    3244 K  |    3243 K  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Active allocs         |     593    |     595    |    4719 K  |    4719 K  |\n",
            "|       from large pool |     160    |     164    |    1475 K  |    1475 K  |\n",
            "|       from small pool |     433    |     516    |    3244 K  |    3243 K  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| GPU reserved segments |      73    |     109    |    2531    |    2458    |\n",
            "|       from large pool |      43    |      43    |     218    |     175    |\n",
            "|       from small pool |      30    |      66    |    2313    |    2283    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Non-releasable allocs |      65    |      67    |    2146 K  |    2146 K  |\n",
            "|       from large pool |      33    |      33    |     940 K  |     940 K  |\n",
            "|       from small pool |      32    |      50    |    1206 K  |    1206 K  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Oversize allocations  |       0    |       0    |       0    |       0    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Oversize GPU segments |       0    |       0    |       0    |       0    |\n",
            "|===========================================================================|\n",
            "\n",
            "2022-03-10 06:29:23 | WARNING | fairseq.trainer | attempting to recover from OOM in forward/backward pass\n",
            "epoch 007:  26% 181/692 [00:41<02:11,  3.88it/s, loss=3.201, ppl=9.2, wps=6638.6, ups=3.45, wpb=1922.4, bsz=128, num_updates=4200, lr=0.0005, gnorm=1.221, train_wall=23, gb_free=2.8, wall=966]2022-03-10 06:29:40 | WARNING | fairseq.trainer | OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 1.69 GiB (GPU 0; 11.17 GiB total capacity; 9.21 GiB already allocated; 349.81 MiB free; 10.31 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
            "2022-03-10 06:29:40 | WARNING | fairseq.trainer | |===========================================================================|\n",
            "|                  PyTorch CUDA memory summary, device ID 0                 |\n",
            "|---------------------------------------------------------------------------|\n",
            "|            CUDA OOMs: 50           |        cudaMalloc retries: 89        |\n",
            "|===========================================================================|\n",
            "|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Allocated memory      |    7701 MB |    9435 MB |    9172 GB |    9164 GB |\n",
            "|       from large pool |    7652 MB |    9386 MB |    8666 GB |    8659 GB |\n",
            "|       from small pool |      48 MB |      60 MB |     505 GB |     505 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Active memory         |    7701 MB |    9435 MB |    9172 GB |    9164 GB |\n",
            "|       from large pool |    7652 MB |    9386 MB |    8666 GB |    8659 GB |\n",
            "|       from small pool |      48 MB |      60 MB |     505 GB |     505 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| GPU reserved memory   |   10558 MB |   10558 MB |  136932 MB |  126374 MB |\n",
            "|       from large pool |   10498 MB |   10498 MB |  132228 MB |  121730 MB |\n",
            "|       from small pool |      60 MB |     138 MB |    4704 MB |    4644 MB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Non-releasable memory |    1122 MB |    1557 MB |   10437 GB |   10436 GB |\n",
            "|       from large pool |    1111 MB |    1545 MB |    9890 GB |    9889 GB |\n",
            "|       from small pool |      11 MB |      14 MB |     547 GB |     547 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Allocations           |     592    |     595    |    4815 K  |    4814 K  |\n",
            "|       from large pool |     160    |     164    |    1505 K  |    1505 K  |\n",
            "|       from small pool |     432    |     516    |    3309 K  |    3309 K  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Active allocs         |     592    |     595    |    4815 K  |    4814 K  |\n",
            "|       from large pool |     160    |     164    |    1505 K  |    1505 K  |\n",
            "|       from small pool |     432    |     516    |    3309 K  |    3309 K  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| GPU reserved segments |      73    |     112    |    2571    |    2498    |\n",
            "|       from large pool |      43    |      43    |     219    |     176    |\n",
            "|       from small pool |      30    |      69    |    2352    |    2322    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Non-releasable allocs |      60    |      60    |    2189 K  |    2189 K  |\n",
            "|       from large pool |      28    |      28    |     959 K  |     959 K  |\n",
            "|       from small pool |      32    |      34    |    1230 K  |    1230 K  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Oversize allocations  |       0    |       0    |       0    |       0    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Oversize GPU segments |       0    |       0    |       0    |       0    |\n",
            "|===========================================================================|\n",
            "\n",
            "2022-03-10 06:29:40 | WARNING | fairseq.trainer | attempting to recover from OOM in forward/backward pass\n",
            "epoch 007:  38% 260/692 [00:58<01:10,  6.09it/s, loss=3.091, ppl=8.52, wps=9332.2, ups=4.96, wpb=1880, bsz=128, num_updates=4300, lr=0.0005, gnorm=1.261, train_wall=19, gb_free=10.6, wall=987]2022-03-10 06:29:57 | WARNING | fairseq.trainer | OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 1.46 GiB (GPU 0; 11.17 GiB total capacity; 7.60 GiB already allocated; 587.81 MiB free; 10.08 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
            "2022-03-10 06:29:57 | WARNING | fairseq.trainer | |===========================================================================|\n",
            "|                  PyTorch CUDA memory summary, device ID 0                 |\n",
            "|---------------------------------------------------------------------------|\n",
            "|            CUDA OOMs: 51           |        cudaMalloc retries: 90        |\n",
            "|===========================================================================|\n",
            "|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Allocated memory      |    6293 MB |    7785 MB |    9326 GB |    9319 GB |\n",
            "|       from large pool |    6237 MB |    7729 MB |    8811 GB |    8805 GB |\n",
            "|       from small pool |      56 MB |      60 MB |     514 GB |     514 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Active memory         |    6293 MB |    7785 MB |    9326 GB |    9319 GB |\n",
            "|       from large pool |    6237 MB |    7729 MB |    8811 GB |    8805 GB |\n",
            "|       from small pool |      56 MB |      60 MB |     514 GB |     514 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| GPU reserved memory   |   10320 MB |   10394 MB |  138502 MB |  128182 MB |\n",
            "|       from large pool |   10256 MB |   10256 MB |  133720 MB |  123464 MB |\n",
            "|       from small pool |      64 MB |     138 MB |    4782 MB |    4718 MB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Non-releasable memory |    2534 MB |    2534 MB |   10620 GB |   10618 GB |\n",
            "|       from large pool |    2526 MB |    2526 MB |   10064 GB |   10061 GB |\n",
            "|       from small pool |       7 MB |      22 MB |     556 GB |     556 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Allocations           |     592    |     595    |    4902 K  |    4902 K  |\n",
            "|       from large pool |     150    |     154    |    1533 K  |    1533 K  |\n",
            "|       from small pool |     442    |     516    |    3369 K  |    3368 K  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Active allocs         |     592    |     595    |    4902 K  |    4902 K  |\n",
            "|       from large pool |     150    |     154    |    1533 K  |    1533 K  |\n",
            "|       from small pool |     442    |     516    |    3369 K  |    3368 K  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| GPU reserved segments |      75    |     112    |    2611    |    2536    |\n",
            "|       from large pool |      43    |      43    |     220    |     177    |\n",
            "|       from small pool |      32    |      69    |    2391    |    2359    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Non-releasable allocs |      60    |      60    |    2229 K  |    2229 K  |\n",
            "|       from large pool |      26    |      26    |     977 K  |     977 K  |\n",
            "|       from small pool |      34    |      49    |    1251 K  |    1251 K  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Oversize allocations  |       0    |       0    |       0    |       0    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Oversize GPU segments |       0    |       0    |       0    |       0    |\n",
            "|===========================================================================|\n",
            "\n",
            "2022-03-10 06:29:57 | WARNING | fairseq.trainer | attempting to recover from OOM in forward/backward pass\n",
            "epoch 007:  45% 311/692 [01:09<00:59,  6.44it/s, loss=3.284, ppl=9.74, wps=8973.6, ups=4.52, wpb=1986.5, bsz=128, num_updates=4400, lr=0.0005, gnorm=1.277, train_wall=21, gb_free=10.8, wall=1009]2022-03-10 06:30:07 | WARNING | fairseq.trainer | OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 1.25 GiB (GPU 0; 11.17 GiB total capacity; 7.23 GiB already allocated; 895.81 MiB free; 9.78 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
            "2022-03-10 06:30:07 | WARNING | fairseq.trainer | |===========================================================================|\n",
            "|                  PyTorch CUDA memory summary, device ID 0                 |\n",
            "|---------------------------------------------------------------------------|\n",
            "|            CUDA OOMs: 52           |        cudaMalloc retries: 91        |\n",
            "|===========================================================================|\n",
            "|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Allocated memory      |    6123 MB |    7404 MB |    9427 GB |    9421 GB |\n",
            "|       from large pool |    6075 MB |    7356 MB |    8906 GB |    8900 GB |\n",
            "|       from small pool |      48 MB |      60 MB |     521 GB |     521 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Active memory         |    6123 MB |    7404 MB |    9427 GB |    9421 GB |\n",
            "|       from large pool |    6075 MB |    7356 MB |    8906 GB |    8900 GB |\n",
            "|       from small pool |      48 MB |      60 MB |     521 GB |     521 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| GPU reserved memory   |   10012 MB |   10084 MB |  139758 MB |  129746 MB |\n",
            "|       from large pool |    9952 MB |    9952 MB |  134908 MB |  124956 MB |\n",
            "|       from small pool |      60 MB |     132 MB |    4850 MB |    4790 MB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Non-releasable memory |    1724 MB |    2607 MB |   10735 GB |   10733 GB |\n",
            "|       from large pool |    1712 MB |    2595 MB |   10171 GB |   10169 GB |\n",
            "|       from small pool |      11 MB |      17 MB |     563 GB |     563 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Allocations           |     592    |     595    |    4958 K  |    4958 K  |\n",
            "|       from large pool |     160    |     164    |    1549 K  |    1549 K  |\n",
            "|       from small pool |     432    |     516    |    3409 K  |    3408 K  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Active allocs         |     592    |     595    |    4958 K  |    4958 K  |\n",
            "|       from large pool |     160    |     164    |    1549 K  |    1549 K  |\n",
            "|       from small pool |     432    |     516    |    3409 K  |    3408 K  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| GPU reserved segments |      73    |     109    |    2646    |    2573    |\n",
            "|       from large pool |      43    |      43    |     221    |     178    |\n",
            "|       from small pool |      30    |      66    |    2425    |    2395    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Non-releasable allocs |      69    |      70    |    2254 K  |    2254 K  |\n",
            "|       from large pool |      35    |      36    |     987 K  |     987 K  |\n",
            "|       from small pool |      34    |      39    |    1267 K  |    1267 K  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Oversize allocations  |       0    |       0    |       0    |       0    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Oversize GPU segments |       0    |       0    |       0    |       0    |\n",
            "|===========================================================================|\n",
            "\n",
            "2022-03-10 06:30:07 | WARNING | fairseq.trainer | attempting to recover from OOM in forward/backward pass\n",
            "epoch 007:  51% 356/692 [01:19<00:59,  5.68it/s, loss=3.284, ppl=9.74, wps=8973.6, ups=4.52, wpb=1986.5, bsz=128, num_updates=4400, lr=0.0005, gnorm=1.277, train_wall=21, gb_free=10.8, wall=1009]2022-03-10 06:30:18 | WARNING | fairseq.trainer | OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 1.46 GiB (GPU 0; 11.17 GiB total capacity; 9.37 GiB already allocated; 91.81 MiB free; 10.56 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
            "2022-03-10 06:30:18 | WARNING | fairseq.trainer | |===========================================================================|\n",
            "|                  PyTorch CUDA memory summary, device ID 0                 |\n",
            "|---------------------------------------------------------------------------|\n",
            "|            CUDA OOMs: 53           |        cudaMalloc retries: 92        |\n",
            "|===========================================================================|\n",
            "|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Allocated memory      |    8104 MB |    9596 MB |    9528 GB |    9520 GB |\n",
            "|       from large pool |    8055 MB |    9547 MB |    9002 GB |    8994 GB |\n",
            "|       from small pool |      48 MB |      60 MB |     525 GB |     525 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Active memory         |    8104 MB |    9596 MB |    9528 GB |    9520 GB |\n",
            "|       from large pool |    8055 MB |    9547 MB |    9002 GB |    8994 GB |\n",
            "|       from small pool |      48 MB |      60 MB |     525 GB |     525 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| GPU reserved memory   |   10816 MB |   10900 MB |  142810 MB |  131994 MB |\n",
            "|       from large pool |   10756 MB |   10772 MB |  137892 MB |  127136 MB |\n",
            "|       from small pool |      60 MB |     128 MB |    4918 MB |    4858 MB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Non-releasable memory |    1219 MB |    1409 MB |   10854 GB |   10853 GB |\n",
            "|       from large pool |    1208 MB |    1397 MB |   10285 GB |   10284 GB |\n",
            "|       from small pool |      11 MB |      37 MB |     569 GB |     569 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Allocations           |     592    |     595    |    5008 K  |    5007 K  |\n",
            "|       from large pool |     160    |     164    |    1565 K  |    1565 K  |\n",
            "|       from small pool |     432    |     516    |    3442 K  |    3442 K  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Active allocs         |     592    |     595    |    5008 K  |    5007 K  |\n",
            "|       from large pool |     160    |     164    |    1565 K  |    1565 K  |\n",
            "|       from small pool |     432    |     516    |    3442 K  |    3442 K  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| GPU reserved segments |      73    |     108    |    2682    |    2609    |\n",
            "|       from large pool |      43    |      44    |     223    |     180    |\n",
            "|       from small pool |      30    |      64    |    2459    |    2429    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Non-releasable allocs |      69    |      69    |    2276 K  |    2276 K  |\n",
            "|       from large pool |      35    |      35    |     997 K  |     997 K  |\n",
            "|       from small pool |      34    |      56    |    1278 K  |    1278 K  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Oversize allocations  |       0    |       0    |       0    |       0    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Oversize GPU segments |       0    |       0    |       0    |       0    |\n",
            "|===========================================================================|\n",
            "\n",
            "2022-03-10 06:30:18 | WARNING | fairseq.trainer | attempting to recover from OOM in forward/backward pass\n",
            "epoch 007:  60% 418/692 [01:34<00:56,  4.85it/s, loss=3.245, ppl=9.48, wps=8332.6, ups=4.33, wpb=1925.4, bsz=128, num_updates=4500, lr=0.0005, gnorm=1.311, train_wall=21, gb_free=10.7, wall=1032]2022-03-10 06:30:33 | WARNING | fairseq.trainer | OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 2.50 GiB (GPU 0; 11.17 GiB total capacity; 8.45 GiB already allocated; 199.81 MiB free; 10.46 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
            "2022-03-10 06:30:33 | WARNING | fairseq.trainer | |===========================================================================|\n",
            "|                  PyTorch CUDA memory summary, device ID 0                 |\n",
            "|---------------------------------------------------------------------------|\n",
            "|            CUDA OOMs: 54           |        cudaMalloc retries: 94        |\n",
            "|===========================================================================|\n",
            "|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Allocated memory      |    8610 MB |    8651 MB |    9672 GB |    9663 GB |\n",
            "|       from large pool |    8560 MB |    8601 MB |    9138 GB |    9130 GB |\n",
            "|       from small pool |      50 MB |      60 MB |     533 GB |     533 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Active memory         |    8610 MB |    8651 MB |    9672 GB |    9663 GB |\n",
            "|       from large pool |    8560 MB |    8601 MB |    9138 GB |    9130 GB |\n",
            "|       from small pool |      50 MB |      60 MB |     533 GB |     533 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| GPU reserved memory   |   10708 MB |   10782 MB |  145664 MB |  134956 MB |\n",
            "|       from large pool |   10648 MB |   10648 MB |  140628 MB |  129980 MB |\n",
            "|       from small pool |      60 MB |     134 MB |    5036 MB |    4976 MB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Non-releasable memory |    2097 MB |    2265 MB |   11019 GB |   11017 GB |\n",
            "|       from large pool |    2087 MB |    2255 MB |   10442 GB |   10440 GB |\n",
            "|       from small pool |       9 MB |      19 MB |     577 GB |     577 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Allocations           |     593    |     595    |    5076 K  |    5075 K  |\n",
            "|       from large pool |     162    |     163    |    1586 K  |    1586 K  |\n",
            "|       from small pool |     431    |     516    |    3489 K  |    3489 K  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Active allocs         |     593    |     595    |    5076 K  |    5075 K  |\n",
            "|       from large pool |     162    |     163    |    1586 K  |    1586 K  |\n",
            "|       from small pool |     431    |     516    |    3489 K  |    3489 K  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| GPU reserved segments |      73    |     110    |    2743    |    2670    |\n",
            "|       from large pool |      43    |      43    |     225    |     182    |\n",
            "|       from small pool |      30    |      67    |    2518    |    2488    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Non-releasable allocs |      65    |      65    |    2307 K  |    2307 K  |\n",
            "|       from large pool |      31    |      31    |    1011 K  |    1011 K  |\n",
            "|       from small pool |      34    |      42    |    1296 K  |    1296 K  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Oversize allocations  |       0    |       0    |       0    |       0    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Oversize GPU segments |       0    |       0    |       0    |       0    |\n",
            "|===========================================================================|\n",
            "\n",
            "2022-03-10 06:30:33 | WARNING | fairseq.trainer | attempting to recover from OOM in forward/backward pass\n",
            "epoch 007: 100% 691/692 [02:33<00:00,  6.11it/s, loss=3.471, ppl=11.09, wps=8577.5, ups=4.31, wpb=1988.5, bsz=128, num_updates=4700, lr=0.0005, gnorm=1.355, train_wall=23, gb_free=10.1, wall=1078]2022-03-10 06:31:31 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "\n",
            "epoch 007 | valid on 'valid' subset:   0% 0/8 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 007 | valid on 'valid' subset:  12% 1/8 [00:00<00:00,  7.89it/s]\u001b[A\n",
            "epoch 007 | valid on 'valid' subset:  25% 2/8 [00:00<00:00,  8.66it/s]\u001b[A\n",
            "epoch 007 | valid on 'valid' subset:  50% 4/8 [00:00<00:00, 10.25it/s]\u001b[A\n",
            "epoch 007 | valid on 'valid' subset:  62% 5/8 [00:00<00:00,  9.75it/s]\u001b[A\n",
            "epoch 007 | valid on 'valid' subset:  75% 6/8 [00:00<00:00,  9.42it/s]\u001b[A\n",
            "epoch 007 | valid on 'valid' subset:  88% 7/8 [00:00<00:00,  8.98it/s]\u001b[A\n",
            "epoch 007 | valid on 'valid' subset: 100% 8/8 [00:01<00:00,  4.99it/s]\u001b[A\n",
            "                                                                      \u001b[A2022-03-10 06:31:32 | INFO | valid | epoch 007 | valid on 'valid' subset | loss 5.043 | ppl 32.97 | wps 14217.2 | wpb 1992.9 | bsz 125 | num_updates 4790 | best_loss 4.912\n",
            "2022-03-10 06:31:32 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 7 @ 4790 updates\n",
            "2022-03-10 06:31:32 | INFO | fairseq.trainer | Saving checkpoint to /content/checkpoints/checkpoint7.pt\n",
            "2022-03-10 06:31:32 | INFO | fairseq.trainer | Finished saving checkpoint to /content/checkpoints/checkpoint7.pt\n",
            "2022-03-10 06:31:33 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/checkpoint7.pt (epoch 7 @ 4790 updates, score 5.043) (writing took 1.295060415999842 seconds)\n",
            "2022-03-10 06:31:33 | INFO | fairseq_cli.train | end of epoch 7 (average epoch stats below)\n",
            "2022-03-10 06:31:33 | INFO | train | epoch 007 | loss 3.283 | ppl 9.73 | wps 8486.3 | ups 4.39 | wpb 1931.4 | bsz 127.9 | num_updates 4790 | lr 0.0005 | gnorm 1.298 | train_wall 144 | gb_free 10.5 | wall 1097\n",
            "2022-03-10 06:31:33 | INFO | fairseq.data.iterators | grouped total_num_itrs = 692\n",
            "epoch 008:   0% 0/692 [00:00<?, ?it/s]2022-03-10 06:31:33 | INFO | fairseq.trainer | begin training epoch 8\n",
            "2022-03-10 06:31:33 | INFO | fairseq_cli.train | Start iterating over samples\n",
            "epoch 008:   4% 30/692 [00:05<01:39,  6.64it/s, loss=3.126, ppl=8.73, wps=8566.9, ups=4.74, wpb=1808, bsz=128, num_updates=4800, lr=0.0005, gnorm=1.325, train_wall=18, gb_free=10.3, wall=1099]2022-03-10 06:31:40 | WARNING | fairseq.trainer | OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 2.01 GiB (GPU 0; 11.17 GiB total capacity; 7.85 GiB already allocated; 1.65 GiB free; 9.00 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
            "2022-03-10 06:31:40 | WARNING | fairseq.trainer | |===========================================================================|\n",
            "|                  PyTorch CUDA memory summary, device ID 0                 |\n",
            "|---------------------------------------------------------------------------|\n",
            "|            CUDA OOMs: 55           |        cudaMalloc retries: 95        |\n",
            "|===========================================================================|\n",
            "|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Allocated memory      |    8038 MB |    8054 MB |   10289 GB |   10281 GB |\n",
            "|       from large pool |    7988 MB |    8005 MB |    9721 GB |    9713 GB |\n",
            "|       from small pool |      49 MB |      60 MB |     568 GB |     568 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Active memory         |    8038 MB |    8054 MB |   10289 GB |   10281 GB |\n",
            "|       from large pool |    7988 MB |    8005 MB |    9721 GB |    9713 GB |\n",
            "|       from small pool |      49 MB |      60 MB |     568 GB |     568 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| GPU reserved memory   |    9218 MB |   10786 MB |  145742 MB |  136524 MB |\n",
            "|       from large pool |    9156 MB |   10648 MB |  140628 MB |  131472 MB |\n",
            "|       from small pool |      62 MB |     138 MB |    5114 MB |    5052 MB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Non-releasable memory |    1179 MB |    1814 MB |   11753 GB |   11752 GB |\n",
            "|       from large pool |    1167 MB |    1800 MB |   11138 GB |   11137 GB |\n",
            "|       from small pool |      12 MB |      17 MB |     615 GB |     615 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Allocations           |     592    |     595    |    5417 K  |    5417 K  |\n",
            "|       from large pool |     162    |     164    |    1694 K  |    1694 K  |\n",
            "|       from small pool |     430    |     516    |    3722 K  |    3722 K  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Active allocs         |     592    |     595    |    5417 K  |    5417 K  |\n",
            "|       from large pool |     162    |     164    |    1694 K  |    1694 K  |\n",
            "|       from small pool |     430    |     516    |    3722 K  |    3722 K  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| GPU reserved segments |      73    |     112    |    2782    |    2709    |\n",
            "|       from large pool |      42    |      43    |     225    |     183    |\n",
            "|       from small pool |      31    |      69    |    2557    |    2526    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Non-releasable allocs |      69    |      69    |    2463 K  |    2463 K  |\n",
            "|       from large pool |      34    |      34    |    1079 K  |    1079 K  |\n",
            "|       from small pool |      35    |      41    |    1383 K  |    1383 K  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Oversize allocations  |       0    |       0    |       0    |       0    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Oversize GPU segments |       0    |       0    |       0    |       0    |\n",
            "|===========================================================================|\n",
            "\n",
            "2022-03-10 06:31:40 | WARNING | fairseq.trainer | attempting to recover from OOM in forward/backward pass\n",
            "epoch 008:  14% 95/692 [00:22<02:51,  3.49it/s, loss=3.126, ppl=8.73, wps=8566.9, ups=4.74, wpb=1808, bsz=128, num_updates=4800, lr=0.0005, gnorm=1.325, train_wall=18, gb_free=10.3, wall=1099]2022-03-10 06:31:57 | WARNING | fairseq.trainer | OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 1.46 GiB (GPU 0; 11.17 GiB total capacity; 7.60 GiB already allocated; 1.40 GiB free; 9.25 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
            "2022-03-10 06:31:57 | WARNING | fairseq.trainer | |===========================================================================|\n",
            "|                  PyTorch CUDA memory summary, device ID 0                 |\n",
            "|---------------------------------------------------------------------------|\n",
            "|            CUDA OOMs: 56           |        cudaMalloc retries: 97        |\n",
            "|===========================================================================|\n",
            "|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Allocated memory      |    6293 MB |    7785 MB |   10453 GB |   10447 GB |\n",
            "|       from large pool |    6236 MB |    7728 MB |    9876 GB |    9870 GB |\n",
            "|       from small pool |      56 MB |      60 MB |     576 GB |     576 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Active memory         |    6293 MB |    7785 MB |   10453 GB |   10447 GB |\n",
            "|       from large pool |    6236 MB |    7728 MB |    9876 GB |    9870 GB |\n",
            "|       from small pool |      56 MB |      60 MB |     576 GB |     576 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| GPU reserved memory   |    9470 MB |   10662 MB |  150170 MB |  140700 MB |\n",
            "|       from large pool |    9404 MB |   10524 MB |  144980 MB |  135576 MB |\n",
            "|       from small pool |      66 MB |     138 MB |    5190 MB |    5124 MB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Non-releasable memory |    1684 MB |    1684 MB |   11937 GB |   11935 GB |\n",
            "|       from large pool |    1675 MB |    1675 MB |   11313 GB |   11312 GB |\n",
            "|       from small pool |       9 MB |      25 MB |     623 GB |     623 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Allocations           |     592    |     595    |    5489 K  |    5488 K  |\n",
            "|       from large pool |     150    |     154    |    1716 K  |    1716 K  |\n",
            "|       from small pool |     442    |     516    |    3772 K  |    3772 K  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Active allocs         |     592    |     595    |    5489 K  |    5488 K  |\n",
            "|       from large pool |     150    |     154    |    1716 K  |    1716 K  |\n",
            "|       from small pool |     442    |     516    |    3772 K  |    3772 K  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| GPU reserved segments |      75    |     112    |    2823    |    2748    |\n",
            "|       from large pool |      42    |      43    |     228    |     186    |\n",
            "|       from small pool |      33    |      69    |    2595    |    2562    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Non-releasable allocs |      64    |      67    |    2495 K  |    2495 K  |\n",
            "|       from large pool |      25    |      27    |    1093 K  |    1093 K  |\n",
            "|       from small pool |      39    |      53    |    1402 K  |    1402 K  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Oversize allocations  |       0    |       0    |       0    |       0    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Oversize GPU segments |       0    |       0    |       0    |       0    |\n",
            "|===========================================================================|\n",
            "\n",
            "2022-03-10 06:31:57 | WARNING | fairseq.trainer | attempting to recover from OOM in forward/backward pass\n",
            "epoch 008:  19% 129/692 [00:29<01:29,  6.30it/s, loss=2.888, ppl=7.4, wps=7728.7, ups=4.16, wpb=1856.9, bsz=127.1, num_updates=4900, lr=0.0005, gnorm=1.256, train_wall=21, gb_free=10.4, wall=1123]2022-03-10 06:32:04 | WARNING | fairseq.trainer | OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 2.11 GiB (GPU 0; 11.17 GiB total capacity; 7.84 GiB already allocated; 1.64 GiB free; 9.01 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
            "2022-03-10 06:32:04 | WARNING | fairseq.trainer | |===========================================================================|\n",
            "|                  PyTorch CUDA memory summary, device ID 0                 |\n",
            "|---------------------------------------------------------------------------|\n",
            "|            CUDA OOMs: 57           |        cudaMalloc retries: 99        |\n",
            "|===========================================================================|\n",
            "|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Allocated memory      |    8033 MB |    8043 MB |   10515 GB |   10507 GB |\n",
            "|       from large pool |    7983 MB |    7994 MB |    9935 GB |    9927 GB |\n",
            "|       from small pool |      49 MB |      60 MB |     580 GB |     580 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Active memory         |    8033 MB |    8043 MB |   10515 GB |   10507 GB |\n",
            "|       from large pool |    7983 MB |    7994 MB |    9935 GB |    9927 GB |\n",
            "|       from small pool |      49 MB |      60 MB |     580 GB |     580 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| GPU reserved memory   |    9224 MB |    9224 MB |  153550 MB |  144326 MB |\n",
            "|       from large pool |    9162 MB |    9162 MB |  148292 MB |  139130 MB |\n",
            "|       from small pool |      62 MB |     134 MB |    5258 MB |    5196 MB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Non-releasable memory |    1190 MB |    1769 MB |   12012 GB |   12011 GB |\n",
            "|       from large pool |    1178 MB |    1756 MB |   11384 GB |   11383 GB |\n",
            "|       from small pool |      12 MB |      19 MB |     628 GB |     628 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Allocations           |     592    |     595    |    5526 K  |    5525 K  |\n",
            "|       from large pool |     162    |     164    |    1727 K  |    1727 K  |\n",
            "|       from small pool |     430    |     516    |    3798 K  |    3798 K  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Active allocs         |     592    |     595    |    5526 K  |    5525 K  |\n",
            "|       from large pool |     162    |     164    |    1727 K  |    1727 K  |\n",
            "|       from small pool |     430    |     516    |    3798 K  |    3798 K  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| GPU reserved segments |      73    |     109    |    2859    |    2786    |\n",
            "|       from large pool |      42    |      42    |     230    |     188    |\n",
            "|       from small pool |      31    |      67    |    2629    |    2598    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Non-releasable allocs |      65    |      65    |    2513 K  |    2512 K  |\n",
            "|       from large pool |      30    |      31    |    1100 K  |    1100 K  |\n",
            "|       from small pool |      35    |      42    |    1412 K  |    1412 K  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Oversize allocations  |       0    |       0    |       0    |       0    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Oversize GPU segments |       0    |       0    |       0    |       0    |\n",
            "|===========================================================================|\n",
            "\n",
            "2022-03-10 06:32:04 | WARNING | fairseq.trainer | attempting to recover from OOM in forward/backward pass\n",
            "epoch 008:  23% 156/692 [00:35<01:20,  6.66it/s, loss=2.888, ppl=7.4, wps=7728.7, ups=4.16, wpb=1856.9, bsz=127.1, num_updates=4900, lr=0.0005, gnorm=1.256, train_wall=21, gb_free=10.4, wall=1123]2022-03-10 06:32:09 | WARNING | fairseq.trainer | OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 1.69 GiB (GPU 0; 11.17 GiB total capacity; 7.52 GiB already allocated; 1.41 GiB free; 9.24 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
            "2022-03-10 06:32:09 | WARNING | fairseq.trainer | |===========================================================================|\n",
            "|                  PyTorch CUDA memory summary, device ID 0                 |\n",
            "|---------------------------------------------------------------------------|\n",
            "|            CUDA OOMs: 58           |        cudaMalloc retries: 101       |\n",
            "|===========================================================================|\n",
            "|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Allocated memory      |    7699 MB |    7759 MB |   10563 GB |   10556 GB |\n",
            "|       from large pool |    7650 MB |    7710 MB |    9979 GB |    9971 GB |\n",
            "|       from small pool |      48 MB |      60 MB |     584 GB |     584 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Active memory         |    7699 MB |    7759 MB |   10563 GB |   10556 GB |\n",
            "|       from large pool |    7650 MB |    7710 MB |    9979 GB |    9971 GB |\n",
            "|       from small pool |      48 MB |      60 MB |     584 GB |     584 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| GPU reserved memory   |    9466 MB |    9466 MB |  155352 MB |  145886 MB |\n",
            "|       from large pool |    9404 MB |    9404 MB |  150026 MB |  140622 MB |\n",
            "|       from small pool |      62 MB |     130 MB |    5326 MB |    5264 MB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Non-releasable memory |    1766 MB |    1766 MB |   12069 GB |   12067 GB |\n",
            "|       from large pool |    1753 MB |    1753 MB |   11437 GB |   11435 GB |\n",
            "|       from small pool |      13 MB |      23 MB |     632 GB |     632 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Allocations           |     593    |     595    |    5555 K  |    5554 K  |\n",
            "|       from large pool |     160    |     164    |    1735 K  |    1735 K  |\n",
            "|       from small pool |     433    |     516    |    3820 K  |    3819 K  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Active allocs         |     593    |     595    |    5555 K  |    5554 K  |\n",
            "|       from large pool |     160    |     164    |    1735 K  |    1735 K  |\n",
            "|       from small pool |     433    |     516    |    3820 K  |    3819 K  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| GPU reserved segments |      73    |     107    |    2894    |    2821    |\n",
            "|       from large pool |      42    |      42    |     231    |     189    |\n",
            "|       from small pool |      31    |      65    |    2663    |    2632    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Non-releasable allocs |      65    |      66    |    2526 K  |    2525 K  |\n",
            "|       from large pool |      32    |      32    |    1105 K  |    1105 K  |\n",
            "|       from small pool |      33    |      43    |    1420 K  |    1420 K  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Oversize allocations  |       0    |       0    |       0    |       0    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Oversize GPU segments |       0    |       0    |       0    |       0    |\n",
            "|===========================================================================|\n",
            "\n",
            "2022-03-10 06:32:09 | WARNING | fairseq.trainer | attempting to recover from OOM in forward/backward pass\n",
            "epoch 008:  58% 402/692 [01:31<00:55,  5.24it/s, loss=2.964, ppl=7.8, wps=7364.1, ups=3.98, wpb=1848.6, bsz=128, num_updates=5100, lr=0.0005, gnorm=1.354, train_wall=25, gb_free=10.4, wall=1169]2022-03-10 06:33:05 | WARNING | fairseq.trainer | OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 2.50 GiB (GPU 0; 11.17 GiB total capacity; 8.45 GiB already allocated; 1.54 GiB free; 9.11 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
            "2022-03-10 06:33:05 | WARNING | fairseq.trainer | |===========================================================================|\n",
            "|                  PyTorch CUDA memory summary, device ID 0                 |\n",
            "|---------------------------------------------------------------------------|\n",
            "|            CUDA OOMs: 59           |        cudaMalloc retries: 106       |\n",
            "|===========================================================================|\n",
            "|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Allocated memory      |    8611 MB |    8653 MB |   11072 GB |   11064 GB |\n",
            "|       from large pool |    8561 MB |    8602 MB |   10459 GB |   10450 GB |\n",
            "|       from small pool |      50 MB |      60 MB |     613 GB |     613 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Active memory         |    8611 MB |    8653 MB |   11072 GB |   11064 GB |\n",
            "|       from large pool |    8561 MB |    8602 MB |   10459 GB |   10450 GB |\n",
            "|       from small pool |      50 MB |      60 MB |     613 GB |     613 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| GPU reserved memory   |    9326 MB |    9398 MB |  171724 MB |  162398 MB |\n",
            "|       from large pool |    9260 MB |    9260 MB |  166102 MB |  156842 MB |\n",
            "|       from small pool |      66 MB |     138 MB |    5622 MB |    5556 MB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Non-releasable memory |  731148 KB |    1587 MB |   12602 GB |   12602 GB |\n",
            "|       from large pool |  715038 KB |    1571 MB |   11939 GB |   11938 GB |\n",
            "|       from small pool |   16110 KB |      23 MB |     663 GB |     663 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Allocations           |     593    |     595    |    5829 K  |    5829 K  |\n",
            "|       from large pool |     162    |     163    |    1821 K  |    1820 K  |\n",
            "|       from small pool |     431    |     516    |    4008 K  |    4008 K  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Active allocs         |     593    |     595    |    5829 K  |    5829 K  |\n",
            "|       from large pool |     162    |     163    |    1821 K  |    1820 K  |\n",
            "|       from small pool |     431    |     516    |    4008 K  |    4008 K  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| GPU reserved segments |      77    |     113    |    3076    |    2999    |\n",
            "|       from large pool |      44    |      44    |     265    |     221    |\n",
            "|       from small pool |      33    |      69    |    2811    |    2778    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Non-releasable allocs |      73    |      73    |    2651 K  |    2651 K  |\n",
            "|       from large pool |      37    |      37    |    1159 K  |    1159 K  |\n",
            "|       from small pool |      36    |      47    |    1491 K  |    1491 K  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Oversize allocations  |       0    |       0    |       0    |       0    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Oversize GPU segments |       0    |       0    |       0    |       0    |\n",
            "|===========================================================================|\n",
            "\n",
            "2022-03-10 06:33:05 | WARNING | fairseq.trainer | attempting to recover from OOM in forward/backward pass\n",
            "epoch 008:  76% 529/692 [01:59<00:22,  7.10it/s, loss=3.197, ppl=9.17, wps=9131.1, ups=4.5, wpb=2030.5, bsz=128, num_updates=5300, lr=0.0005, gnorm=1.343, train_wall=22, gb_free=5.7, wall=1214]2022-03-10 06:33:33 | WARNING | fairseq.trainer | OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 1.46 GiB (GPU 0; 11.17 GiB total capacity; 7.91 GiB already allocated; 1.31 GiB free; 9.35 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
            "2022-03-10 06:33:33 | WARNING | fairseq.trainer | |===========================================================================|\n",
            "|                  PyTorch CUDA memory summary, device ID 0                 |\n",
            "|---------------------------------------------------------------------------|\n",
            "|            CUDA OOMs: 60           |        cudaMalloc retries: 108       |\n",
            "|===========================================================================|\n",
            "|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Allocated memory      |    8103 MB |    8163 MB |   11338 GB |   11330 GB |\n",
            "|       from large pool |    8054 MB |    8114 MB |   10710 GB |   10702 GB |\n",
            "|       from small pool |      48 MB |      60 MB |     627 GB |     627 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Active memory         |    8103 MB |    8163 MB |   11338 GB |   11330 GB |\n",
            "|       from large pool |    8054 MB |    8114 MB |   10710 GB |   10702 GB |\n",
            "|       from small pool |      48 MB |      60 MB |     627 GB |     627 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| GPU reserved memory   |    9570 MB |   10886 MB |  174776 MB |  165206 MB |\n",
            "|       from large pool |    9508 MB |   10752 MB |  169086 MB |  159578 MB |\n",
            "|       from small pool |      62 MB |     134 MB |    5690 MB |    5628 MB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Non-releasable memory |    1466 MB |    1500 MB |   12875 GB |   12873 GB |\n",
            "|       from large pool |    1453 MB |    1487 MB |   12196 GB |   12195 GB |\n",
            "|       from small pool |      13 MB |      24 MB |     678 GB |     678 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Allocations           |     593    |     595    |    5971 K  |    5970 K  |\n",
            "|       from large pool |     160    |     164    |    1865 K  |    1865 K  |\n",
            "|       from small pool |     433    |     516    |    4105 K  |    4104 K  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Active allocs         |     593    |     595    |    5971 K  |    5970 K  |\n",
            "|       from large pool |     160    |     164    |    1865 K  |    1865 K  |\n",
            "|       from small pool |     433    |     516    |    4105 K  |    4104 K  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| GPU reserved segments |      75    |     112    |    3112    |    3037    |\n",
            "|       from large pool |      44    |      45    |     267    |     223    |\n",
            "|       from small pool |      31    |      67    |    2845    |    2814    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Non-releasable allocs |      70    |      70    |    2715 K  |    2715 K  |\n",
            "|       from large pool |      36    |      36    |    1187 K  |    1187 K  |\n",
            "|       from small pool |      34    |      48    |    1527 K  |    1527 K  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Oversize allocations  |       0    |       0    |       0    |       0    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Oversize GPU segments |       0    |       0    |       0    |       0    |\n",
            "|===========================================================================|\n",
            "\n",
            "2022-03-10 06:33:33 | WARNING | fairseq.trainer | attempting to recover from OOM in forward/backward pass\n",
            "epoch 008:  77% 535/692 [02:00<00:28,  5.42it/s, loss=3.197, ppl=9.17, wps=9131.1, ups=4.5, wpb=2030.5, bsz=128, num_updates=5300, lr=0.0005, gnorm=1.343, train_wall=22, gb_free=5.7, wall=1214]2022-03-10 06:33:35 | WARNING | fairseq.trainer | OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 1.74 GiB (GPU 0; 11.17 GiB total capacity; 8.31 GiB already allocated; 755.81 MiB free; 9.91 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
            "2022-03-10 06:33:35 | WARNING | fairseq.trainer | |===========================================================================|\n",
            "|                  PyTorch CUDA memory summary, device ID 0                 |\n",
            "|---------------------------------------------------------------------------|\n",
            "|            CUDA OOMs: 61           |        cudaMalloc retries: 110       |\n",
            "|===========================================================================|\n",
            "|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Allocated memory      |    8511 MB |    8575 MB |   11352 GB |   11344 GB |\n",
            "|       from large pool |    8462 MB |    8526 MB |   10724 GB |   10716 GB |\n",
            "|       from small pool |      48 MB |      60 MB |     628 GB |     628 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Active memory         |    8511 MB |    8575 MB |   11352 GB |   11344 GB |\n",
            "|       from large pool |    8462 MB |    8526 MB |   10724 GB |   10716 GB |\n",
            "|       from small pool |      48 MB |      60 MB |     628 GB |     628 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| GPU reserved memory   |   10152 MB |   10152 MB |  178378 MB |  168226 MB |\n",
            "|       from large pool |   10088 MB |   10088 MB |  172650 MB |  162562 MB |\n",
            "|       from small pool |      64 MB |     100 MB |    5728 MB |    5664 MB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Non-releasable memory |    1640 MB |    1864 MB |   12889 GB |   12887 GB |\n",
            "|       from large pool |    1625 MB |    1849 MB |   12209 GB |   12208 GB |\n",
            "|       from small pool |      15 MB |      24 MB |     679 GB |     679 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Allocations           |     593    |     595    |    5976 K  |    5976 K  |\n",
            "|       from large pool |     160    |     164    |    1867 K  |    1867 K  |\n",
            "|       from small pool |     433    |     516    |    4109 K  |    4108 K  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Active allocs         |     593    |     595    |    5976 K  |    5976 K  |\n",
            "|       from large pool |     160    |     164    |    1867 K  |    1867 K  |\n",
            "|       from small pool |     433    |     516    |    4109 K  |    4108 K  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| GPU reserved segments |      76    |      94    |    3133    |    3057    |\n",
            "|       from large pool |      44    |      44    |     269    |     225    |\n",
            "|       from small pool |      32    |      50    |    2864    |    2832    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Non-releasable allocs |      67    |      67    |    2717 K  |    2717 K  |\n",
            "|       from large pool |      32    |      32    |    1188 K  |    1188 K  |\n",
            "|       from small pool |      35    |      52    |    1529 K  |    1529 K  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Oversize allocations  |       0    |       0    |       0    |       0    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Oversize GPU segments |       0    |       0    |       0    |       0    |\n",
            "|===========================================================================|\n",
            "\n",
            "2022-03-10 06:33:35 | WARNING | fairseq.trainer | attempting to recover from OOM in forward/backward pass\n",
            "epoch 008: 100% 691/692 [02:36<00:00,  5.08it/s, loss=3.193, ppl=9.15, wps=8398.1, ups=4.3, wpb=1951.6, bsz=128, num_updates=5400, lr=0.0005, gnorm=1.4, train_wall=21, gb_free=10.4, wall=1237]2022-03-10 06:34:09 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "\n",
            "epoch 008 | valid on 'valid' subset:   0% 0/8 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 008 | valid on 'valid' subset:  12% 1/8 [00:00<00:01,  6.67it/s]\u001b[A\n",
            "epoch 008 | valid on 'valid' subset:  25% 2/8 [00:00<00:00,  8.20it/s]\u001b[A\n",
            "epoch 008 | valid on 'valid' subset:  50% 4/8 [00:00<00:00,  9.98it/s]\u001b[A\n",
            "epoch 008 | valid on 'valid' subset:  62% 5/8 [00:00<00:00,  9.60it/s]\u001b[A\n",
            "epoch 008 | valid on 'valid' subset:  75% 6/8 [00:00<00:00,  9.37it/s]\u001b[A\n",
            "epoch 008 | valid on 'valid' subset:  88% 7/8 [00:00<00:00,  8.92it/s]\u001b[A\n",
            "epoch 008 | valid on 'valid' subset: 100% 8/8 [00:01<00:00,  4.98it/s]\u001b[A\n",
            "                                                                      \u001b[A2022-03-10 06:34:11 | INFO | valid | epoch 008 | valid on 'valid' subset | loss 5.117 | ppl 34.71 | wps 14198.1 | wpb 1992.9 | bsz 125 | num_updates 5475 | best_loss 4.912\n",
            "2022-03-10 06:34:11 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 8 @ 5475 updates\n",
            "2022-03-10 06:34:11 | INFO | fairseq.trainer | Saving checkpoint to /content/checkpoints/checkpoint8.pt\n",
            "2022-03-10 06:34:11 | INFO | fairseq.trainer | Finished saving checkpoint to /content/checkpoints/checkpoint8.pt\n",
            "2022-03-10 06:34:12 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/checkpoint8.pt (epoch 8 @ 5475 updates, score 5.117) (writing took 1.3889560200000233 seconds)\n",
            "2022-03-10 06:34:12 | INFO | fairseq_cli.train | end of epoch 8 (average epoch stats below)\n",
            "2022-03-10 06:34:12 | INFO | train | epoch 008 | loss 3.065 | ppl 8.37 | wps 8367 | ups 4.31 | wpb 1941.5 | bsz 127.9 | num_updates 5475 | lr 0.0005 | gnorm 1.335 | train_wall 147 | gb_free 10.7 | wall 1256\n",
            "epoch 009:   0% 0/692 [00:00<?, ?it/s]2022-03-10 06:34:12 | INFO | fairseq.data.iterators | grouped total_num_itrs = 692\n",
            "2022-03-10 06:34:12 | INFO | fairseq.trainer | begin training epoch 9\n",
            "2022-03-10 06:34:12 | INFO | fairseq_cli.train | Start iterating over samples\n",
            "epoch 009:  21% 147/692 [00:27<01:17,  7.01it/s, loss=2.473, ppl=5.55, wps=9352.7, ups=5.16, wpb=1811.4, bsz=128, num_updates=5600, lr=0.0005, gnorm=1.268, train_wall=19, gb_free=10.4, wall=1280]2022-03-10 06:34:40 | WARNING | fairseq.trainer | OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 1.46 GiB (GPU 0; 11.17 GiB total capacity; 7.91 GiB already allocated; 759.81 MiB free; 9.91 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
            "2022-03-10 06:34:40 | WARNING | fairseq.trainer | |===========================================================================|\n",
            "|                  PyTorch CUDA memory summary, device ID 0                 |\n",
            "|---------------------------------------------------------------------------|\n",
            "|            CUDA OOMs: 62           |        cudaMalloc retries: 111       |\n",
            "|===========================================================================|\n",
            "|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Allocated memory      |    8102 MB |    8162 MB |   11944 GB |   11936 GB |\n",
            "|       from large pool |    8053 MB |    8113 MB |   11281 GB |   11273 GB |\n",
            "|       from small pool |      48 MB |      60 MB |     662 GB |     662 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Active memory         |    8102 MB |    8162 MB |   11944 GB |   11936 GB |\n",
            "|       from large pool |    8053 MB |    8113 MB |   11281 GB |   11273 GB |\n",
            "|       from small pool |      48 MB |      60 MB |     662 GB |     662 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| GPU reserved memory   |   10148 MB |   10228 MB |  178454 MB |  168306 MB |\n",
            "|       from large pool |   10088 MB |   10088 MB |  172650 MB |  162562 MB |\n",
            "|       from small pool |      60 MB |     140 MB |    5804 MB |    5744 MB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Non-releasable memory |    2045 MB |    2045 MB |   13483 GB |   13481 GB |\n",
            "|       from large pool |    2034 MB |    2034 MB |   12767 GB |   12765 GB |\n",
            "|       from small pool |      11 MB |      44 MB |     716 GB |     716 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Allocations           |     593    |     595    |    6317 K  |    6317 K  |\n",
            "|       from large pool |     160    |     164    |    1975 K  |    1975 K  |\n",
            "|       from small pool |     433    |     516    |    4341 K  |    4341 K  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Active allocs         |     593    |     595    |    6317 K  |    6317 K  |\n",
            "|       from large pool |     160    |     164    |    1975 K  |    1975 K  |\n",
            "|       from small pool |     433    |     516    |    4341 K  |    4341 K  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| GPU reserved segments |      74    |     114    |    3171    |    3097    |\n",
            "|       from large pool |      44    |      44    |     269    |     225    |\n",
            "|       from small pool |      30    |      70    |    2902    |    2872    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Non-releasable allocs |      70    |      70    |    2872 K  |    2872 K  |\n",
            "|       from large pool |      38    |      38    |    1257 K  |    1257 K  |\n",
            "|       from small pool |      32    |      62    |    1614 K  |    1614 K  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Oversize allocations  |       0    |       0    |       0    |       0    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Oversize GPU segments |       0    |       0    |       0    |       0    |\n",
            "|===========================================================================|\n",
            "\n",
            "2022-03-10 06:34:40 | WARNING | fairseq.trainer | attempting to recover from OOM in forward/backward pass\n",
            "epoch 009:  51% 352/692 [01:12<01:22,  4.13it/s, loss=2.913, ppl=7.53, wps=8498, ups=4.33, wpb=1963.2, bsz=128, num_updates=5800, lr=0.0005, gnorm=1.369, train_wall=23, gb_free=8.4, wall=1323]2022-03-10 06:35:26 | WARNING | fairseq.trainer | OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 2.01 GiB (GPU 0; 11.17 GiB total capacity; 9.79 GiB already allocated; 199.81 MiB free; 10.46 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
            "2022-03-10 06:35:26 | WARNING | fairseq.trainer | |===========================================================================|\n",
            "|                  PyTorch CUDA memory summary, device ID 0                 |\n",
            "|---------------------------------------------------------------------------|\n",
            "|            CUDA OOMs: 63           |        cudaMalloc retries: 113       |\n",
            "|===========================================================================|\n",
            "|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Allocated memory      |   10027 MB |   10102 MB |   12375 GB |   12366 GB |\n",
            "|       from large pool |    9978 MB |   10053 MB |   11687 GB |   11677 GB |\n",
            "|       from small pool |      49 MB |      60 MB |     688 GB |     688 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Active memory         |   10027 MB |   10102 MB |   12375 GB |   12366 GB |\n",
            "|       from large pool |    9978 MB |   10053 MB |   11687 GB |   11677 GB |\n",
            "|       from small pool |      49 MB |      60 MB |     688 GB |     688 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| GPU reserved memory   |   10708 MB |   10708 MB |  182656 MB |  171948 MB |\n",
            "|       from large pool |   10648 MB |   10648 MB |  176774 MB |  166126 MB |\n",
            "|       from small pool |      60 MB |     138 MB |    5882 MB |    5822 MB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Non-releasable memory |  696468 KB |    1613 MB |   13912 GB |   13912 GB |\n",
            "|       from large pool |  685528 KB |    1602 MB |   13168 GB |   13168 GB |\n",
            "|       from small pool |   10940 KB |      22 MB |     744 GB |     744 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Allocations           |     593    |     595    |    6546 K  |    6545 K  |\n",
            "|       from large pool |     160    |     164    |    2045 K  |    2045 K  |\n",
            "|       from small pool |     433    |     516    |    4500 K  |    4500 K  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Active allocs         |     593    |     595    |    6546 K  |    6545 K  |\n",
            "|       from large pool |     160    |     164    |    2045 K  |    2045 K  |\n",
            "|       from small pool |     433    |     516    |    4500 K  |    4500 K  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| GPU reserved segments |      74    |     113    |    3212    |    3138    |\n",
            "|       from large pool |      44    |      44    |     271    |     227    |\n",
            "|       from small pool |      30    |      69    |    2941    |    2911    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Non-releasable allocs |      62    |      63    |    2975 K  |    2975 K  |\n",
            "|       from large pool |      31    |      31    |    1301 K  |    1301 K  |\n",
            "|       from small pool |      31    |      52    |    1674 K  |    1674 K  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Oversize allocations  |       0    |       0    |       0    |       0    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Oversize GPU segments |       0    |       0    |       0    |       0    |\n",
            "|===========================================================================|\n",
            "\n",
            "2022-03-10 06:35:26 | WARNING | fairseq.trainer | attempting to recover from OOM in forward/backward pass\n",
            "epoch 009:  59% 408/692 [01:27<00:49,  5.68it/s, loss=2.913, ppl=7.53, wps=8498, ups=4.33, wpb=1963.2, bsz=128, num_updates=5800, lr=0.0005, gnorm=1.369, train_wall=23, gb_free=8.4, wall=1323]2022-03-10 06:35:40 | WARNING | fairseq.trainer | OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 2.50 GiB (GPU 0; 11.17 GiB total capacity; 8.45 GiB already allocated; 193.81 MiB free; 10.46 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
            "2022-03-10 06:35:40 | WARNING | fairseq.trainer | |===========================================================================|\n",
            "|                  PyTorch CUDA memory summary, device ID 0                 |\n",
            "|---------------------------------------------------------------------------|\n",
            "|            CUDA OOMs: 64           |        cudaMalloc retries: 114       |\n",
            "|===========================================================================|\n",
            "|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Allocated memory      |    8611 MB |    8653 MB |   12515 GB |   12507 GB |\n",
            "|       from large pool |    8561 MB |    8602 MB |   11821 GB |   11813 GB |\n",
            "|       from small pool |      50 MB |      60 MB |     694 GB |     694 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Active memory         |    8611 MB |    8653 MB |   12515 GB |   12507 GB |\n",
            "|       from large pool |    8561 MB |    8602 MB |   11821 GB |   11813 GB |\n",
            "|       from small pool |      50 MB |      60 MB |     694 GB |     694 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| GPU reserved memory   |   10714 MB |   10780 MB |  182728 MB |  172014 MB |\n",
            "|       from large pool |   10648 MB |   10648 MB |  176774 MB |  166126 MB |\n",
            "|       from small pool |      66 MB |     132 MB |    5954 MB |    5888 MB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Non-releasable memory |    2102 MB |    2270 MB |   14057 GB |   14055 GB |\n",
            "|       from large pool |    2086 MB |    2254 MB |   13306 GB |   13304 GB |\n",
            "|       from small pool |      15 MB |      20 MB |     750 GB |     750 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Allocations           |     593    |     595    |    6608 K  |    6607 K  |\n",
            "|       from large pool |     162    |     163    |    2065 K  |    2065 K  |\n",
            "|       from small pool |     431    |     516    |    4542 K  |    4541 K  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Active allocs         |     593    |     595    |    6608 K  |    6607 K  |\n",
            "|       from large pool |     162    |     163    |    2065 K  |    2065 K  |\n",
            "|       from small pool |     431    |     516    |    4542 K  |    4541 K  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| GPU reserved segments |      77    |     110    |    3248    |    3171    |\n",
            "|       from large pool |      44    |      44    |     271    |     227    |\n",
            "|       from small pool |      33    |      66    |    2977    |    2944    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Non-releasable allocs |      72    |      72    |    3003 K  |    3003 K  |\n",
            "|       from large pool |      37    |      37    |    1313 K  |    1313 K  |\n",
            "|       from small pool |      35    |      40    |    1689 K  |    1689 K  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Oversize allocations  |       0    |       0    |       0    |       0    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Oversize GPU segments |       0    |       0    |       0    |       0    |\n",
            "|===========================================================================|\n",
            "\n",
            "2022-03-10 06:35:40 | WARNING | fairseq.trainer | attempting to recover from OOM in forward/backward pass\n",
            "epoch 009:  61% 420/692 [01:30<01:26,  3.15it/s, loss=2.913, ppl=7.53, wps=8498, ups=4.33, wpb=1963.2, bsz=128, num_updates=5800, lr=0.0005, gnorm=1.369, train_wall=23, gb_free=8.4, wall=1323]2022-03-10 06:35:43 | WARNING | fairseq.trainer | OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 1.69 GiB (GPU 0; 11.17 GiB total capacity; 9.21 GiB already allocated; 177.81 MiB free; 10.48 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
            "2022-03-10 06:35:43 | WARNING | fairseq.trainer | |===========================================================================|\n",
            "|                  PyTorch CUDA memory summary, device ID 0                 |\n",
            "|---------------------------------------------------------------------------|\n",
            "|            CUDA OOMs: 65           |        cudaMalloc retries: 116       |\n",
            "|===========================================================================|\n",
            "|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Allocated memory      |    7698 MB |    9432 MB |   12550 GB |   12542 GB |\n",
            "|       from large pool |    7650 MB |    9384 MB |   11854 GB |   11847 GB |\n",
            "|       from small pool |      48 MB |      60 MB |     695 GB |     695 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Active memory         |    7698 MB |    9432 MB |   12550 GB |   12542 GB |\n",
            "|       from large pool |    7650 MB |    9384 MB |   11854 GB |   11847 GB |\n",
            "|       from small pool |      48 MB |      60 MB |     695 GB |     695 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| GPU reserved memory   |   10730 MB |   10786 MB |  184534 MB |  173804 MB |\n",
            "|       from large pool |   10670 MB |   10670 MB |  178508 MB |  167838 MB |\n",
            "|       from small pool |      60 MB |     138 MB |    6026 MB |    5966 MB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Non-releasable memory |    1297 MB |    1396 MB |   14092 GB |   14091 GB |\n",
            "|       from large pool |    1285 MB |    1383 MB |   13340 GB |   13339 GB |\n",
            "|       from small pool |      11 MB |      20 MB |     751 GB |     751 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Allocations           |     592    |     595    |    6620 K  |    6620 K  |\n",
            "|       from large pool |     160    |     164    |    2069 K  |    2069 K  |\n",
            "|       from small pool |     432    |     516    |    4551 K  |    4550 K  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Active allocs         |     592    |     595    |    6620 K  |    6620 K  |\n",
            "|       from large pool |     160    |     164    |    2069 K  |    2069 K  |\n",
            "|       from small pool |     432    |     516    |    4551 K  |    4550 K  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| GPU reserved segments |      72    |     113    |    3285    |    3213    |\n",
            "|       from large pool |      42    |      44    |     272    |     230    |\n",
            "|       from small pool |      30    |      69    |    3013    |    2983    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Non-releasable allocs |      64    |      64    |    3009 K  |    3009 K  |\n",
            "|       from large pool |      32    |      32    |    1316 K  |    1316 K  |\n",
            "|       from small pool |      32    |      43    |    1692 K  |    1692 K  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Oversize allocations  |       0    |       0    |       0    |       0    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Oversize GPU segments |       0    |       0    |       0    |       0    |\n",
            "|===========================================================================|\n",
            "\n",
            "2022-03-10 06:35:43 | WARNING | fairseq.trainer | attempting to recover from OOM in forward/backward pass\n",
            "epoch 009:  67% 463/692 [01:40<00:52,  4.40it/s, loss=3.015, ppl=8.08, wps=7992.1, ups=3.9, wpb=2050.8, bsz=127.1, num_updates=5900, lr=0.0005, gnorm=1.408, train_wall=22, gb_free=10.5, wall=1349]2022-03-10 06:35:54 | WARNING | fairseq.trainer | OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 1.46 GiB (GPU 0; 11.17 GiB total capacity; 7.60 GiB already allocated; 699.81 MiB free; 9.97 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
            "2022-03-10 06:35:54 | WARNING | fairseq.trainer | |===========================================================================|\n",
            "|                  PyTorch CUDA memory summary, device ID 0                 |\n",
            "|---------------------------------------------------------------------------|\n",
            "|            CUDA OOMs: 66           |        cudaMalloc retries: 118       |\n",
            "|===========================================================================|\n",
            "|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Allocated memory      |    6291 MB |    7783 MB |   12647 GB |   12641 GB |\n",
            "|       from large pool |    6234 MB |    7726 MB |   11947 GB |   11941 GB |\n",
            "|       from small pool |      56 MB |      60 MB |     700 GB |     700 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Active memory         |    6291 MB |    7783 MB |   12647 GB |   12641 GB |\n",
            "|       from large pool |    6234 MB |    7726 MB |   11947 GB |   11941 GB |\n",
            "|       from small pool |      56 MB |      60 MB |     700 GB |     700 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| GPU reserved memory   |   10208 MB |   10442 MB |  187472 MB |  177264 MB |\n",
            "|       from large pool |   10144 MB |   10304 MB |  181368 MB |  171224 MB |\n",
            "|       from small pool |      64 MB |     138 MB |    6104 MB |    6040 MB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Non-releasable memory |    2424 MB |    2519 MB |   14197 GB |   14195 GB |\n",
            "|       from large pool |    2417 MB |    2511 MB |   13440 GB |   13438 GB |\n",
            "|       from small pool |       7 MB |      16 MB |     757 GB |     757 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Allocations           |     592    |     595    |    6668 K  |    6667 K  |\n",
            "|       from large pool |     150    |     154    |    2084 K  |    2084 K  |\n",
            "|       from small pool |     442    |     516    |    4583 K  |    4583 K  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Active allocs         |     592    |     595    |    6668 K  |    6667 K  |\n",
            "|       from large pool |     150    |     154    |    2084 K  |    2084 K  |\n",
            "|       from small pool |     442    |     516    |    4583 K  |    4583 K  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| GPU reserved segments |      72    |     111    |    3326    |    3254    |\n",
            "|       from large pool |      40    |      42    |     274    |     234    |\n",
            "|       from small pool |      32    |      69    |    3052    |    3020    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Non-releasable allocs |      61    |      62    |    3030 K  |    3030 K  |\n",
            "|       from large pool |      29    |      29    |    1325 K  |    1325 K  |\n",
            "|       from small pool |      32    |      40    |    1704 K  |    1704 K  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Oversize allocations  |       0    |       0    |       0    |       0    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Oversize GPU segments |       0    |       0    |       0    |       0    |\n",
            "|===========================================================================|\n",
            "\n",
            "2022-03-10 06:35:54 | WARNING | fairseq.trainer | attempting to recover from OOM in forward/backward pass\n",
            "epoch 009:  75% 521/692 [01:54<00:40,  4.26it/s, loss=3.015, ppl=8.08, wps=7992.1, ups=3.9, wpb=2050.8, bsz=127.1, num_updates=5900, lr=0.0005, gnorm=1.408, train_wall=22, gb_free=10.5, wall=1349]2022-03-10 06:36:07 | WARNING | fairseq.trainer | OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 1.34 GiB (GPU 0; 11.17 GiB total capacity; 7.45 GiB already allocated; 839.81 MiB free; 9.83 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
            "2022-03-10 06:36:07 | WARNING | fairseq.trainer | |===========================================================================|\n",
            "|                  PyTorch CUDA memory summary, device ID 0                 |\n",
            "|---------------------------------------------------------------------------|\n",
            "|            CUDA OOMs: 67           |        cudaMalloc retries: 119       |\n",
            "|===========================================================================|\n",
            "|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Allocated memory      |    6263 MB |    7629 MB |   12775 GB |   12769 GB |\n",
            "|       from large pool |    6215 MB |    7581 MB |   12069 GB |   12063 GB |\n",
            "|       from small pool |      48 MB |      60 MB |     706 GB |     706 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Active memory         |    6263 MB |    7629 MB |   12775 GB |   12769 GB |\n",
            "|       from large pool |    6215 MB |    7581 MB |   12069 GB |   12063 GB |\n",
            "|       from small pool |      48 MB |      60 MB |     706 GB |     706 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| GPU reserved memory   |   10068 MB |   10136 MB |  188892 MB |  178824 MB |\n",
            "|       from large pool |   10004 MB |   10004 MB |  182720 MB |  172716 MB |\n",
            "|       from small pool |      64 MB |     132 MB |    6172 MB |    6108 MB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Non-releasable memory |    1742 MB |    2438 MB |   14334 GB |   14332 GB |\n",
            "|       from large pool |    1726 MB |    2422 MB |   13570 GB |   13568 GB |\n",
            "|       from small pool |      15 MB |      27 MB |     763 GB |     763 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Allocations           |     592    |     595    |    6732 K  |    6731 K  |\n",
            "|       from large pool |     160    |     164    |    2104 K  |    2104 K  |\n",
            "|       from small pool |     432    |     516    |    4627 K  |    4626 K  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Active allocs         |     592    |     595    |    6732 K  |    6731 K  |\n",
            "|       from large pool |     160    |     164    |    2104 K  |    2104 K  |\n",
            "|       from small pool |     432    |     516    |    4627 K  |    4626 K  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| GPU reserved segments |      72    |     106    |    3361    |    3289    |\n",
            "|       from large pool |      40    |      40    |     275    |     235    |\n",
            "|       from small pool |      32    |      66    |    3086    |    3054    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Non-releasable allocs |      58    |      59    |    3059 K  |    3059 K  |\n",
            "|       from large pool |      25    |      26    |    1338 K  |    1338 K  |\n",
            "|       from small pool |      33    |      51    |    1720 K  |    1720 K  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Oversize allocations  |       0    |       0    |       0    |       0    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Oversize GPU segments |       0    |       0    |       0    |       0    |\n",
            "|===========================================================================|\n",
            "\n",
            "2022-03-10 06:36:07 | WARNING | fairseq.trainer | attempting to recover from OOM in forward/backward pass\n",
            "epoch 009:  76% 525/692 [01:55<00:41,  4.02it/s, loss=3.015, ppl=8.08, wps=7992.1, ups=3.9, wpb=2050.8, bsz=127.1, num_updates=5900, lr=0.0005, gnorm=1.408, train_wall=22, gb_free=10.5, wall=1349]2022-03-10 06:36:08 | WARNING | fairseq.trainer | OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 2.11 GiB (GPU 0; 11.17 GiB total capacity; 7.84 GiB already allocated; 739.81 MiB free; 9.93 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
            "2022-03-10 06:36:08 | WARNING | fairseq.trainer | |===========================================================================|\n",
            "|                  PyTorch CUDA memory summary, device ID 0                 |\n",
            "|---------------------------------------------------------------------------|\n",
            "|            CUDA OOMs: 68           |        cudaMalloc retries: 120       |\n",
            "|===========================================================================|\n",
            "|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Allocated memory      |    8033 MB |    8043 MB |   12789 GB |   12782 GB |\n",
            "|       from large pool |    7983 MB |    7994 MB |   12082 GB |   12075 GB |\n",
            "|       from small pool |      49 MB |      60 MB |     707 GB |     706 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Active memory         |    8033 MB |    8043 MB |   12789 GB |   12782 GB |\n",
            "|       from large pool |    7983 MB |    7994 MB |   12082 GB |   12075 GB |\n",
            "|       from small pool |      49 MB |      60 MB |     707 GB |     706 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| GPU reserved memory   |   10168 MB |   10222 MB |  191108 MB |  180940 MB |\n",
            "|       from large pool |   10106 MB |   10106 MB |  184884 MB |  174778 MB |\n",
            "|       from small pool |      62 MB |     116 MB |    6224 MB |    6162 MB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Non-releasable memory |    2134 MB |    2172 MB |   14348 GB |   14346 GB |\n",
            "|       from large pool |    2122 MB |    2159 MB |   13583 GB |   13581 GB |\n",
            "|       from small pool |      12 MB |      17 MB |     764 GB |     764 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Allocations           |     592    |     595    |    6735 K  |    6735 K  |\n",
            "|       from large pool |     162    |     164    |    2106 K  |    2105 K  |\n",
            "|       from small pool |     430    |     516    |    4629 K  |    4629 K  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Active allocs         |     592    |     595    |    6735 K  |    6735 K  |\n",
            "|       from large pool |     162    |     164    |    2106 K  |    2105 K  |\n",
            "|       from small pool |     430    |     516    |    4629 K  |    4629 K  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| GPU reserved segments |      71    |      98    |    3388    |    3317    |\n",
            "|       from large pool |      40    |      40    |     276    |     236    |\n",
            "|       from small pool |      31    |      58    |    3112    |    3081    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Non-releasable allocs |      61    |      61    |    3060 K  |    3060 K  |\n",
            "|       from large pool |      26    |      27    |    1339 K  |    1339 K  |\n",
            "|       from small pool |      35    |      40    |    1721 K  |    1721 K  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Oversize allocations  |       0    |       0    |       0    |       0    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Oversize GPU segments |       0    |       0    |       0    |       0    |\n",
            "|===========================================================================|\n",
            "\n",
            "2022-03-10 06:36:08 | WARNING | fairseq.trainer | attempting to recover from OOM in forward/backward pass\n",
            "epoch 009:  81% 559/692 [02:03<00:24,  5.36it/s, loss=2.836, ppl=7.14, wps=7968.6, ups=4.16, wpb=1914.7, bsz=128, num_updates=6000, lr=0.0005, gnorm=1.39, train_wall=21, gb_free=10.6, wall=1373]2022-03-10 06:36:16 | WARNING | fairseq.trainer | OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 1.74 GiB (GPU 0; 11.17 GiB total capacity; 8.31 GiB already allocated; 741.81 MiB free; 9.93 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
            "2022-03-10 06:36:16 | WARNING | fairseq.trainer | |===========================================================================|\n",
            "|                  PyTorch CUDA memory summary, device ID 0                 |\n",
            "|---------------------------------------------------------------------------|\n",
            "|            CUDA OOMs: 69           |        cudaMalloc retries: 121       |\n",
            "|===========================================================================|\n",
            "|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Allocated memory      |    8510 MB |    8574 MB |   12865 GB |   12857 GB |\n",
            "|       from large pool |    8461 MB |    8525 MB |   12155 GB |   12146 GB |\n",
            "|       from small pool |      48 MB |      60 MB |     710 GB |     710 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Active memory         |    8510 MB |    8574 MB |   12865 GB |   12857 GB |\n",
            "|       from large pool |    8461 MB |    8525 MB |   12155 GB |   12146 GB |\n",
            "|       from small pool |      48 MB |      60 MB |     710 GB |     710 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| GPU reserved memory   |   10166 MB |   10234 MB |  191174 MB |  181008 MB |\n",
            "|       from large pool |   10106 MB |   10106 MB |  184884 MB |  174778 MB |\n",
            "|       from small pool |      60 MB |     128 MB |    6290 MB |    6230 MB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Non-releasable memory |    1655 MB |    1831 MB |   14430 GB |   14428 GB |\n",
            "|       from large pool |    1644 MB |    1818 MB |   13661 GB |   13660 GB |\n",
            "|       from small pool |      11 MB |      30 MB |     768 GB |     768 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Allocations           |     593    |     595    |    6772 K  |    6772 K  |\n",
            "|       from large pool |     160    |     164    |    2118 K  |    2117 K  |\n",
            "|       from small pool |     433    |     516    |    4654 K  |    4654 K  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Active allocs         |     593    |     595    |    6772 K  |    6772 K  |\n",
            "|       from large pool |     160    |     164    |    2118 K  |    2117 K  |\n",
            "|       from small pool |     433    |     516    |    4654 K  |    4654 K  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| GPU reserved segments |      70    |     104    |    3421    |    3351    |\n",
            "|       from large pool |      40    |      40    |     276    |     236    |\n",
            "|       from small pool |      30    |      64    |    3145    |    3115    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Non-releasable allocs |      63    |      64    |    3077 K  |    3077 K  |\n",
            "|       from large pool |      29    |      29    |    1346 K  |    1346 K  |\n",
            "|       from small pool |      34    |      47    |    1730 K  |    1730 K  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Oversize allocations  |       0    |       0    |       0    |       0    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Oversize GPU segments |       0    |       0    |       0    |       0    |\n",
            "|===========================================================================|\n",
            "\n",
            "2022-03-10 06:36:16 | WARNING | fairseq.trainer | attempting to recover from OOM in forward/backward pass\n",
            "epoch 009: 100% 691/692 [02:32<00:00,  6.66it/s, loss=3.079, ppl=8.45, wps=8880.3, ups=4.28, wpb=2077, bsz=128, num_updates=6100, lr=0.0005, gnorm=1.397, train_wall=22, gb_free=9.9, wall=1396]2022-03-10 06:36:45 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "\n",
            "epoch 009 | valid on 'valid' subset:   0% 0/8 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 009 | valid on 'valid' subset:  12% 1/8 [00:00<00:01,  6.58it/s]\u001b[A\n",
            "epoch 009 | valid on 'valid' subset:  25% 2/8 [00:00<00:00,  8.14it/s]\u001b[A\n",
            "epoch 009 | valid on 'valid' subset:  50% 4/8 [00:00<00:00,  9.96it/s]\u001b[A\n",
            "epoch 009 | valid on 'valid' subset:  62% 5/8 [00:00<00:00,  9.58it/s]\u001b[A\n",
            "epoch 009 | valid on 'valid' subset:  75% 6/8 [00:00<00:00,  9.38it/s]\u001b[A\n",
            "epoch 009 | valid on 'valid' subset:  88% 7/8 [00:00<00:00,  8.92it/s]\u001b[A\n",
            "epoch 009 | valid on 'valid' subset: 100% 8/8 [00:01<00:00,  4.98it/s]\u001b[A\n",
            "                                                                      \u001b[A2022-03-10 06:36:46 | INFO | valid | epoch 009 | valid on 'valid' subset | loss 5.219 | ppl 37.25 | wps 14210.4 | wpb 1992.9 | bsz 125 | num_updates 6159 | best_loss 4.912\n",
            "2022-03-10 06:36:46 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 9 @ 6159 updates\n",
            "2022-03-10 06:36:46 | INFO | fairseq.trainer | Saving checkpoint to /content/checkpoints/checkpoint9.pt\n",
            "2022-03-10 06:36:47 | INFO | fairseq.trainer | Finished saving checkpoint to /content/checkpoints/checkpoint9.pt\n",
            "2022-03-10 06:36:47 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/checkpoint9.pt (epoch 9 @ 6159 updates, score 5.219) (writing took 1.0054289019999487 seconds)\n",
            "epoch 009: 100% 692/692 [02:34<00:00,  1.23it/s, loss=3.079, ppl=8.45, wps=8880.3, ups=4.28, wpb=2077, bsz=128, num_updates=6100, lr=0.0005, gnorm=1.397, train_wall=22, gb_free=9.9, wall=1396]2022-03-10 06:36:47 | INFO | fairseq_cli.train | end of epoch 9 (average epoch stats below)\n",
            "2022-03-10 06:36:47 | INFO | train | epoch 009 | loss 2.834 | ppl 7.13 | wps 8539.1 | ups 4.42 | wpb 1933.9 | bsz 127.9 | num_updates 6159 | lr 0.0005 | gnorm 1.362 | train_wall 143 | gb_free 10.6 | wall 1411\n",
            "epoch 010:   0% 0/692 [00:00<?, ?it/s]2022-03-10 06:36:47 | INFO | fairseq.data.iterators | grouped total_num_itrs = 692\n",
            "2022-03-10 06:36:47 | INFO | fairseq.trainer | begin training epoch 10\n",
            "2022-03-10 06:36:47 | INFO | fairseq_cli.train | Start iterating over samples\n",
            "epoch 010:  12% 81/692 [00:18<02:55,  3.48it/s, loss=2.746, ppl=6.71, wps=8016.2, ups=4.21, wpb=1902.8, bsz=128, num_updates=6200, lr=0.0005, gnorm=1.362, train_wall=21, gb_free=10.3, wall=1420]2022-03-10 06:37:06 | WARNING | fairseq.trainer | OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 1.25 GiB (GPU 0; 11.17 GiB total capacity; 7.23 GiB already allocated; 741.81 MiB free; 9.93 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
            "2022-03-10 06:37:06 | WARNING | fairseq.trainer | |===========================================================================|\n",
            "|                  PyTorch CUDA memory summary, device ID 0                 |\n",
            "|---------------------------------------------------------------------------|\n",
            "|            CUDA OOMs: 70           |        cudaMalloc retries: 122       |\n",
            "|===========================================================================|\n",
            "|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Allocated memory      |    6123 MB |    7404 MB |   13331 GB |   13325 GB |\n",
            "|       from large pool |    6075 MB |    7355 MB |   12596 GB |   12590 GB |\n",
            "|       from small pool |      48 MB |      60 MB |     735 GB |     735 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Active memory         |    6123 MB |    7404 MB |   13331 GB |   13325 GB |\n",
            "|       from large pool |    6075 MB |    7355 MB |   12596 GB |   12590 GB |\n",
            "|       from small pool |      48 MB |      60 MB |     735 GB |     735 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| GPU reserved memory   |   10166 MB |   10246 MB |  191254 MB |  181088 MB |\n",
            "|       from large pool |   10106 MB |   10106 MB |  184884 MB |  174778 MB |\n",
            "|       from small pool |      60 MB |     140 MB |    6370 MB |    6310 MB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Non-releasable memory |    1878 MB |    2761 MB |   14924 GB |   14922 GB |\n",
            "|       from large pool |    1866 MB |    2750 MB |   14128 GB |   14127 GB |\n",
            "|       from small pool |      11 MB |      14 MB |     795 GB |     795 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Allocations           |     592    |     595    |    7013 K  |    7012 K  |\n",
            "|       from large pool |     160    |     164    |    2194 K  |    2193 K  |\n",
            "|       from small pool |     432    |     516    |    4819 K  |    4818 K  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Active allocs         |     592    |     595    |    7013 K  |    7012 K  |\n",
            "|       from large pool |     160    |     164    |    2194 K  |    2193 K  |\n",
            "|       from small pool |     432    |     516    |    4819 K  |    4818 K  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| GPU reserved segments |      70    |     110    |    3461    |    3391    |\n",
            "|       from large pool |      40    |      40    |     276    |     236    |\n",
            "|       from small pool |      30    |      70    |    3185    |    3155    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Non-releasable allocs |      67    |      68    |    3185 K  |    3185 K  |\n",
            "|       from large pool |      33    |      34    |    1394 K  |    1394 K  |\n",
            "|       from small pool |      34    |      35    |    1790 K  |    1790 K  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Oversize allocations  |       0    |       0    |       0    |       0    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Oversize GPU segments |       0    |       0    |       0    |       0    |\n",
            "|===========================================================================|\n",
            "\n",
            "2022-03-10 06:37:06 | WARNING | fairseq.trainer | attempting to recover from OOM in forward/backward pass\n",
            "epoch 010:  24% 167/692 [00:36<01:38,  5.33it/s, loss=2.475, ppl=5.56, wps=8759.6, ups=4.54, wpb=1930.5, bsz=127.1, num_updates=6300, lr=0.0005, gnorm=1.312, train_wall=21, gb_free=10, wall=1442]2022-03-10 06:37:24 | WARNING | fairseq.trainer | OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 1.74 GiB (GPU 0; 11.17 GiB total capacity; 8.31 GiB already allocated; 1.41 GiB free; 9.24 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
            "2022-03-10 06:37:24 | WARNING | fairseq.trainer | |===========================================================================|\n",
            "|                  PyTorch CUDA memory summary, device ID 0                 |\n",
            "|---------------------------------------------------------------------------|\n",
            "|            CUDA OOMs: 71           |        cudaMalloc retries: 123       |\n",
            "|===========================================================================|\n",
            "|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Allocated memory      |    8509 MB |    8573 MB |   13496 GB |   13488 GB |\n",
            "|       from large pool |    8460 MB |    8524 MB |   12751 GB |   12743 GB |\n",
            "|       from small pool |      48 MB |      60 MB |     745 GB |     745 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Active memory         |    8509 MB |    8573 MB |   13496 GB |   13488 GB |\n",
            "|       from large pool |    8460 MB |    8524 MB |   12751 GB |   12743 GB |\n",
            "|       from small pool |      48 MB |      60 MB |     745 GB |     745 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| GPU reserved memory   |    9466 MB |   10904 MB |  194156 MB |  184690 MB |\n",
            "|       from large pool |    9404 MB |   10772 MB |  187714 MB |  178310 MB |\n",
            "|       from small pool |      62 MB |     132 MB |    6442 MB |    6380 MB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Non-releasable memory |     956 MB |    1529 MB |   15093 GB |   15092 GB |\n",
            "|       from large pool |     943 MB |    1514 MB |   14287 GB |   14286 GB |\n",
            "|       from small pool |      13 MB |      17 MB |     805 GB |     805 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Allocations           |     593    |     595    |    7108 K  |    7107 K  |\n",
            "|       from large pool |     160    |     164    |    2224 K  |    2224 K  |\n",
            "|       from small pool |     433    |     516    |    4884 K  |    4883 K  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Active allocs         |     593    |     595    |    7108 K  |    7107 K  |\n",
            "|       from large pool |     160    |     164    |    2224 K  |    2224 K  |\n",
            "|       from small pool |     433    |     516    |    4884 K  |    4883 K  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| GPU reserved segments |      71    |     107    |    3499    |    3428    |\n",
            "|       from large pool |      40    |      41    |     278    |     238    |\n",
            "|       from small pool |      31    |      66    |    3221    |    3190    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Non-releasable allocs |      62    |      62    |    3228 K  |    3228 K  |\n",
            "|       from large pool |      30    |      30    |    1413 K  |    1413 K  |\n",
            "|       from small pool |      32    |      39    |    1814 K  |    1814 K  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Oversize allocations  |       0    |       0    |       0    |       0    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Oversize GPU segments |       0    |       0    |       0    |       0    |\n",
            "|===========================================================================|\n",
            "\n",
            "2022-03-10 06:37:24 | WARNING | fairseq.trainer | attempting to recover from OOM in forward/backward pass\n",
            "epoch 010:  32% 221/692 [00:48<01:52,  4.20it/s, loss=2.475, ppl=5.56, wps=8759.6, ups=4.54, wpb=1930.5, bsz=127.1, num_updates=6300, lr=0.0005, gnorm=1.312, train_wall=21, gb_free=10, wall=1442]2022-03-10 06:37:36 | WARNING | fairseq.trainer | OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 1.69 GiB (GPU 0; 11.17 GiB total capacity; 7.52 GiB already allocated; 1.41 GiB free; 9.24 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
            "2022-03-10 06:37:36 | WARNING | fairseq.trainer | |===========================================================================|\n",
            "|                  PyTorch CUDA memory summary, device ID 0                 |\n",
            "|---------------------------------------------------------------------------|\n",
            "|            CUDA OOMs: 72           |        cudaMalloc retries: 124       |\n",
            "|===========================================================================|\n",
            "|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Allocated memory      |    7699 MB |    7758 MB |   13606 GB |   13599 GB |\n",
            "|       from large pool |    7650 MB |    7710 MB |   12855 GB |   12847 GB |\n",
            "|       from small pool |      48 MB |      60 MB |     751 GB |     751 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Active memory         |    7699 MB |    7758 MB |   13606 GB |   13599 GB |\n",
            "|       from large pool |    7650 MB |    7710 MB |   12855 GB |   12847 GB |\n",
            "|       from small pool |      48 MB |      60 MB |     751 GB |     751 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| GPU reserved memory   |    9466 MB |    9544 MB |  194234 MB |  184768 MB |\n",
            "|       from large pool |    9404 MB |    9404 MB |  187714 MB |  178310 MB |\n",
            "|       from small pool |      62 MB |     140 MB |    6520 MB |    6458 MB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Non-releasable memory |    1766 MB |    1766 MB |   15207 GB |   15205 GB |\n",
            "|       from large pool |    1753 MB |    1753 MB |   14394 GB |   14393 GB |\n",
            "|       from small pool |      13 MB |      17 MB |     812 GB |     812 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Allocations           |     593    |     595    |    7168 K  |    7167 K  |\n",
            "|       from large pool |     160    |     164    |    2243 K  |    2242 K  |\n",
            "|       from small pool |     433    |     516    |    4925 K  |    4924 K  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Active allocs         |     593    |     595    |    7168 K  |    7167 K  |\n",
            "|       from large pool |     160    |     164    |    2243 K  |    2242 K  |\n",
            "|       from small pool |     433    |     516    |    4925 K  |    4924 K  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| GPU reserved segments |      71    |     110    |    3538    |    3467    |\n",
            "|       from large pool |      40    |      40    |     278    |     238    |\n",
            "|       from small pool |      31    |      70    |    3260    |    3229    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Non-releasable allocs |      61    |      62    |    3255 K  |    3255 K  |\n",
            "|       from large pool |      29    |      29    |    1425 K  |    1425 K  |\n",
            "|       from small pool |      32    |      43    |    1830 K  |    1830 K  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Oversize allocations  |       0    |       0    |       0    |       0    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Oversize GPU segments |       0    |       0    |       0    |       0    |\n",
            "|===========================================================================|\n",
            "\n",
            "2022-03-10 06:37:36 | WARNING | fairseq.trainer | attempting to recover from OOM in forward/backward pass\n",
            "epoch 010:  36% 247/692 [00:55<04:12,  1.76it/s, loss=2.587, ppl=6.01, wps=8691.3, ups=4.44, wpb=1956.4, bsz=128, num_updates=6400, lr=0.0005, gnorm=1.331, train_wall=21, gb_free=10.4, wall=1465]2022-03-10 06:37:43 | WARNING | fairseq.trainer | OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 2.50 GiB (GPU 0; 11.17 GiB total capacity; 8.45 GiB already allocated; 1.22 GiB free; 9.43 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
            "2022-03-10 06:37:43 | WARNING | fairseq.trainer | |===========================================================================|\n",
            "|                  PyTorch CUDA memory summary, device ID 0                 |\n",
            "|---------------------------------------------------------------------------|\n",
            "|            CUDA OOMs: 73           |        cudaMalloc retries: 126       |\n",
            "|===========================================================================|\n",
            "|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Allocated memory      |    8611 MB |    8653 MB |   13687 GB |   13679 GB |\n",
            "|       from large pool |    8561 MB |    8602 MB |   12933 GB |   12925 GB |\n",
            "|       from small pool |      50 MB |      61 MB |     754 GB |     754 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Active memory         |    8611 MB |    8653 MB |   13687 GB |   13679 GB |\n",
            "|       from large pool |    8561 MB |    8602 MB |   12933 GB |   12925 GB |\n",
            "|       from small pool |      50 MB |      61 MB |     754 GB |     754 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| GPU reserved memory   |    9658 MB |    9664 MB |  196804 MB |  187146 MB |\n",
            "|       from large pool |    9598 MB |    9598 MB |  190214 MB |  180616 MB |\n",
            "|       from small pool |      60 MB |      66 MB |    6590 MB |    6530 MB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Non-releasable memory |    1046 MB |    2329 MB |   15294 GB |   15293 GB |\n",
            "|       from large pool |    1036 MB |    2317 MB |   14479 GB |   14478 GB |\n",
            "|       from small pool |       9 MB |      16 MB |     815 GB |     815 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Allocations           |     593    |     595    |    7196 K  |    7196 K  |\n",
            "|       from large pool |     162    |     163    |    2252 K  |    2252 K  |\n",
            "|       from small pool |     431    |     516    |    4943 K  |    4943 K  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Active allocs         |     593    |     595    |    7196 K  |    7196 K  |\n",
            "|       from large pool |     162    |     163    |    2252 K  |    2252 K  |\n",
            "|       from small pool |     431    |     516    |    4943 K  |    4943 K  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| GPU reserved segments |      69    |      72    |    3575    |    3506    |\n",
            "|       from large pool |      39    |      39    |     280    |     241    |\n",
            "|       from small pool |      30    |      33    |    3295    |    3265    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Non-releasable allocs |      65    |      65    |    3268 K  |    3268 K  |\n",
            "|       from large pool |      33    |      33    |    1431 K  |    1431 K  |\n",
            "|       from small pool |      32    |      38    |    1836 K  |    1836 K  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Oversize allocations  |       0    |       0    |       0    |       0    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Oversize GPU segments |       0    |       0    |       0    |       0    |\n",
            "|===========================================================================|\n",
            "\n",
            "2022-03-10 06:37:43 | WARNING | fairseq.trainer | attempting to recover from OOM in forward/backward pass\n",
            "epoch 010:  59% 411/692 [01:30<00:56,  4.94it/s, loss=2.705, ppl=6.52, wps=8229.8, ups=4.03, wpb=2040.7, bsz=128, num_updates=6500, lr=0.0005, gnorm=1.361, train_wall=24, gb_free=10.1, wall=1489]2022-03-10 06:38:18 | WARNING | fairseq.trainer | OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 1.34 GiB (GPU 0; 11.17 GiB total capacity; 7.45 GiB already allocated; 1.20 GiB free; 9.45 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
            "2022-03-10 06:38:18 | WARNING | fairseq.trainer | |===========================================================================|\n",
            "|                  PyTorch CUDA memory summary, device ID 0                 |\n",
            "|---------------------------------------------------------------------------|\n",
            "|            CUDA OOMs: 74           |        cudaMalloc retries: 128       |\n",
            "|===========================================================================|\n",
            "|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Allocated memory      |    6262 MB |    7629 MB |   14012 GB |   14006 GB |\n",
            "|       from large pool |    6214 MB |    7580 MB |   13240 GB |   13234 GB |\n",
            "|       from small pool |      48 MB |      60 MB |     772 GB |     771 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Active memory         |    6262 MB |    7629 MB |   14012 GB |   14006 GB |\n",
            "|       from large pool |    6214 MB |    7580 MB |   13240 GB |   13234 GB |\n",
            "|       from small pool |      48 MB |      60 MB |     772 GB |     771 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| GPU reserved memory   |    9674 MB |    9732 MB |  198246 MB |  188572 MB |\n",
            "|       from large pool |    9614 MB |    9614 MB |  191582 MB |  181968 MB |\n",
            "|       from small pool |      60 MB |     134 MB |    6664 MB |    6604 MB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Non-releasable memory |    2043 MB |    2044 MB |   15642 GB |   15640 GB |\n",
            "|       from large pool |    2031 MB |    2033 MB |   14807 GB |   14805 GB |\n",
            "|       from small pool |      11 MB |      15 MB |     834 GB |     834 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Allocations           |     592    |     595    |    7379 K  |    7378 K  |\n",
            "|       from large pool |     160    |     164    |    2311 K  |    2311 K  |\n",
            "|       from small pool |     432    |     516    |    5067 K  |    5067 K  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Active allocs         |     592    |     595    |    7379 K  |    7378 K  |\n",
            "|       from large pool |     160    |     164    |    2311 K  |    2311 K  |\n",
            "|       from small pool |     432    |     516    |    5067 K  |    5067 K  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| GPU reserved segments |      69    |     106    |    3613    |    3544    |\n",
            "|       from large pool |      39    |      39    |     281    |     242    |\n",
            "|       from small pool |      30    |      67    |    3332    |    3302    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Non-releasable allocs |      56    |      57    |    3350 K  |    3350 K  |\n",
            "|       from large pool |      24    |      25    |    1468 K  |    1468 K  |\n",
            "|       from small pool |      32    |      38    |    1881 K  |    1881 K  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Oversize allocations  |       0    |       0    |       0    |       0    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Oversize GPU segments |       0    |       0    |       0    |       0    |\n",
            "|===========================================================================|\n",
            "\n",
            "2022-03-10 06:38:18 | WARNING | fairseq.trainer | attempting to recover from OOM in forward/backward pass\n",
            "epoch 010:  67% 467/692 [01:42<00:36,  6.24it/s, loss=2.494, ppl=5.64, wps=8980.9, ups=5, wpb=1796.7, bsz=128, num_updates=6600, lr=0.0005, gnorm=1.423, train_wall=19, gb_free=9.6, wall=1509]2022-03-10 06:38:30 | WARNING | fairseq.trainer | OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 2.01 GiB (GPU 0; 11.17 GiB total capacity; 7.85 GiB already allocated; 1.47 GiB free; 9.18 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
            "2022-03-10 06:38:30 | WARNING | fairseq.trainer | |===========================================================================|\n",
            "|                  PyTorch CUDA memory summary, device ID 0                 |\n",
            "|---------------------------------------------------------------------------|\n",
            "|            CUDA OOMs: 75           |        cudaMalloc retries: 129       |\n",
            "|===========================================================================|\n",
            "|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Allocated memory      |    8040 MB |    8056 MB |   14116 GB |   14108 GB |\n",
            "|       from large pool |    7990 MB |    8007 MB |   13337 GB |   13330 GB |\n",
            "|       from small pool |      49 MB |      60 MB |     778 GB |     778 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Active memory         |    8040 MB |    8056 MB |   14116 GB |   14108 GB |\n",
            "|       from large pool |    7990 MB |    8007 MB |   13337 GB |   13330 GB |\n",
            "|       from small pool |      49 MB |      60 MB |     778 GB |     778 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| GPU reserved memory   |    9400 MB |    9468 MB |  199408 MB |  190008 MB |\n",
            "|       from large pool |    9340 MB |    9340 MB |  192676 MB |  183336 MB |\n",
            "|       from small pool |      60 MB |     128 MB |    6732 MB |    6672 MB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Non-releasable memory |    1359 MB |    1994 MB |   15747 GB |   15746 GB |\n",
            "|       from large pool |    1349 MB |    1983 MB |   14906 GB |   14904 GB |\n",
            "|       from small pool |      10 MB |      15 MB |     841 GB |     841 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Allocations           |     592    |     595    |    7441 K  |    7440 K  |\n",
            "|       from large pool |     162    |     164    |    2331 K  |    2331 K  |\n",
            "|       from small pool |     430    |     516    |    5109 K  |    5109 K  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Active allocs         |     592    |     595    |    7441 K  |    7440 K  |\n",
            "|       from large pool |     162    |     164    |    2331 K  |    2331 K  |\n",
            "|       from small pool |     430    |     516    |    5109 K  |    5109 K  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| GPU reserved segments |      69    |     103    |    3648    |    3579    |\n",
            "|       from large pool |      39    |      39    |     282    |     243    |\n",
            "|       from small pool |      30    |      64    |    3366    |    3336    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Non-releasable allocs |      61    |      61    |    3378 K  |    3378 K  |\n",
            "|       from large pool |      27    |      27    |    1481 K  |    1481 K  |\n",
            "|       from small pool |      34    |      39    |    1896 K  |    1896 K  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Oversize allocations  |       0    |       0    |       0    |       0    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Oversize GPU segments |       0    |       0    |       0    |       0    |\n",
            "|===========================================================================|\n",
            "\n",
            "2022-03-10 06:38:30 | WARNING | fairseq.trainer | attempting to recover from OOM in forward/backward pass\n",
            "epoch 010:  81% 558/692 [02:03<00:23,  5.64it/s, loss=2.729, ppl=6.63, wps=8685.5, ups=4.56, wpb=1906.7, bsz=128, num_updates=6700, lr=0.0005, gnorm=1.455, train_wall=21, gb_free=9.3, wall=1531]2022-03-10 06:38:52 | WARNING | fairseq.trainer | OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 2.11 GiB (GPU 0; 11.17 GiB total capacity; 7.85 GiB already allocated; 1.76 GiB free; 8.89 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
            "2022-03-10 06:38:52 | WARNING | fairseq.trainer | |===========================================================================|\n",
            "|                  PyTorch CUDA memory summary, device ID 0                 |\n",
            "|---------------------------------------------------------------------------|\n",
            "|            CUDA OOMs: 76           |        cudaMalloc retries: 132       |\n",
            "|===========================================================================|\n",
            "|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Allocated memory      |    8034 MB |    8044 MB |   14322 GB |   14314 GB |\n",
            "|       from large pool |    7984 MB |    7995 MB |   13532 GB |   13524 GB |\n",
            "|       from small pool |      49 MB |      60 MB |     789 GB |     789 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Active memory         |    8034 MB |    8044 MB |   14322 GB |   14314 GB |\n",
            "|       from large pool |    7984 MB |    7995 MB |   13532 GB |   13524 GB |\n",
            "|       from small pool |      49 MB |      60 MB |     789 GB |     789 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| GPU reserved memory   |    9108 MB |   10862 MB |  204456 MB |  195348 MB |\n",
            "|       from large pool |    9044 MB |   10724 MB |  197576 MB |  188532 MB |\n",
            "|       from small pool |      64 MB |     138 MB |    6880 MB |    6816 MB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Non-releasable memory |    1073 MB |    1745 MB |   15967 GB |   15966 GB |\n",
            "|       from large pool |    1059 MB |    1729 MB |   15113 GB |   15112 GB |\n",
            "|       from small pool |      14 MB |      48 MB |     853 GB |     853 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Allocations           |     592    |     595    |    7541 K  |    7541 K  |\n",
            "|       from large pool |     162    |     164    |    2361 K  |    2361 K  |\n",
            "|       from small pool |     430    |     516    |    5180 K  |    5180 K  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Active allocs         |     592    |     595    |    7541 K  |    7541 K  |\n",
            "|       from large pool |     162    |     164    |    2361 K  |    2361 K  |\n",
            "|       from small pool |     430    |     516    |    5180 K  |    5180 K  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| GPU reserved segments |      71    |     109    |    3725    |    3654    |\n",
            "|       from large pool |      39    |      40    |     285    |     246    |\n",
            "|       from small pool |      32    |      69    |    3440    |    3408    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Non-releasable allocs |      60    |      70    |    3424 K  |    3423 K  |\n",
            "|       from large pool |      24    |      26    |    1500 K  |    1500 K  |\n",
            "|       from small pool |      36    |      65    |    1923 K  |    1923 K  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Oversize allocations  |       0    |       0    |       0    |       0    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Oversize GPU segments |       0    |       0    |       0    |       0    |\n",
            "|===========================================================================|\n",
            "\n",
            "2022-03-10 06:38:52 | WARNING | fairseq.trainer | attempting to recover from OOM in forward/backward pass\n",
            "epoch 010:  97% 672/692 [02:29<00:04,  4.75it/s, loss=2.91, ppl=7.51, wps=7840, ups=4.05, wpb=1933.4, bsz=128, num_updates=6800, lr=0.0005, gnorm=1.47, train_wall=23, gb_free=10.5, wall=1556]2022-03-10 06:39:18 | WARNING | fairseq.trainer | OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 1.46 GiB (GPU 0; 11.17 GiB total capacity; 9.38 GiB already allocated; 709.81 MiB free; 9.96 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
            "2022-03-10 06:39:18 | WARNING | fairseq.trainer | |===========================================================================|\n",
            "|                  PyTorch CUDA memory summary, device ID 0                 |\n",
            "|---------------------------------------------------------------------------|\n",
            "|            CUDA OOMs: 77           |        cudaMalloc retries: 136       |\n",
            "|===========================================================================|\n",
            "|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Allocated memory      |    8108 MB |    9600 MB |   14546 GB |   14538 GB |\n",
            "|       from large pool |    8059 MB |    9551 MB |   13742 GB |   13734 GB |\n",
            "|       from small pool |      48 MB |      60 MB |     804 GB |     804 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Active memory         |    8108 MB |    9600 MB |   14546 GB |   14538 GB |\n",
            "|       from large pool |    8059 MB |    9551 MB |   13742 GB |   13734 GB |\n",
            "|       from small pool |      48 MB |      60 MB |     804 GB |     804 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| GPU reserved memory   |   10198 MB |   10828 MB |  220994 MB |  210796 MB |\n",
            "|       from large pool |   10138 MB |   10690 MB |  214040 MB |  203902 MB |\n",
            "|       from small pool |      60 MB |     138 MB |    6954 MB |    6894 MB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Non-releasable memory |  612323 KB |    1307 MB |   16202 GB |   16202 GB |\n",
            "|       from large pool |  600998 KB |    1295 MB |   15332 GB |   15331 GB |\n",
            "|       from small pool |   11325 KB |      26 MB |     870 GB |     870 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Allocations           |     592    |     595    |    7667 K  |    7667 K  |\n",
            "|       from large pool |     160    |     164    |    2397 K  |    2397 K  |\n",
            "|       from small pool |     432    |     516    |    5270 K  |    5269 K  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Active allocs         |     592    |     595    |    7667 K  |    7667 K  |\n",
            "|       from large pool |     160    |     164    |    2397 K  |    2397 K  |\n",
            "|       from small pool |     432    |     516    |    5270 K  |    5269 K  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| GPU reserved segments |      82    |     123    |    3812    |    3730    |\n",
            "|       from large pool |      52    |      54    |     335    |     283    |\n",
            "|       from small pool |      30    |      69    |    3477    |    3447    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Non-releasable allocs |      64    |      66    |    3482 K  |    3482 K  |\n",
            "|       from large pool |      31    |      32    |    1524 K  |    1524 K  |\n",
            "|       from small pool |      33    |      47    |    1957 K  |    1957 K  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Oversize allocations  |       0    |       0    |       0    |       0    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Oversize GPU segments |       0    |       0    |       0    |       0    |\n",
            "|===========================================================================|\n",
            "\n",
            "2022-03-10 06:39:18 | WARNING | fairseq.trainer | attempting to recover from OOM in forward/backward pass\n",
            "epoch 010: 100% 691/692 [02:35<00:00,  5.07it/s, loss=2.91, ppl=7.51, wps=7840, ups=4.05, wpb=1933.4, bsz=128, num_updates=6800, lr=0.0005, gnorm=1.47, train_wall=23, gb_free=10.5, wall=1556]2022-03-10 06:39:22 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "\n",
            "epoch 010 | valid on 'valid' subset:   0% 0/8 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 010 | valid on 'valid' subset:  12% 1/8 [00:00<00:00,  7.72it/s]\u001b[A\n",
            "epoch 010 | valid on 'valid' subset:  25% 2/8 [00:00<00:00,  8.10it/s]\u001b[A\n",
            "epoch 010 | valid on 'valid' subset:  50% 4/8 [00:00<00:00,  9.82it/s]\u001b[A\n",
            "epoch 010 | valid on 'valid' subset:  62% 5/8 [00:00<00:00,  9.41it/s]\u001b[A\n",
            "epoch 010 | valid on 'valid' subset:  75% 6/8 [00:00<00:00,  9.24it/s]\u001b[A\n",
            "epoch 010 | valid on 'valid' subset:  88% 7/8 [00:00<00:00,  8.83it/s]\u001b[A\n",
            "epoch 010 | valid on 'valid' subset: 100% 8/8 [00:01<00:00,  4.96it/s]\u001b[A\n",
            "                                                                      \u001b[A2022-03-10 06:39:23 | INFO | valid | epoch 010 | valid on 'valid' subset | loss 5.34 | ppl 40.5 | wps 13911.2 | wpb 1992.9 | bsz 125 | num_updates 6843 | best_loss 4.912\n",
            "2022-03-10 06:39:23 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 10 @ 6843 updates\n",
            "2022-03-10 06:39:23 | INFO | fairseq.trainer | Saving checkpoint to /content/checkpoints/checkpoint10.pt\n",
            "2022-03-10 06:39:24 | INFO | fairseq.trainer | Finished saving checkpoint to /content/checkpoints/checkpoint10.pt\n",
            "2022-03-10 06:39:25 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/checkpoint10.pt (epoch 10 @ 6843 updates, score 5.34) (writing took 1.2296676130001742 seconds)\n",
            "2022-03-10 06:39:25 | INFO | fairseq_cli.train | end of epoch 10 (average epoch stats below)\n",
            "2022-03-10 06:39:25 | INFO | train | epoch 010 | loss 2.643 | ppl 6.25 | wps 8365.9 | ups 4.34 | wpb 1928.8 | bsz 127.9 | num_updates 6843 | lr 0.0005 | gnorm 1.387 | train_wall 146 | gb_free 10.6 | wall 1568\n",
            "2022-03-10 06:39:25 | INFO | fairseq_cli.train | done training in 1568.3 seconds\n",
            "\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Waiting for W&B process to finish... \u001b[32m(success).\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                                                                                \n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run history:\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:              train/bsz ▁▁▁▁▁▁▁▁▁▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:          train/gb_free █▁████████\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:            train/gnorm ▃▁▂▃▄▅▆▇▇█\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:             train/loss █▅▄▃▃▂▂▂▁▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:               train/lr ▁▁▁▁▁▁▁▁▁▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:              train/ppl █▃▂▂▁▁▁▁▁▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:       train/train_wall ▇▇▇▃▂▁▅█▃▇\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:              train/ups ▄▂▅▆▅█▅▁▇▃\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:             train/wall ▁▂▃▃▄▅▆▆▇█\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:              train/wpb █▇█▄▄▁▄▇▄▃\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:              train/wps ▇▂▇▇▆▇▆▁█▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:        train_inner/bsz █████▁███▁█████▁████▁███████▁█████▁█████\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:    train_inner/gb_free ▆█████▇█▇██▇███████▇▁███▁██▇██▇█▅██▇██▇█\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:      train_inner/gnorm █▂▁▁▁▂▂▂▂▂▂▂▂▂▃▃▃▃▃▃▂▃▄▃▃▃▄▄▃▄▄▄▄▄▄▄▄▄▄▅\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:       train_inner/loss █▇▆▆▅▅▄▄▄▄▃▃▃▃▃▃▂▂▃▂▂▂▂▂▂▂▂▂▁▁▂▂▂▁▂▂▁▁▁▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:         train_inner/lr ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:        train_inner/ppl █▄▃▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: train_inner/train_wall ▂▁▃█▃▃▃▃▆▃▃▅▃▃▄▃▂▃▄▃▃▃▃▆▅▂▃▅▃▂▃▃▃▂▄▄▃▃▂▅\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:        train_inner/ups ▆█▆▂▂▅▆▆▁▄▅▄▃▄▄▅▅▅▅▆▃▅▅▂▁▆▄▄▄▅▅▄▄▆▃▄▄▅▆▃\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:       train_inner/wall ▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:        train_inner/wpb ▁▂▄█▄▂▂▃▇▃▂▆▂▄▃▅▁▂▆▃▅▅▁▇▃▂▃▄▂▂▆▄▃▂▅▆▃▄▁▃\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:        train_inner/wps ▆█▇▄▂▅▆▆▂▄▅▆▃▅▄▇▄▄▇▆▄▆▅▃▁▆▄▅▃▅▆▅▄▆▄▆▄▅▆▃\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:        valid/best_loss █▃▂▁▁▁▁▁▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:              valid/bsz ▁▁▁▁▁▁▁▁▁▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:             valid/loss █▄▂▁▁▁▂▂▂▃\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:              valid/ppl █▃▂▁▁▁▁▂▂▂\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:              valid/wpb ▁▁▁▁▁▁▁▁▁▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:              valid/wps ▃▇▆▇▇▂███▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run summary:\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:              train/bsz 127.9\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:          train/gb_free 10.6\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:            train/gnorm 1.387\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:             train/loss 2.643\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:               train/lr 0.0005\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:              train/ppl 6.25\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:       train/train_wall 146.0\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:              train/ups 4.34\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:             train/wall 1568.0\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:              train/wpb 1928.8\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:              train/wps 8365.9\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:        train_inner/bsz 128.0\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:    train_inner/gb_free 10.5\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:      train_inner/gnorm 1.47\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:       train_inner/loss 2.91\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:         train_inner/lr 0.0005\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:        train_inner/ppl 7.51\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: train_inner/train_wall 23.0\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:        train_inner/ups 4.05\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:       train_inner/wall 1556.0\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:        train_inner/wpb 1933.4\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:        train_inner/wps 7840.0\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:        valid/best_loss 4.912\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:              valid/bsz 125.0\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:             valid/loss 5.34\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:              valid/ppl 40.5\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:              valid/wpb 1992.9\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:              valid/wps 13911.2\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Synced \u001b[33mcheckpoints\u001b[0m: \u001b[34m\u001b[4mhttps://wandb.ai/rahul2023_usa/Transformer%20from%20scratch%20-%2010%20March%20-%20kn%20to%20te/runs/epd9hqwt\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Find logs at: \u001b[35m\u001b[1m./wandb/run-20220310_061317-epd9hqwt/logs\u001b[0m\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!fairseq-generate /content/content/nmt-kn-te/tokenized.kn-te \\\n",
        "  --path /content/checkpoints/checkpoint_best.pt \\\n",
        "  --beam 5 \\\n",
        "  --remove-bpe \\\n",
        " | grep ^H | LC_ALL=C sort -V | cut -f3- > test_te_generated_scratch_transformer.txt "
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "o2Wrg-JBcTDx",
        "outputId": "56b00d12-bf0a-4d47-90b4-08850e4f1cb6"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2022-03-10 06:40:20 | INFO | fairseq.tasks.text_to_speech | Please install tensorboardX: pip install tensorboardX\n",
            "2022-03-10 06:40:23 | INFO | fairseq_cli.generate | {'_name': None, 'common': {'_name': None, 'no_progress_bar': False, 'log_interval': 100, 'log_format': None, 'log_file': None, 'tensorboard_logdir': None, 'wandb_project': None, 'azureml_logging': False, 'seed': 1, 'cpu': False, 'tpu': False, 'bf16': False, 'memory_efficient_bf16': False, 'fp16': False, 'memory_efficient_fp16': False, 'fp16_no_flatten_grads': False, 'fp16_init_scale': 128, 'fp16_scale_window': None, 'fp16_scale_tolerance': 0.0, 'on_cpu_convert_precision': False, 'min_loss_scale': 0.0001, 'threshold_loss_scale': None, 'amp': False, 'amp_batch_retries': 2, 'amp_init_scale': 128, 'amp_scale_window': None, 'user_dir': None, 'empty_cache_freq': 0, 'all_gather_list_size': 16384, 'model_parallel_size': 1, 'quantization_config_path': None, 'profile': False, 'reset_logging': False, 'suppress_crashes': False, 'use_plasma_view': False, 'plasma_path': '/tmp/plasma'}, 'common_eval': {'_name': None, 'path': '/content/checkpoints/checkpoint_best.pt', 'post_process': 'subword_nmt', 'quiet': False, 'model_overrides': '{}', 'results_path': None}, 'distributed_training': {'_name': None, 'distributed_world_size': 1, 'distributed_num_procs': 1, 'distributed_rank': 0, 'distributed_backend': 'nccl', 'distributed_init_method': None, 'distributed_port': -1, 'device_id': 0, 'distributed_no_spawn': False, 'ddp_backend': 'pytorch_ddp', 'ddp_comm_hook': 'none', 'bucket_cap_mb': 25, 'fix_batches_to_gpus': False, 'find_unused_parameters': False, 'gradient_as_bucket_view': False, 'fast_stat_sync': False, 'heartbeat_timeout': -1, 'broadcast_buffers': False, 'slowmo_momentum': None, 'slowmo_base_algorithm': 'localsgd', 'localsgd_frequency': 3, 'nprocs_per_node': 1, 'pipeline_model_parallel': False, 'pipeline_balance': None, 'pipeline_devices': None, 'pipeline_chunks': 0, 'pipeline_encoder_balance': None, 'pipeline_encoder_devices': None, 'pipeline_decoder_balance': None, 'pipeline_decoder_devices': None, 'pipeline_checkpoint': 'never', 'zero_sharding': 'none', 'fp16': False, 'memory_efficient_fp16': False, 'tpu': False, 'no_reshard_after_forward': False, 'fp32_reduce_scatter': False, 'cpu_offload': False, 'use_sharded_state': False, 'not_fsdp_flatten_parameters': False}, 'dataset': {'_name': None, 'num_workers': 1, 'skip_invalid_size_inputs_valid_test': False, 'max_tokens': 12000, 'batch_size': None, 'required_batch_size_multiple': 8, 'required_seq_len_multiple': 1, 'dataset_impl': None, 'data_buffer_size': 10, 'train_subset': 'train', 'valid_subset': 'valid', 'combine_valid_subsets': None, 'ignore_unused_valid_subsets': False, 'validate_interval': 1, 'validate_interval_updates': 0, 'validate_after_updates': 0, 'fixed_validation_seed': None, 'disable_validation': False, 'max_tokens_valid': None, 'batch_size_valid': None, 'max_valid_steps': None, 'curriculum': 0, 'gen_subset': 'test', 'num_shards': 1, 'shard_id': 0, 'grouped_shuffling': False, 'update_epoch_batch_itr': False, 'update_ordered_indices_seed': False}, 'optimization': {'_name': None, 'max_epoch': 0, 'max_update': 0, 'stop_time_hours': 0.0, 'clip_norm': 0.0, 'sentence_avg': False, 'update_freq': [1], 'lr': [0.25], 'stop_min_lr': -1.0, 'use_bmuf': False, 'skip_remainder_batch': False}, 'checkpoint': {'_name': None, 'save_dir': 'checkpoints', 'restore_file': 'checkpoint_last.pt', 'continue_once': None, 'finetune_from_model': None, 'reset_dataloader': False, 'reset_lr_scheduler': False, 'reset_meters': False, 'reset_optimizer': False, 'optimizer_overrides': '{}', 'save_interval': 1, 'save_interval_updates': 0, 'keep_interval_updates': -1, 'keep_interval_updates_pattern': -1, 'keep_last_epochs': -1, 'keep_best_checkpoints': -1, 'no_save': False, 'no_epoch_checkpoints': False, 'no_last_checkpoints': False, 'no_save_optimizer_state': False, 'best_checkpoint_metric': 'loss', 'maximize_best_checkpoint_metric': False, 'patience': -1, 'checkpoint_suffix': '', 'checkpoint_shard_count': 1, 'load_checkpoint_on_all_dp_ranks': False, 'write_checkpoints_asynchronously': False, 'model_parallel_size': 1}, 'bmuf': {'_name': None, 'block_lr': 1.0, 'block_momentum': 0.875, 'global_sync_iter': 50, 'warmup_iterations': 500, 'use_nbm': False, 'average_sync': False, 'distributed_world_size': 1}, 'generation': {'_name': None, 'beam': 5, 'nbest': 1, 'max_len_a': 0.0, 'max_len_b': 200, 'min_len': 1, 'match_source_len': False, 'unnormalized': False, 'no_early_stop': False, 'no_beamable_mm': False, 'lenpen': 1.0, 'unkpen': 0.0, 'replace_unk': None, 'sacrebleu': False, 'score_reference': False, 'prefix_size': 0, 'no_repeat_ngram_size': 0, 'sampling': False, 'sampling_topk': -1, 'sampling_topp': -1.0, 'constraints': None, 'temperature': 1.0, 'diverse_beam_groups': -1, 'diverse_beam_strength': 0.5, 'diversity_rate': -1.0, 'print_alignment': None, 'print_step': False, 'lm_path': None, 'lm_weight': 0.0, 'iter_decode_eos_penalty': 0.0, 'iter_decode_max_iter': 10, 'iter_decode_force_max_iter': False, 'iter_decode_with_beam': 1, 'iter_decode_with_external_reranker': False, 'retain_iter_history': False, 'retain_dropout': False, 'retain_dropout_modules': None, 'decoding_format': None, 'no_seed_provided': False}, 'eval_lm': {'_name': None, 'output_word_probs': False, 'output_word_stats': False, 'context_window': 0, 'softmax_batch': 9223372036854775807}, 'interactive': {'_name': None, 'buffer_size': 0, 'input': '-'}, 'model': {'_name': 'wav2vec2', 'extractor_mode': 'default', 'encoder_layers': 12, 'encoder_embed_dim': 768, 'encoder_ffn_embed_dim': 3072, 'encoder_attention_heads': 12, 'activation_fn': 'gelu', 'layer_type': 'transformer', 'dropout': 0.1, 'attention_dropout': 0.1, 'activation_dropout': 0.0, 'encoder_layerdrop': 0.0, 'dropout_input': 0.0, 'dropout_features': 0.0, 'final_dim': 0, 'layer_norm_first': False, 'conv_feature_layers': '[(512, 10, 5)] + [(512, 3, 2)] * 4 + [(512,2,2)] + [(512,2,2)]', 'conv_bias': False, 'logit_temp': 0.1, 'quantize_targets': False, 'quantize_input': False, 'same_quantizer': False, 'target_glu': False, 'feature_grad_mult': 1.0, 'quantizer_depth': 1, 'quantizer_factor': 3, 'latent_vars': 320, 'latent_groups': 2, 'latent_dim': 0, 'mask_length': 10, 'mask_prob': 0.65, 'mask_selection': 'static', 'mask_other': 0.0, 'no_mask_overlap': False, 'mask_min_space': 1, 'require_same_masks': True, 'mask_dropout': 0.0, 'mask_channel_length': 10, 'mask_channel_prob': 0.0, 'mask_channel_before': False, 'mask_channel_selection': 'static', 'mask_channel_other': 0.0, 'no_mask_channel_overlap': False, 'mask_channel_min_space': 1, 'num_negatives': 100, 'negatives_from_everywhere': False, 'cross_sample_negatives': 0, 'codebook_negatives': 0, 'conv_pos': 128, 'conv_pos_groups': 16, 'pos_conv_depth': 1, 'latent_temp': [2.0, 0.5, 0.999995], 'max_positions': 100000, 'checkpoint_activations': False, 'required_seq_len_multiple': 1, 'crop_seq_to_multiple': 1, 'depthwise_conv_kernel_size': 31, 'attn_type': '', 'pos_enc_type': 'abs', 'fp16': False}, 'task': {'_name': 'translation', 'data': '/content/content/nmt-kn-te/tokenized.kn-te', 'source_lang': None, 'target_lang': None, 'load_alignments': False, 'left_pad_source': True, 'left_pad_target': False, 'max_source_positions': 1024, 'max_target_positions': 1024, 'upsample_primary': -1, 'truncate_source': False, 'num_batch_buckets': 0, 'train_subset': 'train', 'dataset_impl': None, 'required_seq_len_multiple': 1, 'eval_bleu': False, 'eval_bleu_args': '{}', 'eval_bleu_detok': 'space', 'eval_bleu_detok_args': '{}', 'eval_tokenized_bleu': False, 'eval_bleu_remove_bpe': None, 'eval_bleu_print_samples': False}, 'criterion': {'_name': 'cross_entropy', 'sentence_avg': True}, 'optimizer': None, 'lr_scheduler': {'_name': 'fixed', 'force_anneal': None, 'lr_shrink': 0.1, 'warmup_updates': 0, 'lr': [0.25]}, 'scoring': {'_name': 'bleu', 'pad': 1, 'eos': 2, 'unk': 3}, 'bpe': None, 'tokenizer': None, 'ema': {'_name': None, 'store_ema': False, 'ema_decay': 0.9999, 'ema_start_update': 0, 'ema_seed_model': None, 'ema_update_freq': 1, 'ema_fp32': False}, 'simul_type': None}\n",
            "2022-03-10 06:40:23 | INFO | fairseq.tasks.translation | [kn] dictionary: 16624 types\n",
            "2022-03-10 06:40:23 | INFO | fairseq.tasks.translation | [te] dictionary: 15992 types\n",
            "2022-03-10 06:40:23 | INFO | fairseq_cli.generate | loading model(s) from /content/checkpoints/checkpoint_best.pt\n",
            "2022-03-10 06:40:24 | INFO | fairseq.data.data_utils | loaded 1,000 examples from: /content/content/nmt-kn-te/tokenized.kn-te/test.kn-te.kn\n",
            "2022-03-10 06:40:24 | INFO | fairseq.data.data_utils | loaded 1,000 examples from: /content/content/nmt-kn-te/tokenized.kn-te/test.kn-te.te\n",
            "2022-03-10 06:40:24 | INFO | fairseq.tasks.translation | /content/content/nmt-kn-te/tokenized.kn-te test kn-te 1000 examples\n",
            "2022-03-10 06:40:45 | INFO | fairseq_cli.generate | NOTE: hypothesis and token scores are output in base 2\n",
            "2022-03-10 06:40:45 | INFO | fairseq_cli.generate | Translated 1,000 sentences (17,154 tokens) in 13.0s (76.94 sentences/s, 1319.90 tokens/s)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!fairseq-generate /content/content/nmt-kn-te/tokenized.kn-te \\\n",
        "  --path /content/checkpoints/checkpoint_best.pt \\\n",
        "  --beam 5 \\\n",
        "   --remove-bpe \\\n",
        "  | grep ^T | LC_ALL=C sort -V | cut -f2- > test_te_actual_scratch_transformer.txt "
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7PbBrq2lcXWG",
        "outputId": "18e347ca-70c3-47f1-8349-cb556fd4fcff"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2022-03-10 06:41:08 | INFO | fairseq.tasks.text_to_speech | Please install tensorboardX: pip install tensorboardX\n",
            "2022-03-10 06:41:11 | INFO | fairseq_cli.generate | {'_name': None, 'common': {'_name': None, 'no_progress_bar': False, 'log_interval': 100, 'log_format': None, 'log_file': None, 'tensorboard_logdir': None, 'wandb_project': None, 'azureml_logging': False, 'seed': 1, 'cpu': False, 'tpu': False, 'bf16': False, 'memory_efficient_bf16': False, 'fp16': False, 'memory_efficient_fp16': False, 'fp16_no_flatten_grads': False, 'fp16_init_scale': 128, 'fp16_scale_window': None, 'fp16_scale_tolerance': 0.0, 'on_cpu_convert_precision': False, 'min_loss_scale': 0.0001, 'threshold_loss_scale': None, 'amp': False, 'amp_batch_retries': 2, 'amp_init_scale': 128, 'amp_scale_window': None, 'user_dir': None, 'empty_cache_freq': 0, 'all_gather_list_size': 16384, 'model_parallel_size': 1, 'quantization_config_path': None, 'profile': False, 'reset_logging': False, 'suppress_crashes': False, 'use_plasma_view': False, 'plasma_path': '/tmp/plasma'}, 'common_eval': {'_name': None, 'path': '/content/checkpoints/checkpoint_best.pt', 'post_process': 'subword_nmt', 'quiet': False, 'model_overrides': '{}', 'results_path': None}, 'distributed_training': {'_name': None, 'distributed_world_size': 1, 'distributed_num_procs': 1, 'distributed_rank': 0, 'distributed_backend': 'nccl', 'distributed_init_method': None, 'distributed_port': -1, 'device_id': 0, 'distributed_no_spawn': False, 'ddp_backend': 'pytorch_ddp', 'ddp_comm_hook': 'none', 'bucket_cap_mb': 25, 'fix_batches_to_gpus': False, 'find_unused_parameters': False, 'gradient_as_bucket_view': False, 'fast_stat_sync': False, 'heartbeat_timeout': -1, 'broadcast_buffers': False, 'slowmo_momentum': None, 'slowmo_base_algorithm': 'localsgd', 'localsgd_frequency': 3, 'nprocs_per_node': 1, 'pipeline_model_parallel': False, 'pipeline_balance': None, 'pipeline_devices': None, 'pipeline_chunks': 0, 'pipeline_encoder_balance': None, 'pipeline_encoder_devices': None, 'pipeline_decoder_balance': None, 'pipeline_decoder_devices': None, 'pipeline_checkpoint': 'never', 'zero_sharding': 'none', 'fp16': False, 'memory_efficient_fp16': False, 'tpu': False, 'no_reshard_after_forward': False, 'fp32_reduce_scatter': False, 'cpu_offload': False, 'use_sharded_state': False, 'not_fsdp_flatten_parameters': False}, 'dataset': {'_name': None, 'num_workers': 1, 'skip_invalid_size_inputs_valid_test': False, 'max_tokens': 12000, 'batch_size': None, 'required_batch_size_multiple': 8, 'required_seq_len_multiple': 1, 'dataset_impl': None, 'data_buffer_size': 10, 'train_subset': 'train', 'valid_subset': 'valid', 'combine_valid_subsets': None, 'ignore_unused_valid_subsets': False, 'validate_interval': 1, 'validate_interval_updates': 0, 'validate_after_updates': 0, 'fixed_validation_seed': None, 'disable_validation': False, 'max_tokens_valid': None, 'batch_size_valid': None, 'max_valid_steps': None, 'curriculum': 0, 'gen_subset': 'test', 'num_shards': 1, 'shard_id': 0, 'grouped_shuffling': False, 'update_epoch_batch_itr': False, 'update_ordered_indices_seed': False}, 'optimization': {'_name': None, 'max_epoch': 0, 'max_update': 0, 'stop_time_hours': 0.0, 'clip_norm': 0.0, 'sentence_avg': False, 'update_freq': [1], 'lr': [0.25], 'stop_min_lr': -1.0, 'use_bmuf': False, 'skip_remainder_batch': False}, 'checkpoint': {'_name': None, 'save_dir': 'checkpoints', 'restore_file': 'checkpoint_last.pt', 'continue_once': None, 'finetune_from_model': None, 'reset_dataloader': False, 'reset_lr_scheduler': False, 'reset_meters': False, 'reset_optimizer': False, 'optimizer_overrides': '{}', 'save_interval': 1, 'save_interval_updates': 0, 'keep_interval_updates': -1, 'keep_interval_updates_pattern': -1, 'keep_last_epochs': -1, 'keep_best_checkpoints': -1, 'no_save': False, 'no_epoch_checkpoints': False, 'no_last_checkpoints': False, 'no_save_optimizer_state': False, 'best_checkpoint_metric': 'loss', 'maximize_best_checkpoint_metric': False, 'patience': -1, 'checkpoint_suffix': '', 'checkpoint_shard_count': 1, 'load_checkpoint_on_all_dp_ranks': False, 'write_checkpoints_asynchronously': False, 'model_parallel_size': 1}, 'bmuf': {'_name': None, 'block_lr': 1.0, 'block_momentum': 0.875, 'global_sync_iter': 50, 'warmup_iterations': 500, 'use_nbm': False, 'average_sync': False, 'distributed_world_size': 1}, 'generation': {'_name': None, 'beam': 5, 'nbest': 1, 'max_len_a': 0.0, 'max_len_b': 200, 'min_len': 1, 'match_source_len': False, 'unnormalized': False, 'no_early_stop': False, 'no_beamable_mm': False, 'lenpen': 1.0, 'unkpen': 0.0, 'replace_unk': None, 'sacrebleu': False, 'score_reference': False, 'prefix_size': 0, 'no_repeat_ngram_size': 0, 'sampling': False, 'sampling_topk': -1, 'sampling_topp': -1.0, 'constraints': None, 'temperature': 1.0, 'diverse_beam_groups': -1, 'diverse_beam_strength': 0.5, 'diversity_rate': -1.0, 'print_alignment': None, 'print_step': False, 'lm_path': None, 'lm_weight': 0.0, 'iter_decode_eos_penalty': 0.0, 'iter_decode_max_iter': 10, 'iter_decode_force_max_iter': False, 'iter_decode_with_beam': 1, 'iter_decode_with_external_reranker': False, 'retain_iter_history': False, 'retain_dropout': False, 'retain_dropout_modules': None, 'decoding_format': None, 'no_seed_provided': False}, 'eval_lm': {'_name': None, 'output_word_probs': False, 'output_word_stats': False, 'context_window': 0, 'softmax_batch': 9223372036854775807}, 'interactive': {'_name': None, 'buffer_size': 0, 'input': '-'}, 'model': {'_name': 'wav2vec2', 'extractor_mode': 'default', 'encoder_layers': 12, 'encoder_embed_dim': 768, 'encoder_ffn_embed_dim': 3072, 'encoder_attention_heads': 12, 'activation_fn': 'gelu', 'layer_type': 'transformer', 'dropout': 0.1, 'attention_dropout': 0.1, 'activation_dropout': 0.0, 'encoder_layerdrop': 0.0, 'dropout_input': 0.0, 'dropout_features': 0.0, 'final_dim': 0, 'layer_norm_first': False, 'conv_feature_layers': '[(512, 10, 5)] + [(512, 3, 2)] * 4 + [(512,2,2)] + [(512,2,2)]', 'conv_bias': False, 'logit_temp': 0.1, 'quantize_targets': False, 'quantize_input': False, 'same_quantizer': False, 'target_glu': False, 'feature_grad_mult': 1.0, 'quantizer_depth': 1, 'quantizer_factor': 3, 'latent_vars': 320, 'latent_groups': 2, 'latent_dim': 0, 'mask_length': 10, 'mask_prob': 0.65, 'mask_selection': 'static', 'mask_other': 0.0, 'no_mask_overlap': False, 'mask_min_space': 1, 'require_same_masks': True, 'mask_dropout': 0.0, 'mask_channel_length': 10, 'mask_channel_prob': 0.0, 'mask_channel_before': False, 'mask_channel_selection': 'static', 'mask_channel_other': 0.0, 'no_mask_channel_overlap': False, 'mask_channel_min_space': 1, 'num_negatives': 100, 'negatives_from_everywhere': False, 'cross_sample_negatives': 0, 'codebook_negatives': 0, 'conv_pos': 128, 'conv_pos_groups': 16, 'pos_conv_depth': 1, 'latent_temp': [2.0, 0.5, 0.999995], 'max_positions': 100000, 'checkpoint_activations': False, 'required_seq_len_multiple': 1, 'crop_seq_to_multiple': 1, 'depthwise_conv_kernel_size': 31, 'attn_type': '', 'pos_enc_type': 'abs', 'fp16': False}, 'task': {'_name': 'translation', 'data': '/content/content/nmt-kn-te/tokenized.kn-te', 'source_lang': None, 'target_lang': None, 'load_alignments': False, 'left_pad_source': True, 'left_pad_target': False, 'max_source_positions': 1024, 'max_target_positions': 1024, 'upsample_primary': -1, 'truncate_source': False, 'num_batch_buckets': 0, 'train_subset': 'train', 'dataset_impl': None, 'required_seq_len_multiple': 1, 'eval_bleu': False, 'eval_bleu_args': '{}', 'eval_bleu_detok': 'space', 'eval_bleu_detok_args': '{}', 'eval_tokenized_bleu': False, 'eval_bleu_remove_bpe': None, 'eval_bleu_print_samples': False}, 'criterion': {'_name': 'cross_entropy', 'sentence_avg': True}, 'optimizer': None, 'lr_scheduler': {'_name': 'fixed', 'force_anneal': None, 'lr_shrink': 0.1, 'warmup_updates': 0, 'lr': [0.25]}, 'scoring': {'_name': 'bleu', 'pad': 1, 'eos': 2, 'unk': 3}, 'bpe': None, 'tokenizer': None, 'ema': {'_name': None, 'store_ema': False, 'ema_decay': 0.9999, 'ema_start_update': 0, 'ema_seed_model': None, 'ema_update_freq': 1, 'ema_fp32': False}, 'simul_type': None}\n",
            "2022-03-10 06:41:11 | INFO | fairseq.tasks.translation | [kn] dictionary: 16624 types\n",
            "2022-03-10 06:41:11 | INFO | fairseq.tasks.translation | [te] dictionary: 15992 types\n",
            "2022-03-10 06:41:11 | INFO | fairseq_cli.generate | loading model(s) from /content/checkpoints/checkpoint_best.pt\n",
            "2022-03-10 06:41:11 | INFO | fairseq.data.data_utils | loaded 1,000 examples from: /content/content/nmt-kn-te/tokenized.kn-te/test.kn-te.kn\n",
            "2022-03-10 06:41:11 | INFO | fairseq.data.data_utils | loaded 1,000 examples from: /content/content/nmt-kn-te/tokenized.kn-te/test.kn-te.te\n",
            "2022-03-10 06:41:11 | INFO | fairseq.tasks.translation | /content/content/nmt-kn-te/tokenized.kn-te test kn-te 1000 examples\n",
            "2022-03-10 06:41:32 | INFO | fairseq_cli.generate | NOTE: hypothesis and token scores are output in base 2\n",
            "2022-03-10 06:41:32 | INFO | fairseq_cli.generate | Translated 1,000 sentences (17,154 tokens) in 13.0s (77.07 sentences/s, 1322.07 tokens/s)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!fairseq-score --sys /content/test_te_generated_scratch_transformer.txt --ref /content/test_te_actual_scratch_transformer.txt"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XBzyhAutcanG",
        "outputId": "edb7091a-abb0-4726-bfa5-560acb3ab97f"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2022-03-10 06:41:40 | INFO | fairseq.tasks.text_to_speech | Please install tensorboardX: pip install tensorboardX\n",
            "Namespace(ignore_case=False, order=4, ref='/content/test_te_actual_scratch_transformer.txt', sacrebleu=False, sentence_bleu=False, sys='/content/test_te_generated_scratch_transformer.txt')\n",
            "BLEU4 = 9.33, 32.3/11.3/6.1/3.4 (BP=1.000, ratio=1.181, syslen=15981, reflen=13533)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from nltk.translate.bleu_score import sentence_bleu\n",
        "f = open(\"/content/test_te_generated_scratch_transformer.txt\")\n",
        "ml_candidates = [i.strip() for i in f.readlines()]\n",
        "f = open(\"/content/test_te_actual_scratch_transformer.txt\")\n",
        "ml_gold = [i.strip() for i in f.readlines()]\n",
        "total_four = 0\n",
        "for i in range(len(ml_candidates)):\n",
        "    candidate = ml_candidates[i].split(\" \")\n",
        "    references = ml_gold[i].split(\" \")\n",
        "    reference = [references]\n",
        "    score_cumulative4 = sentence_bleu(reference, candidate, weights=(0.25, 0.25, 0.25, 0.25))\n",
        "    total_four = total_four + score_cumulative4\n",
        "print(total_four/len(ml_candidates))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aecAcO0CccNU",
        "outputId": "c548fb48-eaa6-4df0-a14e-6c7b1a56884e"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.4272681144273711\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/nltk/translate/bleu_score.py:490: UserWarning: \n",
            "Corpus/Sentence contains 0 counts of 2-gram overlaps.\n",
            "BLEU scores might be undesirable; use SmoothingFunction().\n",
            "  warnings.warn(_msg)\n",
            "/usr/local/lib/python3.7/dist-packages/nltk/translate/bleu_score.py:490: UserWarning: \n",
            "Corpus/Sentence contains 0 counts of 3-gram overlaps.\n",
            "BLEU scores might be undesirable; use SmoothingFunction().\n",
            "  warnings.warn(_msg)\n",
            "/usr/local/lib/python3.7/dist-packages/nltk/translate/bleu_score.py:490: UserWarning: \n",
            "Corpus/Sentence contains 0 counts of 4-gram overlaps.\n",
            "BLEU scores might be undesirable; use SmoothingFunction().\n",
            "  warnings.warn(_msg)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oFT7ibF7kPFv",
        "outputId": "a8f79591-0c44-40da-c9df-1b38f5299c6c"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!cp /content/checkpoints/checkpoint_best.pt /content/drive/MyDrive/Transformer_from_scratch_te/"
      ],
      "metadata": {
        "id": "TuliiOC-kU5r"
      },
      "execution_count": 10,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "Transformer from scratch - 10 March - kn to te.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}